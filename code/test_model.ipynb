{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params\n",
    "sequence_length: int = 32 # Not sure what happens when setting it with a number different from what its trained on\n",
    "\n",
    "# Model \n",
    "# model_name = \"bashar-talafha/multi-dialect-bert-base-arabic\"\n",
    "model_name: str = \"aubmindlab/bert-large-arabertv2\"\n",
    "model_names = [\n",
    "    \"aubmindlab/bert-base-arabertv2\",\n",
    "    # \"aubmindlab/bert-base-arabertv2\",\n",
    "    # \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "    # \"bashar-talafha/multi-dialect-bert-base-arabic\"\n",
    "]\n",
    "\n",
    "pretrained_classifier_names = [\"2021-12-05-train-0.898193359375\"]\n",
    "# pretrained_classifier_names = [name[name.rindex(\"\\\\\")+1:] for name in glob(\"models\\\\finalized_models\\\\*\")]\n",
    "# pretrained_classifier_names.pop(-1)\n",
    "\n",
    "# Data\n",
    "df: pd.DataFrame = get_annotated_data_folder_data()\n",
    "dfs = {\n",
    "    \"annotated_data\": get_annotated_data_folder_data(),\n",
    "    \"arabic_dialects\": get_arabic_dialects_dataset_folder_data(),\n",
    "    \"dart\": get_dart_folder_data()\n",
    "}\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "batch_size: int = 128\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_name, pretrained_classifier_name, df):\n",
    "    # Model\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "    arabert_prep = ArabertPreprocessor(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Preprocessing\n",
    "    temp_df = get_SMADC_folder_data(code_folder_path)\n",
    "    classes = temp_df[\"Region\"].unique()\n",
    "    num_labels = len(classes)\n",
    "    class_to_index = {class_:index for class_, index in zip(classes, range(num_labels))}\n",
    "    index_to_class = {index:class_ for class_, index in zip(classes, range(num_labels))}\n",
    "    temp_df[\"Labels\"] = temp_df[\"Region\"].apply(class_to_index.get)\n",
    "\n",
    "    df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "    df_encoding = tokenize(tokenizer, df[\"Text\"].to_list(), sequence_length)\n",
    "\n",
    "    test_set = Dialect_dataset(df_encoding, df[\"Labels\"].to_list())\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=pretrained_classifier, \n",
    "        compute_metrics=compute_metrics, \n",
    "        args=generate_training_args(\"models\", do_warmup=False, batch_size=batch_size)\n",
    "    )\n",
    "    prediction = trainer.predict(test_set)\n",
    "    return prediction.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:52:45,849 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:38<00:00,  5.43it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:54:24,760 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:12<00:00,  5.45it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:13<00:00,  5.51it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:55:12,798 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:34<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "results = []\n",
    "for model_name_ in model_names:\n",
    "    for pretrained_classifier_name_ in pretrained_classifier_names:\n",
    "        for name_df, df_ in dfs.items():\n",
    "            r = test(model_name_, pretrained_classifier_name_, df_)\n",
    "            results.append((model_name_, pretrained_classifier_name_, name_df, df_, \n",
    "                 r[\"test_loss\"], r[\"test_macro_f1\"], r[\"test_macro_precision\"], r[\"test_macro_recall\"], r[\"test_accuracy\"]\n",
    "            ))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070189</td>\n",
       "      <td>0.764301</td>\n",
       "      <td>0.775940</td>\n",
       "      <td>0.766432</td>\n",
       "      <td>0.761811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>0.624620</td>\n",
       "      <td>0.613687</td>\n",
       "      <td>0.679687</td>\n",
       "      <td>0.721758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658692</td>\n",
       "      <td>0.457107</td>\n",
       "      <td>0.469979</td>\n",
       "      <td>0.447111</td>\n",
       "      <td>0.642535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model name       Pretrained classifier name  \\\n",
       "2  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "0  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "1  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "\n",
       "           Dataset                                                 df  \\\n",
       "2             dart                                                ...   \n",
       "0   annotated_data       Region                                   ...   \n",
       "1  arabic_dialects                                                ...   \n",
       "\n",
       "       Loss  Macro F1  Macro precision  Macro recall  Accuracy  \n",
       "2  1.070189  0.764301         0.775940      0.766432  0.761811  \n",
       "0  1.513531  0.624620         0.613687      0.679687  0.721758  \n",
       "1  1.658692  0.457107         0.469979      0.447111  0.642535  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Model name\", \"Pretrained classifier name\", \"Dataset\", \"df\", \"Loss\", \"Macro F1\", \"Macro precision\", \"Macro recall\", \"Accuracy\"])\n",
    "df_results.sort_values(\"Macro F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annotated_data</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>7.294701</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.123827</td>\n",
       "      <td>0.082429</td>\n",
       "      <td>0.068684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabic_dialects</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>6.864985</td>\n",
       "      <td>0.082905</td>\n",
       "      <td>0.139608</td>\n",
       "      <td>0.073939</td>\n",
       "      <td>0.079095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>6.710032</td>\n",
       "      <td>0.066968</td>\n",
       "      <td>0.074348</td>\n",
       "      <td>0.066111</td>\n",
       "      <td>0.067589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model name  \\\n",
       "Dataset                                            \n",
       "annotated_data   aubmindlab/bert-large-arabertv2   \n",
       "arabic_dialects  aubmindlab/bert-large-arabertv2   \n",
       "dart             aubmindlab/bert-large-arabertv2   \n",
       "\n",
       "                                        Pretrained classifier name  \\\n",
       "Dataset                                                              \n",
       "annotated_data   2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "arabic_dialects  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "dart             2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "\n",
       "                         Dataset  \\\n",
       "Dataset                            \n",
       "annotated_data    annotated_data   \n",
       "arabic_dialects  arabic_dialects   \n",
       "dart                        dart   \n",
       "\n",
       "                                                                df      Loss  \\\n",
       "Dataset                                                                        \n",
       "annotated_data        Region                                   ...  7.294701   \n",
       "arabic_dialects                                                ...  6.864985   \n",
       "dart                                                           ...  6.710032   \n",
       "\n",
       "                 Macro F1  Macro precision  Macro recall  Accuracy  \n",
       "Dataset                                                             \n",
       "annotated_data   0.063544         0.123827      0.082429  0.068684  \n",
       "arabic_dialects  0.082905         0.139608      0.073939  0.079095  \n",
       "dart             0.066968         0.074348      0.066111  0.067589  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      "          Model name &         Dataset &  Macro F1 &  Macro precision &  Macro recall \\\\\n",
      "bert-large-arabertv2 &  annotated\\_data &  0.063544 &         0.123827 &      0.082429 \\\\\n",
      "\\midrule\n",
      "bert-large-arabertv2 & arabic\\_dialects &  0.082905 &         0.139608 &      0.073939 \\\\\n",
      "bert-large-arabertv2 &            dart &  0.066968 &         0.074348 &      0.066111 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/307787898.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_displayed[cols].to_latex(index=False,))\n"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1]).copy()\n",
    "cols = [\"Model name\", \"Dataset\", \"Macro F1\", \"Macro precision\", \"Macro recall\"]\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "print(df_displayed[cols].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "                     Model name &  SMADC Accuracy \\\\\n",
      "aubmindlab/bert-large-arabertv2 &        0.946777 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/3835315148.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_acc[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "df_acc = df_results.groupby(\"Model name\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1]).copy()\n",
    "# df_acc[\"Model name\"] = df_acc[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_acc[\"SMADC Accuracy\"] = df_acc[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "print(df_acc[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "          Model name &  SMADC Accuracy \\\\\n",
      "\\midrule\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/2447509516.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_displayed[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False,))\n"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results[[\"Model name\", \"Pretrained classifier name\"]].copy()\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "\n",
    "print(df_displayed[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>SMADC Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model name                         Pretrained classifier name  \\\n",
       "0  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "1  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "2  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "\n",
       "   SMADC Accuracy  \n",
       "0        0.946777  \n",
       "1        0.946777  \n",
       "2        0.946777  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete anything after this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = False \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.013 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "model_name = \"aubmindlab/bert-large-arabertv2\"\n",
    "from_pretrained_classifier: bool = False\n",
    "pretrained_classifier_name: str = \"2021-12-05-train-0.898193359375\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 4\n",
    "warmup_ratio: float = 0.2\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = True\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 19:24:23,444 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\36c5494fd851799740cb1dd3e94d010bad805cda3464b8e78a834d46e9d9f7a8.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\a7774340a9d7adbbae7d9d6f511e1e501684c13f7296bec4029a85938b157155.80a4303e73f2866371cbb896f25127a7a17110f071a37939f044882b3258b027\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b23b65a31311082d9b2031bee1f67d74bb76b438db863eeb8b9abad07f96452.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\e8c5e2a933856e2ffa93a0acf00d00bc27d06a7d01ea95cfe3ee88182d9fd191.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\f0fbe70f788dc9ff505358beebbf70a8e87a4a7c17f81ba1b8692c42d05e9182.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\36c5494fd851799740cb1dd3e94d010bad805cda3464b8e78a834d46e9d9f7a8.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", \"2021-12-05-train-0.898193359375\")\n",
    "pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>يا عم أنجز تعالى خلي +نا ناخد إجاز +ة</td>\n",
       "      <td>EGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هه طاح حض +ك</td>\n",
       "      <td>IRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>في أشياء تبقى حلو +ه ب+ ال+ سر تستلذ في +ها أن...</td>\n",
       "      <td>GLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أين ال+ مصدقيه في ال+ توظيف منذو زمن طويل و+ أ...</td>\n",
       "      <td>GLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ألف مبرووك حمددلله</td>\n",
       "      <td>GLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>هب +ة شتردين من +ي ل+ خاطر الله هه</td>\n",
       "      <td>IRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>يسلم ذوق +ك هي +ك ال+ مفروض على ال+ فنان ال+ م...</td>\n",
       "      <td>LEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>اللي هو ازاى يعنى</td>\n",
       "      <td>EGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>حرام علي +ك يا وزير ال+ تموين و+ ال+ رز اللي ي...</td>\n",
       "      <td>EGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>ن مافي +ه حماي +ة ل+ ال+ مستهلك ال+ حين صر +نا...</td>\n",
       "      <td>GLF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Region\n",
       "0                 يا عم أنجز تعالى خلي +نا ناخد إجاز +ة    EGY\n",
       "1                                          هه طاح حض +ك    IRQ\n",
       "2     في أشياء تبقى حلو +ه ب+ ال+ سر تستلذ في +ها أن...    GLF\n",
       "3     أين ال+ مصدقيه في ال+ توظيف منذو زمن طويل و+ أ...    GLF\n",
       "4                                    ألف مبرووك حمددلله    GLF\n",
       "...                                                 ...    ...\n",
       "4091                 هب +ة شتردين من +ي ل+ خاطر الله هه    IRQ\n",
       "4092  يسلم ذوق +ك هي +ك ال+ مفروض على ال+ فنان ال+ م...    LEV\n",
       "4093                                  اللي هو ازاى يعنى    EGY\n",
       "4094  حرام علي +ك يا وزير ال+ تموين و+ ال+ رز اللي ي...    EGY\n",
       "4095  ن مافي +ه حماي +ة ل+ ال+ مستهلك ال+ حين صر +نا...    GLF\n",
       "\n",
       "[4096 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test(model_name_, pretrained_classifier_name_, df_)\n",
    "# df = get_SMADC_folder_data()\n",
    "# df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "# train, test = train_test_split(df, test_size=test_validation_proportion, random_state=1)\n",
    "# validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=1)\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "# validate.reset_index(drop=True, inplace=True)\n",
    "# test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Tokenize\n",
    "# validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "# test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "# train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "test[\"Labels\"] = test[\"Region\"].apply(class_to_index.get)\n",
    "validate[\"Labels\"] = validate[\"Region\"].apply(class_to_index.get)\n",
    "train[\"Labels\"] = train[\"Region\"].apply(class_to_index.get)\n",
    "\n",
    "# Make Dataset \n",
    "validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14214\n",
      "  Batch size = 64\n",
      "100%|██████████| 223/223 [00:26<00:00,  7.18it/s]Trainer is attempting to log a value of \"{'NOR': {'precision': 0.9333740831295844, 'recall': 0.9666086406076911, 'f1-score': 0.9497006919070202, 'support': 6319}, 'IRQ': {'precision': 0.8625565610859729, 'recall': 0.8630447085455575, 'f1-score': 0.8628005657708628, 'support': 1767}, 'LEV': {'precision': 0.8424396442185514, 'recall': 0.8236024844720496, 'f1-score': 0.8329145728643216, 'support': 1610}, 'EGY': {'precision': 0.8195848855774348, 'recall': 0.7754279959718026, 'f1-score': 0.7968952134540751, 'support': 1986}, 'GLF': {'precision': 0.8664761126990609, 'recall': 0.8380726698262243, 'f1-score': 0.852037743425015, 'support': 2532}, 'accuracy': 0.8879273955255382, 'macro avg': {'precision': 0.8648862573421209, 'recall': 0.853351299884665, 'f1-score': 0.858869757484259, 'support': 14214}, 'weighted avg': {'precision': 0.8864548474769547, 'recall': 0.8879273955255382, 'f1-score': 0.8869222734248672, 'support': 14214}}\" of type <class 'dict'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|██████████| 223/223 [00:32<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "res = Trainer(\n",
    "        model=pretrained_classifier, \n",
    "        compute_metrics=compute_metrics, \n",
    "        args=generate_training_args(\"models\", do_warmup=False, batch_size=batch_size)\n",
    "    ).evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOR': {'precision': 0.9333740831295844,\n",
       "  'recall': 0.9666086406076911,\n",
       "  'f1-score': 0.9497006919070202,\n",
       "  'support': 6319},\n",
       " 'IRQ': {'precision': 0.8625565610859729,\n",
       "  'recall': 0.8630447085455575,\n",
       "  'f1-score': 0.8628005657708628,\n",
       "  'support': 1767},\n",
       " 'LEV': {'precision': 0.8424396442185514,\n",
       "  'recall': 0.8236024844720496,\n",
       "  'f1-score': 0.8329145728643216,\n",
       "  'support': 1610},\n",
       " 'EGY': {'precision': 0.8195848855774348,\n",
       "  'recall': 0.7754279959718026,\n",
       "  'f1-score': 0.7968952134540751,\n",
       "  'support': 1986},\n",
       " 'GLF': {'precision': 0.8664761126990609,\n",
       "  'recall': 0.8380726698262243,\n",
       "  'f1-score': 0.852037743425015,\n",
       "  'support': 2532},\n",
       " 'accuracy': 0.8879273955255382,\n",
       " 'macro avg': {'precision': 0.8648862573421209,\n",
       "  'recall': 0.853351299884665,\n",
       "  'f1-score': 0.858869757484259,\n",
       "  'support': 14214},\n",
       " 'weighted avg': {'precision': 0.8864548474769547,\n",
       "  'recall': 0.8879273955255382,\n",
       "  'f1-score': 0.8869222734248672,\n",
       "  'support': 14214}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"eval_report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graduation_project')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
