{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params\n",
    "sequence_length: int = 32 # Not sure what happens when setting it with a number different from what its trained on\n",
    "\n",
    "# Model \n",
    "# model_name = \"bashar-talafha/multi-dialect-bert-base-arabic\"\n",
    "model_name: str = \"aubmindlab/bert-large-arabertv2\"\n",
    "model_names = [\n",
    "    \"aubmindlab/bert-base-arabertv2\",\n",
    "    # \"aubmindlab/bert-base-arabertv2\",\n",
    "    # \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "    # \"bashar-talafha/multi-dialect-bert-base-arabic\"\n",
    "]\n",
    "\n",
    "pretrained_classifier_names = [\"2021-12-05-train-0.898193359375\"]\n",
    "# pretrained_classifier_names = [name[name.rindex(\"\\\\\")+1:] for name in glob(\"models\\\\finalized_models\\\\*\")]\n",
    "# pretrained_classifier_names.pop(-1)\n",
    "\n",
    "# Data\n",
    "df: pd.DataFrame = get_annotated_data_folder_data()\n",
    "dfs = {\n",
    "    \"annotated_data\": get_annotated_data_folder_data(),\n",
    "    \"arabic_dialects\": get_arabic_dialects_dataset_folder_data(),\n",
    "    \"dart\": get_dart_folder_data()\n",
    "}\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "batch_size: int = 128\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_name, pretrained_classifier_name, df):\n",
    "    # Model\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "    arabert_prep = ArabertPreprocessor(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Preprocessing\n",
    "    temp_df = get_SMADC_folder_data(code_folder_path)\n",
    "    classes = temp_df[\"Region\"].unique()\n",
    "    num_labels = len(classes)\n",
    "    class_to_index = {class_:index for class_, index in zip(classes, range(num_labels))}\n",
    "    index_to_class = {index:class_ for class_, index in zip(classes, range(num_labels))}\n",
    "    temp_df[\"Labels\"] = temp_df[\"Region\"].apply(class_to_index.get)\n",
    "\n",
    "    df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "    df_encoding = tokenize(tokenizer, df[\"Text\"].to_list(), sequence_length)\n",
    "\n",
    "    test_set = Dialect_dataset(df_encoding, df[\"Labels\"].to_list())\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=pretrained_classifier, \n",
    "        compute_metrics=compute_metrics, \n",
    "        args=generate_training_args(\"models\", do_warmup=False, batch_size=batch_size)\n",
    "    )\n",
    "    prediction = trainer.predict(test_set)\n",
    "    return prediction.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:52:45,849 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:38<00:00,  5.43it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:54:24,760 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:12<00:00,  5.45it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:13<00:00,  5.51it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 18:55:12,798 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:34<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "results = []\n",
    "for model_name_ in model_names:\n",
    "    for pretrained_classifier_name_ in pretrained_classifier_names:\n",
    "        for name_df, df_ in dfs.items():\n",
    "            r = test(model_name_, pretrained_classifier_name_, df_)\n",
    "            results.append((model_name_, pretrained_classifier_name_, name_df, df_, \n",
    "                 r[\"test_loss\"], r[\"test_macro_f1\"], r[\"test_macro_precision\"], r[\"test_macro_recall\"], r[\"test_accuracy\"]\n",
    "            ))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070189</td>\n",
       "      <td>0.764301</td>\n",
       "      <td>0.775940</td>\n",
       "      <td>0.766432</td>\n",
       "      <td>0.761811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>0.624620</td>\n",
       "      <td>0.613687</td>\n",
       "      <td>0.679687</td>\n",
       "      <td>0.721758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658692</td>\n",
       "      <td>0.457107</td>\n",
       "      <td>0.469979</td>\n",
       "      <td>0.447111</td>\n",
       "      <td>0.642535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model name       Pretrained classifier name  \\\n",
       "2  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "0  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "1  aubmindlab/bert-base-arabertv2  2021-12-05-train-0.898193359375   \n",
       "\n",
       "           Dataset                                                 df  \\\n",
       "2             dart                                                ...   \n",
       "0   annotated_data       Region                                   ...   \n",
       "1  arabic_dialects                                                ...   \n",
       "\n",
       "       Loss  Macro F1  Macro precision  Macro recall  Accuracy  \n",
       "2  1.070189  0.764301         0.775940      0.766432  0.761811  \n",
       "0  1.513531  0.624620         0.613687      0.679687  0.721758  \n",
       "1  1.658692  0.457107         0.469979      0.447111  0.642535  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Model name\", \"Pretrained classifier name\", \"Dataset\", \"df\", \"Loss\", \"Macro F1\", \"Macro precision\", \"Macro recall\", \"Accuracy\"])\n",
    "df_results.sort_values(\"Macro F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annotated_data</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>7.294701</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.123827</td>\n",
       "      <td>0.082429</td>\n",
       "      <td>0.068684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabic_dialects</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>6.864985</td>\n",
       "      <td>0.082905</td>\n",
       "      <td>0.139608</td>\n",
       "      <td>0.073939</td>\n",
       "      <td>0.079095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>6.710032</td>\n",
       "      <td>0.066968</td>\n",
       "      <td>0.074348</td>\n",
       "      <td>0.066111</td>\n",
       "      <td>0.067589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model name  \\\n",
       "Dataset                                            \n",
       "annotated_data   aubmindlab/bert-large-arabertv2   \n",
       "arabic_dialects  aubmindlab/bert-large-arabertv2   \n",
       "dart             aubmindlab/bert-large-arabertv2   \n",
       "\n",
       "                                        Pretrained classifier name  \\\n",
       "Dataset                                                              \n",
       "annotated_data   2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "arabic_dialects  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "dart             2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "\n",
       "                         Dataset  \\\n",
       "Dataset                            \n",
       "annotated_data    annotated_data   \n",
       "arabic_dialects  arabic_dialects   \n",
       "dart                        dart   \n",
       "\n",
       "                                                                df      Loss  \\\n",
       "Dataset                                                                        \n",
       "annotated_data        Region                                   ...  7.294701   \n",
       "arabic_dialects                                                ...  6.864985   \n",
       "dart                                                           ...  6.710032   \n",
       "\n",
       "                 Macro F1  Macro precision  Macro recall  Accuracy  \n",
       "Dataset                                                             \n",
       "annotated_data   0.063544         0.123827      0.082429  0.068684  \n",
       "arabic_dialects  0.082905         0.139608      0.073939  0.079095  \n",
       "dart             0.066968         0.074348      0.066111  0.067589  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      "          Model name &         Dataset &  Macro F1 &  Macro precision &  Macro recall \\\\\n",
      "bert-large-arabertv2 &  annotated\\_data &  0.063544 &         0.123827 &      0.082429 \\\\\n",
      "\\midrule\n",
      "bert-large-arabertv2 & arabic\\_dialects &  0.082905 &         0.139608 &      0.073939 \\\\\n",
      "bert-large-arabertv2 &            dart &  0.066968 &         0.074348 &      0.066111 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/307787898.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_displayed[cols].to_latex(index=False,))\n"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1]).copy()\n",
    "cols = [\"Model name\", \"Dataset\", \"Macro F1\", \"Macro precision\", \"Macro recall\"]\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "print(df_displayed[cols].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "                     Model name &  SMADC Accuracy \\\\\n",
      "aubmindlab/bert-large-arabertv2 &        0.946777 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/3835315148.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_acc[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "df_acc = df_results.groupby(\"Model name\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1]).copy()\n",
    "# df_acc[\"Model name\"] = df_acc[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_acc[\"SMADC Accuracy\"] = df_acc[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "print(df_acc[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "          Model name &  SMADC Accuracy \\\\\n",
      "\\midrule\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "bert-large-arabertv2 &        0.946777 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/2447509516.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_displayed[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False,))\n"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results[[\"Model name\", \"Pretrained classifier name\"]].copy()\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "\n",
    "print(df_displayed[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>SMADC Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-03-23-train-bert-large-arabertv2-0.946777...</td>\n",
       "      <td>0.946777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model name                         Pretrained classifier name  \\\n",
       "0  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "1  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "2  bert-large-arabertv2  2022-03-23-train-bert-large-arabertv2-0.946777...   \n",
       "\n",
       "   SMADC Accuracy  \n",
       "0        0.946777  \n",
       "1        0.946777  \n",
       "2        0.946777  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMADC evaluation per class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = False \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.013 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "model_name = \"aubmindlab/bert-large-arabertv2\"\n",
    "from_pretrained_classifier: bool = False\n",
    "pretrained_classifier_name: str = \"2021-12-05-train-0.898193359375\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 4\n",
    "warmup_ratio: float = 0.2\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = True\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[2022-03-29 19:24:23,444 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\36c5494fd851799740cb1dd3e94d010bad805cda3464b8e78a834d46e9d9f7a8.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\a7774340a9d7adbbae7d9d6f511e1e501684c13f7296bec4029a85938b157155.80a4303e73f2866371cbb896f25127a7a17110f071a37939f044882b3258b027\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b23b65a31311082d9b2031bee1f67d74bb76b438db863eeb8b9abad07f96452.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\e8c5e2a933856e2ffa93a0acf00d00bc27d06a7d01ea95cfe3ee88182d9fd191.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\f0fbe70f788dc9ff505358beebbf70a8e87a4a7c17f81ba1b8692c42d05e9182.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-large-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\36c5494fd851799740cb1dd3e94d010bad805cda3464b8e78a834d46e9d9f7a8.4e430221c6308a357f8f5ff50aa75690d79c5e48e583c66b9d9ea9845944e8e5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", \"2021-12-05-train-0.898193359375\")\n",
    "pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test(model_name_, pretrained_classifier_name_, df_)\n",
    "# df = get_SMADC_folder_data()\n",
    "# df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "# train, test = train_test_split(df, test_size=test_validation_proportion, random_state=1)\n",
    "# validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=1)\n",
    "# train.reset_index(drop=True, inplace=True)\n",
    "# validate.reset_index(drop=True, inplace=True)\n",
    "# test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Tokenize\n",
    "# validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "# test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "# train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "test[\"Labels\"] = test[\"Region\"].apply(class_to_index.get)\n",
    "validate[\"Labels\"] = validate[\"Region\"].apply(class_to_index.get)\n",
    "train[\"Labels\"] = train[\"Region\"].apply(class_to_index.get)\n",
    "\n",
    "# Make Dataset \n",
    "validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14214\n",
      "  Batch size = 64\n",
      "100%|██████████| 223/223 [00:26<00:00,  7.18it/s]Trainer is attempting to log a value of \"{'NOR': {'precision': 0.9333740831295844, 'recall': 0.9666086406076911, 'f1-score': 0.9497006919070202, 'support': 6319}, 'IRQ': {'precision': 0.8625565610859729, 'recall': 0.8630447085455575, 'f1-score': 0.8628005657708628, 'support': 1767}, 'LEV': {'precision': 0.8424396442185514, 'recall': 0.8236024844720496, 'f1-score': 0.8329145728643216, 'support': 1610}, 'EGY': {'precision': 0.8195848855774348, 'recall': 0.7754279959718026, 'f1-score': 0.7968952134540751, 'support': 1986}, 'GLF': {'precision': 0.8664761126990609, 'recall': 0.8380726698262243, 'f1-score': 0.852037743425015, 'support': 2532}, 'accuracy': 0.8879273955255382, 'macro avg': {'precision': 0.8648862573421209, 'recall': 0.853351299884665, 'f1-score': 0.858869757484259, 'support': 14214}, 'weighted avg': {'precision': 0.8864548474769547, 'recall': 0.8879273955255382, 'f1-score': 0.8869222734248672, 'support': 14214}}\" of type <class 'dict'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|██████████| 223/223 [00:32<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "res = Trainer(\n",
    "        model=pretrained_classifier, \n",
    "        compute_metrics=compute_metrics, \n",
    "        args=generate_training_args(\"models\", do_warmup=False, batch_size=batch_size)\n",
    "    ).evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame([res[\"eval_report\"][class_].values() for class_ in classes], index=classes, columns=[\"Precision\", \"Recall\", \"F1-Score\", \"Samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Precision &    Recall &  F1-Score \\\\\n",
      "\\midrule\n",
      "EGY &   0.819585 &  0.775428 &  0.796895 \\\\\n",
      "GLF &   0.866476 &  0.838073 &  0.852038 \\\\\n",
      "IRQ &   0.862557 &  0.863045 &  0.862801 \\\\\n",
      "LEV &   0.842440 &  0.823602 &  0.832915 \\\\\n",
      "NOR &   0.933374 &  0.966609 &  0.949701 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohnd\\AppData\\Local\\Temp/ipykernel_16064/1243165913.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(evaluation[[\"Precision\", \"Recall\", \"F1-Score\"]].to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(evaluation[[\"Precision\", \"Recall\", \"F1-Score\"]].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbuf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcol_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NaN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mformatters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msparsify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbold_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcolumn_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlongtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mescape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmulticolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmulticolumn_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmultirow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcaption\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Render object to a LaTeX tabular, longtable, or nested table.\n",
      "\n",
      "Requires ``\\usepackage{booktabs}``.  The output can be copy/pasted\n",
      "into a main LaTeX document or read from an external file\n",
      "with ``\\input{table.tex}``.\n",
      "\n",
      ".. versionchanged:: 1.0.0\n",
      "   Added caption and label arguments.\n",
      "\n",
      ".. versionchanged:: 1.2.0\n",
      "   Added position argument, changed meaning of caption argument.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "buf : str, Path or StringIO-like, optional, default None\n",
      "    Buffer to write to. If None, the output is returned as a string.\n",
      "columns : list of label, optional\n",
      "    The subset of columns to write. Writes all columns by default.\n",
      "col_space : int, optional\n",
      "    The minimum width of each column.\n",
      "header : bool or list of str, default True\n",
      "    Write out the column names. If a list of strings is given,\n",
      "    it is assumed to be aliases for the column names.\n",
      "index : bool, default True\n",
      "    Write row names (index).\n",
      "na_rep : str, default 'NaN'\n",
      "    Missing data representation.\n",
      "formatters : list of functions or dict of {str: function}, optional\n",
      "    Formatter functions to apply to columns' elements by position or\n",
      "    name. The result of each function must be a unicode string.\n",
      "    List must be of length equal to the number of columns.\n",
      "float_format : one-parameter function or str, optional, default None\n",
      "    Formatter for floating point numbers. For example\n",
      "    ``float_format=\"%.2f\"`` and ``float_format=\"{:0.2f}\".format`` will\n",
      "    both result in 0.1234 being formatted as 0.12.\n",
      "sparsify : bool, optional\n",
      "    Set to False for a DataFrame with a hierarchical index to print\n",
      "    every multiindex key at each row. By default, the value will be\n",
      "    read from the config module.\n",
      "index_names : bool, default True\n",
      "    Prints the names of the indexes.\n",
      "bold_rows : bool, default False\n",
      "    Make the row labels bold in the output.\n",
      "column_format : str, optional\n",
      "    The columns format as specified in `LaTeX table format\n",
      "    <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3\n",
      "    columns. By default, 'l' will be used for all columns except\n",
      "    columns of numbers, which default to 'r'.\n",
      "longtable : bool, optional\n",
      "    By default, the value will be read from the pandas config\n",
      "    module. Use a longtable environment instead of tabular. Requires\n",
      "    adding a \\usepackage{longtable} to your LaTeX preamble.\n",
      "escape : bool, optional\n",
      "    By default, the value will be read from the pandas config\n",
      "    module. When set to False prevents from escaping latex special\n",
      "    characters in column names.\n",
      "encoding : str, optional\n",
      "    A string representing the encoding to use in the output file,\n",
      "    defaults to 'utf-8'.\n",
      "decimal : str, default '.'\n",
      "    Character recognized as decimal separator, e.g. ',' in Europe.\n",
      "multicolumn : bool, default True\n",
      "    Use \\multicolumn to enhance MultiIndex columns.\n",
      "    The default will be read from the config module.\n",
      "multicolumn_format : str, default 'l'\n",
      "    The alignment for multicolumns, similar to `column_format`\n",
      "    The default will be read from the config module.\n",
      "multirow : bool, default False\n",
      "    Use \\multirow to enhance MultiIndex rows. Requires adding a\n",
      "    \\usepackage{multirow} to your LaTeX preamble. Will print\n",
      "    centered labels (instead of top-aligned) across the contained\n",
      "    rows, separating groups via clines. The default will be read\n",
      "    from the pandas config module.\n",
      "caption : str or tuple, optional\n",
      "    Tuple (full_caption, short_caption),\n",
      "    which results in ``\\caption[short_caption]{full_caption}``;\n",
      "    if a single string is passed, no short caption will be set.\n",
      "\n",
      "    .. versionadded:: 1.0.0\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "       Optionally allow caption to be a tuple ``(full_caption, short_caption)``.\n",
      "\n",
      "label : str, optional\n",
      "    The LaTeX label to be placed inside ``\\label{}`` in the output.\n",
      "    This is used with ``\\ref{}`` in the main ``.tex`` file.\n",
      "\n",
      "    .. versionadded:: 1.0.0\n",
      "position : str, optional\n",
      "    The LaTeX positional argument for tables, to be placed after\n",
      "    ``\\begin{}`` in the output.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        str or None\n",
      "            If buf is None, returns the result as a string. Otherwise returns\n",
      "            None.\n",
      "    \n",
      "See Also\n",
      "--------\n",
      "Styler.to_latex : Render a DataFrame to LaTeX with conditional formatting.\n",
      "DataFrame.to_string : Render a DataFrame to a console-friendly\n",
      "    tabular output.\n",
      "DataFrame.to_html : Render a DataFrame as an HTML table.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n",
      "...                   mask=['red', 'purple'],\n",
      "...                   weapon=['sai', 'bo staff']))\n",
      ">>> print(df.to_latex(index=False))  # doctest: +SKIP\n",
      "\\begin{tabular}{lll}\n",
      " \\toprule\n",
      "       name &    mask &    weapon \\\\\n",
      " \\midrule\n",
      "    Raphael &     red &       sai \\\\\n",
      "  Donatello &  purple &  bo staff \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mohnd\\appdata\\roaming\\python\\python39\\site-packages\\pandas\\core\\generic.py\n",
      "\u001b[1;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "evaluation.to_latex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graduation_project')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
