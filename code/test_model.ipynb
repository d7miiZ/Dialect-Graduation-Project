{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params\n",
    "sequence_length: int = 32 # Not sure what happens when setting it with a number different from what its trained on\n",
    "\n",
    "# Model \n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "# model_name: str = \"aubmindlab/bert-large-arabertv2\"\n",
    "model_names = [\n",
    "    \"aubmindlab/bert-large-arabertv2\",\n",
    "    \"aubmindlab/bert-base-arabertv2\",\n",
    "    \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "]\n",
    "\n",
    "pretrained_classifier_name: str = \"2021-09-30-train-0.8921535648994515\"\n",
    "pretrained_classifier_names = [name[name.rindex(\"\\\\\")+1:] for name in glob(\"models\\\\finalized_models\\\\*\")]\n",
    "pretrained_classifier_names.pop(-1)\n",
    "\n",
    "# Data\n",
    "df: pd.DataFrame = get_annotated_data_folder_data()\n",
    "dfs = {\n",
    "    \"annotated_data\": get_annotated_data_folder_data(),\n",
    "    \"arabic_dialects\": get_arabic_dialects_dataset_folder_data(),\n",
    "    \"dart\": get_dart_folder_data()\n",
    "}\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "batch_size: int = 128\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_name, pretrained_classifier_name, df):\n",
    "    # Model\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "    arabert_prep = ArabertPreprocessor(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Preprocessing\n",
    "    temp_df = get_SMADC_folder_data(code_folder_path)\n",
    "    classes = temp_df[\"Region\"].unique()\n",
    "    num_labels = len(classes)\n",
    "    class_to_index = {class_:index for class_, index in zip(classes, range(num_labels))}\n",
    "    index_to_class = {index:class_ for class_, index in zip(classes, range(num_labels))}\n",
    "    temp_df[\"Labels\"] = temp_df[\"Region\"].apply(class_to_index.get)\n",
    "\n",
    "    df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "    df_encoding = tokenize(tokenizer, df[\"Text\"].to_list(), sequence_length)\n",
    "\n",
    "    test_set = Dialect_dataset(df_encoding, df[\"Labels\"].to_list())\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=pretrained_classifier, \n",
    "        compute_metrics=compute_metrics, \n",
    "        args=generate_training_args(\"models\", do_warmup=False, batch_size=batch_size)\n",
    "    )\n",
    "    prediction = trainer.predict(test_set)\n",
    "    return prediction.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "results = []\n",
    "for model_name_ in model_names:\n",
    "    for pretrained_classifier_name_ in pretrained_classifier_names:\n",
    "        for name_df, df_ in dfs.items():\n",
    "            r = test(model_name_, pretrained_classifier_name_, df_)\n",
    "            results.append((model_name_, pretrained_classifier_name_, name_df, df_, \n",
    "                 r[\"test_loss\"], r[\"test_macro_f1\"], r[\"test_macro_precision\"], r[\"test_macro_recall\"], r[\"test_accuracy\"]\n",
    "            ))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-09-30-train-0.8921535648994515.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:15<00:00, 13.62it/s]\n",
      "loading configuration file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-09-30-train-0.8921535648994515.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:05<00:00, 13.45it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:05<00:00, 13.47it/s]\n",
      "loading configuration file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-09-30-train-0.8921535648994515\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-09-30-train-0.8921535648994515.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:14<00:00, 13.45it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-04-train-0.87158203125\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-04-train-0.87158203125\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-04-train-0.87158203125.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:40<00:00,  5.18it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-04-train-0.87158203125\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-04-train-0.87158203125\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-04-train-0.87158203125.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:12<00:00,  5.67it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:12<00:00,  5.76it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-04-train-0.87158203125\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-04-train-0.87158203125\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-04-train-0.87158203125.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:34<00:00,  5.52it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:37<00:00,  5.61it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:12<00:00,  4.77it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:12<00:00,  5.61it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-05-train-0.898193359375\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-large-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NOR\",\n",
      "    \"1\": \"EGY\",\n",
      "    \"2\": \"LEV\",\n",
      "    \"3\": \"GLF\",\n",
      "    \"4\": \"IRQ\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 1,\n",
      "    \"GLF\": 3,\n",
      "    \"IRQ\": 4,\n",
      "    \"LEV\": 2,\n",
      "    \"NOR\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-05-train-0.898193359375\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-05-train-0.898193359375.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:33<00:00,  5.67it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-09-train-0.963134765625\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-08-train-0.943115234375\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-09-train-0.963134765625\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-09-train-0.963134765625.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:39<00:00,  5.33it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-09-train-0.963134765625\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-08-train-0.943115234375\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-09-train-0.963134765625\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-09-train-0.963134765625.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:12<00:00,  5.59it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:12<00:00,  5.72it/s]\n",
      "loading configuration file models\\finalized_models\\2021-12-09-train-0.963134765625\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-08-train-0.943115234375\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2021-12-09-train-0.963134765625\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2021-12-09-train-0.963134765625.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:33<00:00,  5.62it/s]\n",
      "loading configuration file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-09-train-0.963134765625\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2022-01-29-train-0.826416015625-twitter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 26804\n",
      "  Batch size = 128\n",
      "100%|██████████| 210/210 [00:37<00:00,  5.63it/s]\n",
      "loading configuration file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-09-train-0.963134765625\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2022-01-29-train-0.826416015625-twitter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9103\n",
      "  Batch size = 128\n",
      " 99%|█████████▊| 71/72 [00:14<00:00,  4.97it/s]D:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 72/72 [00:14<00:00,  5.08it/s]\n",
      "loading configuration file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models\\\\finalized_models\\\\2021-12-09-train-0.963134765625\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"NOR\",\n",
      "    \"3\": \"IRQ\",\n",
      "    \"4\": \"LEV\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 3,\n",
      "    \"LEV\": 4,\n",
      "    \"NOR\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models\\finalized_models\\2022-01-29-train-0.826416015625-twitter\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models\\finalized_models\\2022-01-29-train-0.826416015625-twitter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\dbef00ddc9b64a66ba8057785b166b744cef2a41be973446ad897a56ad317019.aa4ad61e3b0a52c7bcf5410af86ef01a27cf1147665acd6bfba80731d053f78a\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\46fef3ab20b06df535befe0412ab892f9baec0a9f8e64d75a0142a67ce366959.c7c33ce0611a0a55c52a9ba4c03992b47db6e8b9862113443132ed9af7185a19\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\7f74425f6809cddb05d5de7967a5af4e325b04245017a7b1917fe7d5cfb06988.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv02-twitter/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\582bc76b2b3acaaf545878170de8fbf8d6d1f65bd0180769ff4ed901cd60d3c4.9badb1b6af7f7e89d855c8fbc79dd73ef57ac1c9e573a43862ddaeb2c798a290\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running Prediction *****\n",
      "  Num examples = 24279\n",
      "  Batch size = 128\n",
      "100%|██████████| 190/190 [00:38<00:00,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for model_name_ in [\"aubmindlab/bert-base-arabertv02-twitter\"]:\n",
    "    for pretrained_classifier_name_ in pretrained_classifier_names:\n",
    "        for name_df, df_ in dfs.items():\n",
    "            r = test(model_name_, pretrained_classifier_name_, df_)\n",
    "            results.append((model_name_, pretrained_classifier_name_, name_df, df_, \n",
    "                 r[\"test_loss\"], r[\"test_macro_f1\"], r[\"test_macro_precision\"], r[\"test_macro_recall\"], r[\"test_accuracy\"]\n",
    "            ))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783378</td>\n",
       "      <td>0.760816</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.759033</td>\n",
       "      <td>0.756127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788159</td>\n",
       "      <td>0.760212</td>\n",
       "      <td>0.765969</td>\n",
       "      <td>0.758317</td>\n",
       "      <td>0.755344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224970</td>\n",
       "      <td>0.751637</td>\n",
       "      <td>0.767417</td>\n",
       "      <td>0.751757</td>\n",
       "      <td>0.748136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114726</td>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.732053</td>\n",
       "      <td>0.732794</td>\n",
       "      <td>0.727254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114564</td>\n",
       "      <td>0.731025</td>\n",
       "      <td>0.731854</td>\n",
       "      <td>0.732672</td>\n",
       "      <td>0.727089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323409</td>\n",
       "      <td>0.702196</td>\n",
       "      <td>0.703231</td>\n",
       "      <td>0.704094</td>\n",
       "      <td>0.699040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.668126</td>\n",
       "      <td>0.608431</td>\n",
       "      <td>0.597582</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.702917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.051848</td>\n",
       "      <td>0.593083</td>\n",
       "      <td>0.596362</td>\n",
       "      <td>0.653978</td>\n",
       "      <td>0.700269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.052340</td>\n",
       "      <td>0.591879</td>\n",
       "      <td>0.595272</td>\n",
       "      <td>0.653304</td>\n",
       "      <td>0.698814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.626672</td>\n",
       "      <td>0.572821</td>\n",
       "      <td>0.575651</td>\n",
       "      <td>0.642667</td>\n",
       "      <td>0.675832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.626997</td>\n",
       "      <td>0.572733</td>\n",
       "      <td>0.575499</td>\n",
       "      <td>0.642570</td>\n",
       "      <td>0.675683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.874559</td>\n",
       "      <td>0.548745</td>\n",
       "      <td>0.552640</td>\n",
       "      <td>0.611583</td>\n",
       "      <td>0.650425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.839932</td>\n",
       "      <td>0.449060</td>\n",
       "      <td>0.459210</td>\n",
       "      <td>0.440770</td>\n",
       "      <td>0.635834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>5.228093</td>\n",
       "      <td>0.436698</td>\n",
       "      <td>0.438582</td>\n",
       "      <td>0.438772</td>\n",
       "      <td>0.392314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>5.228151</td>\n",
       "      <td>0.436686</td>\n",
       "      <td>0.438552</td>\n",
       "      <td>0.438772</td>\n",
       "      <td>0.392314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151310</td>\n",
       "      <td>0.434327</td>\n",
       "      <td>0.474211</td>\n",
       "      <td>0.420434</td>\n",
       "      <td>0.629133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.152616</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.472570</td>\n",
       "      <td>0.418936</td>\n",
       "      <td>0.627376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.797925</td>\n",
       "      <td>0.416338</td>\n",
       "      <td>0.447406</td>\n",
       "      <td>0.406124</td>\n",
       "      <td>0.604416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.798437</td>\n",
       "      <td>0.416113</td>\n",
       "      <td>0.447165</td>\n",
       "      <td>0.405880</td>\n",
       "      <td>0.604196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>2.141354</td>\n",
       "      <td>0.400781</td>\n",
       "      <td>0.429551</td>\n",
       "      <td>0.388206</td>\n",
       "      <td>0.585851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>5.881700</td>\n",
       "      <td>0.328641</td>\n",
       "      <td>0.317336</td>\n",
       "      <td>0.414216</td>\n",
       "      <td>0.344762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>5.881880</td>\n",
       "      <td>0.328590</td>\n",
       "      <td>0.317254</td>\n",
       "      <td>0.414194</td>\n",
       "      <td>0.344725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>2.632432</td>\n",
       "      <td>0.291799</td>\n",
       "      <td>0.270334</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.309692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>7.001749</td>\n",
       "      <td>0.214540</td>\n",
       "      <td>0.237986</td>\n",
       "      <td>0.221853</td>\n",
       "      <td>0.212128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>7.001344</td>\n",
       "      <td>0.214514</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.221853</td>\n",
       "      <td>0.212128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>2.816528</td>\n",
       "      <td>0.207853</td>\n",
       "      <td>0.211128</td>\n",
       "      <td>0.293659</td>\n",
       "      <td>0.239852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246858</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>0.221535</td>\n",
       "      <td>0.245530</td>\n",
       "      <td>0.225215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246838</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>0.221535</td>\n",
       "      <td>0.245530</td>\n",
       "      <td>0.225215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>2.210538</td>\n",
       "      <td>0.188805</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>0.226297</td>\n",
       "      <td>0.249105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>2.210515</td>\n",
       "      <td>0.188805</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>0.226297</td>\n",
       "      <td>0.249105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>6.229696</td>\n",
       "      <td>0.182311</td>\n",
       "      <td>0.197071</td>\n",
       "      <td>0.198713</td>\n",
       "      <td>0.193542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>3.997659</td>\n",
       "      <td>0.179672</td>\n",
       "      <td>0.195517</td>\n",
       "      <td>0.195414</td>\n",
       "      <td>0.191070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176285</td>\n",
       "      <td>0.160425</td>\n",
       "      <td>0.185560</td>\n",
       "      <td>0.175136</td>\n",
       "      <td>0.170823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>3.553226</td>\n",
       "      <td>0.159429</td>\n",
       "      <td>0.189109</td>\n",
       "      <td>0.196609</td>\n",
       "      <td>0.191112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>2.752270</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>0.192154</td>\n",
       "      <td>0.200093</td>\n",
       "      <td>0.195148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>4.484740</td>\n",
       "      <td>0.146962</td>\n",
       "      <td>0.193203</td>\n",
       "      <td>0.138919</td>\n",
       "      <td>0.173020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>2.326163</td>\n",
       "      <td>0.145124</td>\n",
       "      <td>0.202456</td>\n",
       "      <td>0.173612</td>\n",
       "      <td>0.181808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>2.326201</td>\n",
       "      <td>0.145124</td>\n",
       "      <td>0.202456</td>\n",
       "      <td>0.173612</td>\n",
       "      <td>0.181808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>6.578391</td>\n",
       "      <td>0.144235</td>\n",
       "      <td>0.198348</td>\n",
       "      <td>0.196159</td>\n",
       "      <td>0.151918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>6.416016</td>\n",
       "      <td>0.141494</td>\n",
       "      <td>0.211775</td>\n",
       "      <td>0.140998</td>\n",
       "      <td>0.146325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>4.316211</td>\n",
       "      <td>0.140676</td>\n",
       "      <td>0.184433</td>\n",
       "      <td>0.188464</td>\n",
       "      <td>0.159193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>3.972376</td>\n",
       "      <td>0.124613</td>\n",
       "      <td>0.189088</td>\n",
       "      <td>0.219505</td>\n",
       "      <td>0.134122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732191</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.201959</td>\n",
       "      <td>0.130710</td>\n",
       "      <td>0.171482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>2.805301</td>\n",
       "      <td>0.122314</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>0.204330</td>\n",
       "      <td>0.158558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>aubmindlab/bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>3.962030</td>\n",
       "      <td>0.110142</td>\n",
       "      <td>0.179051</td>\n",
       "      <td>0.099146</td>\n",
       "      <td>0.117873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model name  \\\n",
       "20           aubmindlab/bert-base-arabertv2   \n",
       "5           aubmindlab/bert-large-arabertv2   \n",
       "2           aubmindlab/bert-large-arabertv2   \n",
       "8           aubmindlab/bert-large-arabertv2   \n",
       "23           aubmindlab/bert-base-arabertv2   \n",
       "17           aubmindlab/bert-base-arabertv2   \n",
       "0           aubmindlab/bert-large-arabertv2   \n",
       "3           aubmindlab/bert-large-arabertv2   \n",
       "18           aubmindlab/bert-base-arabertv2   \n",
       "21           aubmindlab/bert-base-arabertv2   \n",
       "6           aubmindlab/bert-large-arabertv2   \n",
       "15           aubmindlab/bert-base-arabertv2   \n",
       "1           aubmindlab/bert-large-arabertv2   \n",
       "26           aubmindlab/bert-base-arabertv2   \n",
       "11          aubmindlab/bert-large-arabertv2   \n",
       "4           aubmindlab/bert-large-arabertv2   \n",
       "19           aubmindlab/bert-base-arabertv2   \n",
       "22           aubmindlab/bert-base-arabertv2   \n",
       "7           aubmindlab/bert-large-arabertv2   \n",
       "16           aubmindlab/bert-base-arabertv2   \n",
       "9           aubmindlab/bert-large-arabertv2   \n",
       "24           aubmindlab/bert-base-arabertv2   \n",
       "44  aubmindlab/bert-base-arabertv02-twitter   \n",
       "10          aubmindlab/bert-large-arabertv2   \n",
       "25           aubmindlab/bert-base-arabertv2   \n",
       "42  aubmindlab/bert-base-arabertv02-twitter   \n",
       "14          aubmindlab/bert-large-arabertv2   \n",
       "29           aubmindlab/bert-base-arabertv2   \n",
       "12          aubmindlab/bert-large-arabertv2   \n",
       "27           aubmindlab/bert-base-arabertv2   \n",
       "41  aubmindlab/bert-base-arabertv02-twitter   \n",
       "38  aubmindlab/bert-base-arabertv02-twitter   \n",
       "43  aubmindlab/bert-base-arabertv02-twitter   \n",
       "32  aubmindlab/bert-base-arabertv02-twitter   \n",
       "35  aubmindlab/bert-base-arabertv02-twitter   \n",
       "37  aubmindlab/bert-base-arabertv02-twitter   \n",
       "28           aubmindlab/bert-base-arabertv2   \n",
       "13          aubmindlab/bert-large-arabertv2   \n",
       "39  aubmindlab/bert-base-arabertv02-twitter   \n",
       "40  aubmindlab/bert-base-arabertv02-twitter   \n",
       "36  aubmindlab/bert-base-arabertv02-twitter   \n",
       "30  aubmindlab/bert-base-arabertv02-twitter   \n",
       "34  aubmindlab/bert-base-arabertv02-twitter   \n",
       "33  aubmindlab/bert-base-arabertv02-twitter   \n",
       "31  aubmindlab/bert-base-arabertv02-twitter   \n",
       "\n",
       "                 Pretrained classifier name          Dataset  \\\n",
       "20           2021-12-04-train-0.87158203125             dart   \n",
       "5            2021-12-04-train-0.87158203125             dart   \n",
       "2       2021-09-30-train-0.8921535648994515             dart   \n",
       "8           2021-12-05-train-0.898193359375             dart   \n",
       "23          2021-12-05-train-0.898193359375             dart   \n",
       "17      2021-09-30-train-0.8921535648994515             dart   \n",
       "0       2021-09-30-train-0.8921535648994515   annotated_data   \n",
       "3            2021-12-04-train-0.87158203125   annotated_data   \n",
       "18           2021-12-04-train-0.87158203125   annotated_data   \n",
       "21          2021-12-05-train-0.898193359375   annotated_data   \n",
       "6           2021-12-05-train-0.898193359375   annotated_data   \n",
       "15      2021-09-30-train-0.8921535648994515   annotated_data   \n",
       "1       2021-09-30-train-0.8921535648994515  arabic_dialects   \n",
       "26          2021-12-09-train-0.963134765625             dart   \n",
       "11          2021-12-09-train-0.963134765625             dart   \n",
       "4            2021-12-04-train-0.87158203125  arabic_dialects   \n",
       "19           2021-12-04-train-0.87158203125  arabic_dialects   \n",
       "22          2021-12-05-train-0.898193359375  arabic_dialects   \n",
       "7           2021-12-05-train-0.898193359375  arabic_dialects   \n",
       "16      2021-09-30-train-0.8921535648994515  arabic_dialects   \n",
       "9           2021-12-09-train-0.963134765625   annotated_data   \n",
       "24          2021-12-09-train-0.963134765625   annotated_data   \n",
       "44  2022-01-29-train-0.826416015625-twitter             dart   \n",
       "10          2021-12-09-train-0.963134765625  arabic_dialects   \n",
       "25          2021-12-09-train-0.963134765625  arabic_dialects   \n",
       "42  2022-01-29-train-0.826416015625-twitter   annotated_data   \n",
       "14  2022-01-29-train-0.826416015625-twitter             dart   \n",
       "29  2022-01-29-train-0.826416015625-twitter             dart   \n",
       "12  2022-01-29-train-0.826416015625-twitter   annotated_data   \n",
       "27  2022-01-29-train-0.826416015625-twitter   annotated_data   \n",
       "41          2021-12-09-train-0.963134765625             dart   \n",
       "38          2021-12-05-train-0.898193359375             dart   \n",
       "43  2022-01-29-train-0.826416015625-twitter  arabic_dialects   \n",
       "32      2021-09-30-train-0.8921535648994515             dart   \n",
       "35           2021-12-04-train-0.87158203125             dart   \n",
       "37          2021-12-05-train-0.898193359375  arabic_dialects   \n",
       "28  2022-01-29-train-0.826416015625-twitter  arabic_dialects   \n",
       "13  2022-01-29-train-0.826416015625-twitter  arabic_dialects   \n",
       "39          2021-12-09-train-0.963134765625   annotated_data   \n",
       "40          2021-12-09-train-0.963134765625  arabic_dialects   \n",
       "36          2021-12-05-train-0.898193359375   annotated_data   \n",
       "30      2021-09-30-train-0.8921535648994515   annotated_data   \n",
       "34           2021-12-04-train-0.87158203125  arabic_dialects   \n",
       "33           2021-12-04-train-0.87158203125   annotated_data   \n",
       "31      2021-09-30-train-0.8921535648994515  arabic_dialects   \n",
       "\n",
       "                                                   df      Loss  Macro F1  \\\n",
       "20                                                ...  0.783378  0.760816   \n",
       "5                                                 ...  0.788159  0.760212   \n",
       "2                                                 ...  1.224970  0.751637   \n",
       "8                                                 ...  1.114726  0.731175   \n",
       "23                                                ...  1.114564  0.731025   \n",
       "17                                                ...  1.323409  0.702196   \n",
       "0        Region                                   ...  1.668126  0.608431   \n",
       "3        Region                                   ...  1.051848  0.593083   \n",
       "18       Region                                   ...  1.052340  0.591879   \n",
       "21       Region                                   ...  1.626672  0.572821   \n",
       "6        Region                                   ...  1.626997  0.572733   \n",
       "15       Region                                   ...  1.874559  0.548745   \n",
       "1                                                 ...  1.839932  0.449060   \n",
       "26                                                ...  5.228093  0.436698   \n",
       "11                                                ...  5.228151  0.436686   \n",
       "4                                                 ...  1.151310  0.434327   \n",
       "19                                                ...  1.152616  0.432234   \n",
       "22                                                ...  1.797925  0.416338   \n",
       "7                                                 ...  1.798437  0.416113   \n",
       "16                                                ...  2.141354  0.400781   \n",
       "9        Region                                   ...  5.881700  0.328641   \n",
       "24       Region                                   ...  5.881880  0.328590   \n",
       "44                                                ...  2.632432  0.291799   \n",
       "10                                                ...  7.001749  0.214540   \n",
       "25                                                ...  7.001344  0.214514   \n",
       "42       Region                                   ...  2.816528  0.207853   \n",
       "14                                                ...  2.246858  0.207807   \n",
       "29                                                ...  2.246838  0.207807   \n",
       "12       Region                                   ...  2.210538  0.188805   \n",
       "27       Region                                   ...  2.210515  0.188805   \n",
       "41                                                ...  6.229696  0.182311   \n",
       "38                                                ...  3.997659  0.179672   \n",
       "43                                                ...  3.176285  0.160425   \n",
       "32                                                ...  3.553226  0.159429   \n",
       "35                                                ...  2.752270  0.154599   \n",
       "37                                                ...  4.484740  0.146962   \n",
       "28                                                ...  2.326163  0.145124   \n",
       "13                                                ...  2.326201  0.145124   \n",
       "39       Region                                   ...  6.578391  0.144235   \n",
       "40                                                ...  6.416016  0.141494   \n",
       "36       Region                                   ...  4.316211  0.140676   \n",
       "30       Region                                   ...  3.972376  0.124613   \n",
       "34                                                ...  2.732191  0.123264   \n",
       "33       Region                                   ...  2.805301  0.122314   \n",
       "31                                                ...  3.962030  0.110142   \n",
       "\n",
       "    Macro precision  Macro recall  Accuracy  \n",
       "20         0.766500      0.759033  0.756127  \n",
       "5          0.765969      0.758317  0.755344  \n",
       "2          0.767417      0.751757  0.748136  \n",
       "8          0.732053      0.732794  0.727254  \n",
       "23         0.731854      0.732672  0.727089  \n",
       "17         0.703231      0.704094  0.699040  \n",
       "0          0.597582      0.660434  0.702917  \n",
       "3          0.596362      0.653978  0.700269  \n",
       "18         0.595272      0.653304  0.698814  \n",
       "21         0.575651      0.642667  0.675832  \n",
       "6          0.575499      0.642570  0.675683  \n",
       "15         0.552640      0.611583  0.650425  \n",
       "1          0.459210      0.440770  0.635834  \n",
       "26         0.438582      0.438772  0.392314  \n",
       "11         0.438552      0.438772  0.392314  \n",
       "4          0.474211      0.420434  0.629133  \n",
       "19         0.472570      0.418936  0.627376  \n",
       "22         0.447406      0.406124  0.604416  \n",
       "7          0.447165      0.405880  0.604196  \n",
       "16         0.429551      0.388206  0.585851  \n",
       "9          0.317336      0.414216  0.344762  \n",
       "24         0.317254      0.414194  0.344725  \n",
       "44         0.270334      0.320980  0.309692  \n",
       "10         0.237986      0.221853  0.212128  \n",
       "25         0.237900      0.221853  0.212128  \n",
       "42         0.211128      0.293659  0.239852  \n",
       "14         0.221535      0.245530  0.225215  \n",
       "29         0.221535      0.245530  0.225215  \n",
       "12         0.218947      0.226297  0.249105  \n",
       "27         0.218947      0.226297  0.249105  \n",
       "41         0.197071      0.198713  0.193542  \n",
       "38         0.195517      0.195414  0.191070  \n",
       "43         0.185560      0.175136  0.170823  \n",
       "32         0.189109      0.196609  0.191112  \n",
       "35         0.192154      0.200093  0.195148  \n",
       "37         0.193203      0.138919  0.173020  \n",
       "28         0.202456      0.173612  0.181808  \n",
       "13         0.202456      0.173612  0.181808  \n",
       "39         0.198348      0.196159  0.151918  \n",
       "40         0.211775      0.140998  0.146325  \n",
       "36         0.184433      0.188464  0.159193  \n",
       "30         0.189088      0.219505  0.134122  \n",
       "34         0.201959      0.130710  0.171482  \n",
       "33         0.186589      0.204330  0.158558  \n",
       "31         0.179051      0.099146  0.117873  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Model name\", \"Pretrained classifier name\", \"Dataset\", \"df\", \"Loss\", \"Macro F1\", \"Macro precision\", \"Macro recall\", \"Accuracy\"])\n",
    "df_results.sort_values(\"Macro F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>df</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro precision</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annotated_data</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>annotated_data</td>\n",
       "      <td>Region                                   ...</td>\n",
       "      <td>1.668126</td>\n",
       "      <td>0.608431</td>\n",
       "      <td>0.597582</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.702917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabic_dialects</th>\n",
       "      <td>aubmindlab/bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "      <td>arabic_dialects</td>\n",
       "      <td>...</td>\n",
       "      <td>1.839932</td>\n",
       "      <td>0.449060</td>\n",
       "      <td>0.459210</td>\n",
       "      <td>0.440770</td>\n",
       "      <td>0.635834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>aubmindlab/bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "      <td>dart</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783378</td>\n",
       "      <td>0.760816</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.759033</td>\n",
       "      <td>0.756127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model name  \\\n",
       "Dataset                                            \n",
       "annotated_data   aubmindlab/bert-large-arabertv2   \n",
       "arabic_dialects  aubmindlab/bert-large-arabertv2   \n",
       "dart              aubmindlab/bert-base-arabertv2   \n",
       "\n",
       "                          Pretrained classifier name          Dataset  \\\n",
       "Dataset                                                                 \n",
       "annotated_data   2021-09-30-train-0.8921535648994515   annotated_data   \n",
       "arabic_dialects  2021-09-30-train-0.8921535648994515  arabic_dialects   \n",
       "dart                  2021-12-04-train-0.87158203125             dart   \n",
       "\n",
       "                                                                df      Loss  \\\n",
       "Dataset                                                                        \n",
       "annotated_data        Region                                   ...  1.668126   \n",
       "arabic_dialects                                                ...  1.839932   \n",
       "dart                                                           ...  0.783378   \n",
       "\n",
       "                 Macro F1  Macro precision  Macro recall  Accuracy  \n",
       "Dataset                                                             \n",
       "annotated_data   0.608431         0.597582      0.660434  0.702917  \n",
       "arabic_dialects  0.449060         0.459210      0.440770  0.635834  \n",
       "dart             0.760816         0.766500      0.759033  0.756127  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlrrr}\n",
      "\\toprule\n",
      "          Model name &  SMADC Accuracy &         Dataset &  Macro F1 &  Macro precision &  Macro recall \\\\\n",
      "bert-large-arabertv2 &        0.892154 &  annotated\\_data &  0.608431 &         0.597582 &      0.660434 \\\\\n",
      "\\midrule\n",
      "bert-large-arabertv2 &        0.892154 & arabic\\_dialects &  0.449060 &         0.459210 &      0.440770 \\\\\n",
      " bert-base-arabertv2 &        0.871582 &            dart &  0.760816 &         0.766500 &      0.759033 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results.groupby(\"Dataset\").apply(lambda df: df.sort_values(\"Macro F1\").iloc[-1]).copy()\n",
    "cols = [\"Model name\", \"SMADC Accuracy\", \"Dataset\", \"Macro F1\", \"Macro precision\", \"Macro recall\"]\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "print(df_displayed[cols].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'twitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26484/222969871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_displayed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Pretrained classifier name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SMADC Accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pretrained classifier name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SMADC Accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32mD:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwarez\\Anaconda\\envs\\graduation_project\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26484/222969871.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_displayed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Pretrained classifier name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SMADC Accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pretrained classifier name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_displayed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SMADC Accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'twitter'"
     ]
    }
   ],
   "source": [
    "df_displayed = df_results[[\"Model name\", \"Pretrained classifier name\"]].copy()\n",
    "df_displayed[\"Model name\"] = df_displayed[\"Model name\"].apply(lambda name: name[name.rindex(\"/\")+1:])\n",
    "df_displayed[\"SMADC Accuracy\"] = df_displayed[\"Pretrained classifier name\"].apply(lambda name: round(float(name[name.rindex(\"-\")+1:]), 6))\n",
    "\n",
    "print(df_displayed[[\"Model name\", \"SMADC Accuracy\"]].to_latex(index=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Pretrained classifier name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert-large-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bert-base-arabertv2</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-09-30-train-0.8921535648994515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-04-train-0.87158203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-05-train-0.898193359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2021-12-09-train-0.963134765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bert-base-arabertv02-twitter</td>\n",
       "      <td>2022-01-29-train-0.826416015625-twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model name               Pretrained classifier name\n",
       "0           bert-large-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "1           bert-large-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "2           bert-large-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "3           bert-large-arabertv2           2021-12-04-train-0.87158203125\n",
       "4           bert-large-arabertv2           2021-12-04-train-0.87158203125\n",
       "5           bert-large-arabertv2           2021-12-04-train-0.87158203125\n",
       "6           bert-large-arabertv2          2021-12-05-train-0.898193359375\n",
       "7           bert-large-arabertv2          2021-12-05-train-0.898193359375\n",
       "8           bert-large-arabertv2          2021-12-05-train-0.898193359375\n",
       "9           bert-large-arabertv2          2021-12-09-train-0.963134765625\n",
       "10          bert-large-arabertv2          2021-12-09-train-0.963134765625\n",
       "11          bert-large-arabertv2          2021-12-09-train-0.963134765625\n",
       "12          bert-large-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "13          bert-large-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "14          bert-large-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "15           bert-base-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "16           bert-base-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "17           bert-base-arabertv2      2021-09-30-train-0.8921535648994515\n",
       "18           bert-base-arabertv2           2021-12-04-train-0.87158203125\n",
       "19           bert-base-arabertv2           2021-12-04-train-0.87158203125\n",
       "20           bert-base-arabertv2           2021-12-04-train-0.87158203125\n",
       "21           bert-base-arabertv2          2021-12-05-train-0.898193359375\n",
       "22           bert-base-arabertv2          2021-12-05-train-0.898193359375\n",
       "23           bert-base-arabertv2          2021-12-05-train-0.898193359375\n",
       "24           bert-base-arabertv2          2021-12-09-train-0.963134765625\n",
       "25           bert-base-arabertv2          2021-12-09-train-0.963134765625\n",
       "26           bert-base-arabertv2          2021-12-09-train-0.963134765625\n",
       "27           bert-base-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "28           bert-base-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "29           bert-base-arabertv2  2022-01-29-train-0.826416015625-twitter\n",
       "30  bert-base-arabertv02-twitter      2021-09-30-train-0.8921535648994515\n",
       "31  bert-base-arabertv02-twitter      2021-09-30-train-0.8921535648994515\n",
       "32  bert-base-arabertv02-twitter      2021-09-30-train-0.8921535648994515\n",
       "33  bert-base-arabertv02-twitter           2021-12-04-train-0.87158203125\n",
       "34  bert-base-arabertv02-twitter           2021-12-04-train-0.87158203125\n",
       "35  bert-base-arabertv02-twitter           2021-12-04-train-0.87158203125\n",
       "36  bert-base-arabertv02-twitter          2021-12-05-train-0.898193359375\n",
       "37  bert-base-arabertv02-twitter          2021-12-05-train-0.898193359375\n",
       "38  bert-base-arabertv02-twitter          2021-12-05-train-0.898193359375\n",
       "39  bert-base-arabertv02-twitter          2021-12-09-train-0.963134765625\n",
       "40  bert-base-arabertv02-twitter          2021-12-09-train-0.963134765625\n",
       "41  bert-base-arabertv02-twitter          2021-12-09-train-0.963134765625\n",
       "42  bert-base-arabertv02-twitter  2022-01-29-train-0.826416015625-twitter\n",
       "43  bert-base-arabertv02-twitter  2022-01-29-train-0.826416015625-twitter\n",
       "44  bert-base-arabertv02-twitter  2022-01-29-train-0.826416015625-twitter"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('graduation_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
