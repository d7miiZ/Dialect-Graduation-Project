{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.realpath(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicons(text , lexicon_list):\n",
    "    text_lexicon = []\n",
    "    for lexicon in lexicon_list:\n",
    "        if lexicon in text:\n",
    "            text_lexicon.append(lexicon)\n",
    "    return \" \".join(text_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "egy_joy = [ \"تزغرط\", \"هيرضى\", \"تمزحو\", \"ميحرمناش\", \"متشكر\", \"هتستمتع\", \"هفرح\", \"هتضحك\", \"مريحني\"]\n",
    "glf_joy = [ \"ينحب\", \"ماقصرت\", \"سالخير\", \"مشكور\", \"لاهنتوا\", \"عقبالك\", \"يهنيكم\", \"نكتة\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMADC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1408456 entries, 0 to 256631\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   Text    1408456 non-null  object\n",
      " 1   Region  1408456 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 32.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = get_SMADC_folder_data(\"..\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_egy = df[df[\"Region\"] == \"EGY\"]\n",
    "df_glf = df[df[\"Region\"] == \"GLF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGY Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/4190196375.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n"
     ]
    }
   ],
   "source": [
    "df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n",
    "df_egy = df_egy[df_egy[\"Contains_Lexicon\"]]\n",
    "df_egy[\"Lexicon\"] = df_egy[\"Text\"].apply(get_lexicons,args=(egy_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>متشكر</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>هتستمتع</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>هتضحك</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>هفرح</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>هيرضى</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تمزحو</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>مريحني</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ميحرمناش</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تزغرط</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Text\n",
       "Lexicon       \n",
       "متشكر      140\n",
       "هتستمتع     74\n",
       "هتضحك       36\n",
       "هفرح        18\n",
       "هيرضى       12\n",
       "تمزحو       10\n",
       "مريحني       8\n",
       "ميحرمناش     8\n",
       "تزغرط        6"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_egy.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لو تقصد باقات الموبايل انترنت الباقات المتاحة بتكون باقة جنيه بتديلك ميجابايت تقدر تشترك باقة جنيه بتديلك ميجابايت تقدر تشترك باقة جنيه بتديلك جيجابايت تقدر تشترك باقة جنيه بتديلك جيجابايت تقدر تشترك باقة جنيه بتديلك جيجابايت تقدر تشترك باقة جنيه بتديلك جيجابايت تقدر تشترك باقة جنيه بتديلك جيجابايت تقدر تشترك و ممكن من خلال تقدر تشترك فى اى باقه و تعرف استهلاكك او من خلال برنامج انا فودافون و فى عرض ضعف باقة الموبايل انترنت متاح من الكود تقدر تشترك و \u001b[41m\u001b[37mهتستمتع\u001b[0m بضعف الباقة على السوشيال\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمتشكرين\u001b[0m علي المعلومة يا عم الناصح صبرنا يا رب\n",
      "-----------------\n",
      "والله انتم يامصرين شكو بزاف دمكم خفيف ودوام تحبو المزح والضحك رغم كل الظروف الصعبة ولكن دوم \u001b[41m\u001b[37mتمزحو\u001b[0m ربي يسعدكم يافراعنه تحياتي ليكم اخوكم ابراهيم م المنصوره\n",
      "-----------------\n",
      "يا عم علي محمد الشماع انت ولا عايز ترحم ولاتخلي رحمة ربنا تنزل مش كدة \u001b[41m\u001b[37mمتشكر\u001b[0m للناس المحترمة\n",
      "-----------------\n",
      "اصبر بكرة \u001b[41m\u001b[37mهتضحك\u001b[0m الحزن مع الوقت بيصغر الفرحه مبتجيش غير فجأه ربنا اصله بيرسم لوحه علشانك عملها مفاجأه الموضوع ع فكره بسيط\n",
      "-----------------\n",
      "بنشكر الاخوه المسلمين علي احترامهم لينا ولدينا ونشكر ايضا الناس التانيه اللي بتشتمنا وبتتريق علينا وعلي السيد المسيح ونقولهم وانتو طيبين \u001b[41m\u001b[37mمتشكرين\u001b[0m ليكوا\n",
      "-----------------\n",
      "ممكن الاشتراك في و عن طريقه لو بتستخدمي موبايل إنترنت من الفليكسات \u001b[41m\u001b[37mهتستمتعي\u001b[0m بضعف الباقة يعني ال ميجا ب فليكس بدل ال ميجا ب فليكس الاشتراك بكود باشتراك شهري جنيه\n",
      "-----------------\n",
      "ب جنيه اكسر الحاجة ده انا لو مدايقة وزهقانة وادتونى ال جنيه \u001b[41m\u001b[37mهفرح\u001b[0m وازقطط ومش هحتاج اكسر اى بتنجان\n",
      "-----------------\n",
      "انا عملت حاجه لطيفه جدا قفلت تويتر من وقمت ايه بقي فاتحه تويتر من البراوزر وضميري دلوقتي \u001b[41m\u001b[37mمريحني\u001b[0m جدا مش عارفه ليه\n",
      "-----------------\n",
      "عندنا أنظمة يومية و شهرية تقدري تختاري منهم على حسب احتياجك مع أنظمة الجديدة هتكلم صحابك بدقايق لكل الشبكات و نت بلا حدود سوشيال ومزيكا مع نظام جنيه اليومي \u001b[41m\u001b[37mهتستمتع\u001b[0m لمدة ساعة ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة أشترك من خلال من الموبايل مع نظام جنيه اليومي \u001b[41m\u001b[37mهتستمتع\u001b[0m لمدة ساعة ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة أشترك من خلال من الموبايل مع نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتع\u001b[0m ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة أشترك من خلال باشتراك شهري جنيه مع نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتعي\u001b[0m ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة أشترك من خلال باشتراك شهري جنيه مع نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتع\u001b[0m ب جيجا تستخدمها على كل المواقع و ال و نت بلا حدود سوشيال و مزيكا و كمان دقيقة لكل الشبكات أشترك من خلال باشتراك شهري جنيه تقدر تتواصل معانا من خلال صفحتنا من خلال\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمتشكرين\u001b[0m يا أوختى\n",
      "-----------------\n",
      "انظمة القديمة غير متاحة و دلوقتي جددت أنظمتها و هتكلمى صحابك بدقايق لكل الشبكات و نت بلا حدود سوشيال ومزيكا في أنظمة تختارى منهم على حسب احتياجاتك أنظمة يومية و شهرية نظام جنيه اليومي لمدة ساعة دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة و من تقدرى تشتركى و نظام جنيه اليومي لمدة ساعة دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة و من تقدرى تشتركى و نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتعى\u001b[0m ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة و من خلال تقدرى تشتركى باشتراك شهري ج و نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتعى\u001b[0m ب دقيقة لكل الشبكات و نت بلا حدود سوشيال و مزيكا أول ميجا باقصى سرعة و من خلال تقدرى تشتركى باشتراك شهري ج و نظام جنيه الشهري \u001b[41m\u001b[37mهتستمتعى\u001b[0m ب جيجا تستخدميها على كل المواقع و ال و نت بلا حدود سوشيال و مزيكا و كمان دقيقة لكل الشبكات و من خلال من تقدرى تشتركى باشتراك شهري ج\n",
      "-----------------\n",
      "والله انتم يامصرين شكو بزاف دمكم خفيف ودوام تحبو المزح والضحك رغم كل الظروف الصعبة ولكن دوم \u001b[41m\u001b[37mتمزحو\u001b[0m ربي يسعدكم يافراعنه تحياتي ليكم اخوكم احمد من مصر\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمتشكر\u001b[0m ليك وشعور محترم بس اسف العيد باظ والعيدية وصلت ودمنا مش بفلوس محتاجين مسوة محتاجين محسبة الجناة او اهلى الجناة ومحسبة كل من يزرع الكرة ومحسبة بعض من شيوخ ال بتحرض علينا القتل لا للاحتفلات والمعيادات\n",
      "-----------------\n",
      "اصبر بكرة \u001b[41m\u001b[37mهتضحك\u001b[0m الحزن مع الوقت بيصغر الفرحه مبتجيش غير فجأه ربنا اصله بيرسم لوحه علشانك عملها مفاجأه الموضوع ع فكره بسيط\n",
      "-----------------\n",
      "القمار حلال ولا حرام وايه الدليل انا مش بتاع قمار بس انا حد سألني السؤال دا ومعرفتش ارد عليه يا ريت اللى يعرف \u001b[41m\u001b[37mميحرمناش\u001b[0m من الاجابه يا رب اهدينا جميعا\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمتشكرين\u001b[0m علي وصول الاقباط للفردوس\n",
      "-----------------\n",
      "ممكن الاشتراك في و عن طريقه لو بتستخدمي موبايل إنترنت من الفليكسات \u001b[41m\u001b[37mهتستمتعي\u001b[0m بضعف الباقة يعني ال ميجا ب فليكس بدل ال ميجا ب فليكس الاشتراك بكود باشتراك شهري جنيه\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mميحرمناش\u001b[0m من فونك الجميل دا يا هنونه\n",
      "-----------------\n",
      "تصوري انك حقيرة وكلبة وخاينة هو انتي فاكره ان احنا جاهلين \u001b[41m\u001b[37mهتضحكي\u001b[0m علينا ماحناعارفين مين اللي بيقبض من الانجليز والصهاينة عشان يخرب بلده واللي نجاناوفوت عليكم الفرصة انتم والصهاينة بعدنكسة العاريناير بتقولولواعليه بلحة ياكلاب يااوساخ\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_egy[df_egy[\"Contains_Lexicon\"]].sample(20)[\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in egy_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/3884630044.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n"
     ]
    }
   ],
   "source": [
    "df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n",
    "df_glf = df_glf[df_glf[\"Contains_Lexicon\"]]\n",
    "df_glf[\"Lexicon\"] = df_glf[\"Text\"].apply(get_lexicons,args=(glf_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>مشكور</th>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ماقصرت</th>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ماقصرت مشكور</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>عقبالك</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>نكتة</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يهنيكم</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>لاهنتوا</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ينحب</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سالخير</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Text\n",
       "Lexicon           \n",
       "مشكور          873\n",
       "ماقصرت         324\n",
       "ماقصرت مشكور    57\n",
       "عقبالك          37\n",
       "نكتة            11\n",
       "يهنيكم           9\n",
       "لاهنتوا          8\n",
       "ينحب             7\n",
       "سالخير           6"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glf.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شكرا اخوي عماد بيض الله وجهك و \u001b[41m\u001b[37mماقصرت\u001b[0m اتوقع ان مافيه في كلية المجتمع كما ذكرت نفس التخصص لكن الشيء اللي يقدر يسويه انه يروح لهم بنفس القسم و يناقشهم اذا امكن الحصول على دبلوم و الله اعلم\n",
      "-----------------\n",
      "طيب هل الاسئله بالعربي او بالانجليزي \u001b[41m\u001b[37mومشكور\u001b[0m على الرد\n",
      "-----------------\n",
      "امنة \u001b[41m\u001b[37mمشكورة\u001b[0m على الرد انا عندي رسوم المعهد والتأشيرة والسيفيس ورسوم موعد مقابلة السفارة الامريكية بالاضافة الى ايصالات فيدكس ايش الي ممكن يرفضون تعويضها\n",
      "-----------------\n",
      "اي انتقاد لليهود في المجال الاكاديمي الغربي ينظر إليه بشكل سلبي وهناك مبتعثين سعوديين يدرسون في السياسة والعلوم الانسانية والاديان وهاذي التخصصات فيها مواضيع عن اليهود و نظرة الغرب لليهود تختلف تماما عن نظرتنا لهم لذلك تفضل الأخ \u001b[41m\u001b[37mمشكورا\u001b[0m بتقديم النصيحة\n",
      "-----------------\n",
      "اعمل خيرا تلقى شرا و كأنه بالأصل هذا الشخص اللي يشتكي من طمس شعارات تعب فيها و كتبها من راسه وش ها الحسد حتى في الاشياء اللي ما تعبت فيها انا يكفيني الرابط تبع جامعة مانشستر و طز في الصورة \u001b[41m\u001b[37mمشكور\u001b[0m يا د عبدالمحسن جزاك الله خير واصل و لا عليك\n",
      "-----------------\n",
      "شريفة \u001b[41m\u001b[37mمشكورة\u001b[0m يالغالية هذا مافعلته\n",
      "-----------------\n",
      "شكرا لك أخي أحمد على التوضيح \u001b[41m\u001b[37mماقصرت\u001b[0m\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكوره\u001b[0m يا اختي الله يكتب أجرك ويجعله في ميزان حسناتك يارب اسأل الله ان يسعدكم جمعيا ويوفقكم يارب\n",
      "-----------------\n",
      "الله يبشرك بالخير \u001b[41m\u001b[37mمشكور\u001b[0m\n",
      "-----------------\n",
      "يعطيكم العافية \u001b[41m\u001b[37mماقصرتوا\u001b[0m\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكورة\u001b[0m على مشاركتك الكريمة معنا\n",
      "-----------------\n",
      "اللهم امين ويجزاكم خير جميعا \u001b[41m\u001b[37mمشكورين\u001b[0m\n",
      "-----------------\n",
      "شكرا \u001b[41m\u001b[37mماقصرتي\u001b[0m شكرا للجميع\n",
      "-----------------\n",
      "أخني \u001b[41m\u001b[37mمشكورة\u001b[0m على تعليقك الآن عندما أريد أن أبحث عن قبول ماستر هل أكتب التخصص العام تربية خاصة أو التخصص الدقيق صعوبات التعلم لأني وجدت التخصص العام والدقيق في محرك البحث التابع للجامعات الموصى بها واحترت في الاخنيار\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكورين\u001b[0m يعطيكم الف عافيه\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكورين\u001b[0m الله يوفقكم يارب بس انا ابي احد يكتب لي عندي النقاط كلها ووكاتبتها بس ابي اححد اسلوبه ادبي ويفل نيتف سبيكر يضبطه لي كل شيء جاهز عندي ودرجاتي عالية بس قالت لي المشرفة لازم ستيتمنت قوي جدا لان التنافس قوي والمقاعد قليلة جدا\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكورين\u001b[0m وجزاكم الله خير تواصلي كان مستمر معاهم لكن ماعقدت اللجنه ننتظر اللجنه تعقد لسا ماتم توقيع الاوراق وكل يوم عذر وللان مانتهت والجامعه رفضت الضمان اللي بفرض القبول من الملحقية\n",
      "-----------------\n",
      "ان شاء الله خير \u001b[41m\u001b[37mمشكور\u001b[0m اخوي الله يسهل أمرك ويوفقك\n",
      "-----------------\n",
      "الله يعطيكم العافيه \u001b[41m\u001b[37mومشكورة\u001b[0m ع المجهود الطيب والحمدلله ع سلامتكم\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mماقصرت\u001b[0m اخوي\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_glf[df_glf[\"Contains_Lexicon\"]].sample(20)[\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in glf_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26804 entries, 2 to 6514\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Region  26804 non-null  object\n",
      " 1   Text    26803 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 628.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = get_annotated_data_folder_data(\"..\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_egy = df[df[\"Region\"] == \"EGY\"]\n",
    "df_glf = df[df[\"Region\"] == \"GLF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGY Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/4190196375.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n"
     ]
    }
   ],
   "source": [
    "df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n",
    "df_egy = df_egy[df_egy[\"Contains_Lexicon\"]]\n",
    "df_egy[\"Lexicon\"] = df_egy[\"Text\"].apply(get_lexicons,args=(egy_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>متشكر</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text\n",
       "Lexicon      \n",
       "متشكر       3"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_egy.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنا فخور جدا بوطني العزيز ونحن \u001b[41m\u001b[37mمتشكرون\u001b[0m لإخوتنا السعوديين!\n",
      "-----------------\n",
      "طبعا \u001b[41m\u001b[37mمتشكر\u001b[0m جدا على الآهتمام بس فعلا انا متضايق جدا لان انا شيلت كل الصور ديه من عندى علشان متكونشى سبب ان اى حد يزعل منى لانى مكنتش اقصد اى حاجه وحشه ولانى مكنتش فاهم ان ده غلط وحضاراتكم نزلتوها حتى من غير استاذان وسببتولى مشاكل مع الآنتاج\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمتشكر\u001b[0m جدا بس اعتقد ان الصحافه مش كده لان الصحافه تحترم عقول وشخوص الناس\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_egy[df_egy[\"Contains_Lexicon\"]][\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in egy_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/3884630044.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n"
     ]
    }
   ],
   "source": [
    "df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n",
    "df_glf = df_glf[df_glf[\"Contains_Lexicon\"]]\n",
    "df_glf[\"Lexicon\"] = df_glf[\"Text\"].apply(get_lexicons,args=(glf_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ماقصرت</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>مشكور</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>نكتة</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>عقبالك</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text\n",
       "Lexicon      \n",
       "ماقصرت      9\n",
       "مشكور       6\n",
       "نكتة        2\n",
       "عقبالك      1"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glf.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "رافت علي يا ادارتنا \u001b[41m\u001b[37mمشكورين\u001b[0m على كل شي قدمتوة بس بحب قلكم شي مرة فكرة ادارة مدريد تتخلى عن راؤؤل كل جماهير المدريدي وقفت\n",
      "-----------------\n",
      "الله يعطيها العافية نورة الفايز \u001b[41m\u001b[37mماقصرت\u001b[0m بجد تهتم بمشاكل المرأة كل يوم هي ببلد ماترتاح تفكر بغيرها الله يجزاك خير والى الامام والله يعطيعا العافية ويحقق مرادها ويسعدها ويخليلها والديها وشكرا\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mالنكتة\u001b[0m اتوقع كل هالمحاباة للنصر واخرتها بيفوز الهلال 1-صفر ههه ان شاء الله\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكورين\u001b[0m بس ياليت يشمل المكرمه كليات البنات بضرما وغيرها ترى ثقلنا على اهلنا المواصلات وبلا مكافئات يا ريت ينضر لحال الكليات يدفع الاهالي اكثر من 500ريال شهريا\n",
      "-----------------\n",
      "اي والله لا وصيكم يا صقور نايف اي والله انكم انتم الي \u001b[41m\u001b[37mماقصرتم\u001b[0m الله يحفظكم ويحميكم ياربي\n",
      "-----------------\n",
      "انت ليش لازم تدخل الهيئة في الموضوع..يا عبدالله هذولي نافعين الناس ماضروها والحكومة الله يبارك فيها \u001b[41m\u001b[37mماقصرت\u001b[0m وانت فاهم المعنى,,مو زي بعض الناس بس يهدم ولا يبني,, اجل جن هاه,,, بالله أسألهم ما عندهم وظايف. قال ايش قال جمعية صداقة مع الجن ,,الحمد لله والشكر أولو الالباب في نعمة.\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكور\u001b[0m أستاذ فهد ع هالمقال الجميل\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mمشكور\u001b[0m يابو متعب عساك دوم بصحه وعافيه وواصلي يابلادي\n",
      "-----------------\n",
      "الله يعينكم السؤال الصريح والموجه الى المسؤالين في البلديات والاما نات الطرق مكسرة وبعضها مرقعه وبعضها بها حفرتاخذلها عشر سنوات مادرو عنها ون دار يقول مهي مسؤليتي والا مالي شغل فيها مسؤليه فلان والاتصريفه يقولك ابشر ولاشفنا شي وعلى الحال الدوله \u001b[41m\u001b[37mماقصرت\u001b[0m تعطي المسؤالين الفلوس علشان يريحون المواطن\n",
      "-----------------\n",
      "خبر عاجل زياني لن يشارك في المونديال هذه \u001b[41m\u001b[37mنكتة\u001b[0m ماي\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mعقبالك\u001b[0m يا شوبكي من السعوديه يجيك ولد اشقر وعيونه زرق\n",
      "-----------------\n",
      "والدوله \u001b[41m\u001b[37mماقصرت\u001b[0m وياكم بلفلوس بس تبيكم ترفعو راس الوطن بلأخلاق قبل الكره وبعدين انت يالاعب رقم 12 ليش زعلان ومعصب وكل مباراه مسوي مشاكل مع اصحابك\n",
      "-----------------\n",
      "يعطيكم العآفيه يا ليوث \u001b[41m\u001b[37mماقصرتوا\u001b[0m و بيضتو وجوهنا آمس.. أما الزعيم فمباراة ع المستوى الفني و المهاري والفكري للاعبين كانت عاليه.. لكن بو دعيع واضح انه صار يتكبر ع الكوره.. مفروض يعتزل و لا يسوي اللي سواه أمس و الفريدي كأنو يعاني من مشكله سايكولوجيه و كثرت أخطاءه الغير مبرره أبدا\" أتمنآ يتدخل الجهاز الاداري و يحاول يبعده عن أي ضغوط.,’\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mماقصرت\u001b[0m ياصادق علمك وصل\n",
      "-----------------\n",
      "عز الله انك \u001b[41m\u001b[37mماقصرت\u001b[0m يامحمد الحمود.. كنت وجه سعد على الطلبه المبتعثين في البحرين.. راعي فزعه ولابينك وبين الطلاب كلافه ومكتبك مفتوح للجميع. الله يجزاك الف خير ويوفقك ويطول بعمرك.\n",
      "-----------------\n",
      "هو صحيح ان القبيلية طفرا وفقرانين بس الغريبة لية مايجوز بناتهم مع ان الحكومة الله يقويها ويقوي باسها \u001b[41m\u001b[37mماقصرت\u001b[0m عطت الوضايف والدراهم للي( يستاهلون) مهوب هالقبيليين العنصريين مع اني اشوف العرب مغرورين وشايفين نفسهم كانهم طالعين للمريخ او تاريخهم كلة سلب ونهب لية مايجوزن بناتهم للراعي الدين\n",
      "-----------------\n",
      "مبروك ياللي فصلت \u001b[41m\u001b[37mالوراعين--ومشكور\u001b[0m يومك اسعدت ربعك\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mواصلوامشكورين\u001b[0m\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_glf[df_glf[\"Contains_Lexicon\"]][\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in glf_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DART Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24279 entries, 1 to 3929\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    24279 non-null  object\n",
      " 1   Region  24279 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 569.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = get_dart_folder_data(\"..\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_egy = df[df[\"Region\"] == \"EGY\"]\n",
    "df_glf = df[df[\"Region\"] == \"GLF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGY Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/4190196375.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n"
     ]
    }
   ],
   "source": [
    "df_egy[\"Contains_Lexicon\"] = df_egy[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in egy_joy))\n",
    "df_egy = df_egy[df_egy[\"Contains_Lexicon\"]]\n",
    "df_egy[\"Lexicon\"] = df_egy[\"Text\"].apply(get_lexicons,args=(egy_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>مريحني</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>هفرح</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text\n",
       "Lexicon      \n",
       "مريحني      2\n",
       "هفرح        1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_egy.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مقسومة نصين .. مفيش نص فيهم \u001b[41m\u001b[37mمريحني.\u001b[0m\n",
      "-----------------\n",
      "الاحساس ده مفتدقاه يمكن عشان مش ملهوفة ع حاجة بس بجد الاحساس ده كان منور قلبى اوي و كان \u001b[41m\u001b[37mمريحني\u001b[0m مش عارفة اوصفه و فعلا ربنا كرمني جدا ❤️❤️\n",
      "-----------------\n",
      "مضايقة لدرجتي اني \u001b[41m\u001b[37mهفرح\u001b[0m بس لو قتلت حد وشربت من دمه حرفيا والله\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_egy[df_egy[\"Contains_Lexicon\"]][\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in egy_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\AppData\\Local\\Temp/ipykernel_50672/3884630044.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n"
     ]
    }
   ],
   "source": [
    "df_glf[\"Contains_Lexicon\"] = df_glf[\"Text\"].apply(lambda text: any(lexicon in text for lexicon in glf_joy))\n",
    "df_glf = df_glf[df_glf[\"Contains_Lexicon\"]]\n",
    "df_glf[\"Lexicon\"] = df_glf[\"Text\"].apply(get_lexicons,args=(glf_joy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ينحب</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سالخير</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>نكتة</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text\n",
       "Lexicon      \n",
       "ينحب        2\n",
       "سالخير      1\n",
       "نكتة        1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glf.groupby([\"Lexicon\"]).agg({\"Text\" : \"count\"}).sort_values(\"Text\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بالعربي جمعان الحربش انحاش و خلى الشباب \u001b[41m\u001b[37mينحبسون\u001b[0m .. #الريتويتات_السابقة\n",
      "-----------------\n",
      "\u001b[41m\u001b[37mنكتة\u001b[0m الموسم 😂😂😂😂😂 لا جد هيك ختموا اللعبة..مع إنه ما كنا رح نحس بالفرق هيك هيك بجبولنا أسئلة مش من اللي مندرسه 🌝🤦… https://t.co/DRckhp1ZMc\n",
      "-----------------\n",
      "كُلشي البيِك حبيته شنو البيِك ما \u001b[41m\u001b[37mينحب؟\u001b[0m\n",
      "-----------------\n",
      "@al_wisal \u001b[41m\u001b[37mمسالخير\u001b[0m حلوه الاجازه ع كورنيش مطرح😍😍 محتاج اسمع خالد عبد الرحمن وشلون مغليك😉\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for text in df_glf[df_glf[\"Contains_Lexicon\"]][\"Text\"]:\n",
    "    formattedText = []\n",
    "    for word in text.split():\n",
    "        if any(lexicon in word for lexicon in glf_joy):\n",
    "            formattedText.append(colored(word,'white','on_red'))\n",
    "        else: \n",
    "            formattedText.append(word)\n",
    "    print(\" \".join(formattedText))\n",
    "    print(\"-----------------\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04bf7c5048c44052f72b8bf37baf02c4a3c1f9aba82796c3f014f38260524f40"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
