{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import torch\n",
    "from torch import nn\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "orignal_egy = fasttext.load_model(\"embedding_EGY.bin\")\n",
    "mapped_egy = KeyedVectors.load_word2vec_format(\"vectors-EGY.txt\")\n",
    "assert orignal_egy.get_words() == mapped_egy.index_to_key\n",
    "word_list = orignal_egy.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_from = len(orignal_egy[\"تست\"])\n",
    "project_to = mapped_egy.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([orignal_egy.get_word_vector(word) for word in word_list])\n",
    "Y = np.array([mapped_egy.get_vector(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "[1,  2000] loss: 0.196\n",
      "[1,  4000] loss: 0.141\n",
      "[1,  6000] loss: 0.112\n",
      "[1,  8000] loss: 0.094\n",
      "[1, 10000] loss: 0.083\n",
      "[1, 12000] loss: 0.076\n",
      "[1, 14000] loss: 0.071\n",
      "[1, 16000] loss: 0.069\n",
      "[1, 18000] loss: 0.067\n",
      "[1, 20000] loss: 0.066\n",
      "[1, 22000] loss: 0.064\n",
      "[1, 24000] loss: 0.064\n",
      "[1, 26000] loss: 0.064\n",
      "[1, 28000] loss: 0.062\n",
      "[1, 30000] loss: 0.062\n",
      "[1, 32000] loss: 0.061\n",
      "[1, 34000] loss: 0.061\n",
      "Starting epoch 2\n",
      "[2,  2000] loss: 0.061\n",
      "[2,  4000] loss: 0.061\n",
      "[2,  6000] loss: 0.061\n",
      "[2,  8000] loss: 0.060\n",
      "[2, 10000] loss: 0.060\n",
      "[2, 12000] loss: 0.060\n",
      "[2, 14000] loss: 0.060\n",
      "[2, 16000] loss: 0.060\n",
      "[2, 18000] loss: 0.060\n",
      "[2, 20000] loss: 0.060\n",
      "[2, 22000] loss: 0.060\n",
      "[2, 24000] loss: 0.060\n",
      "[2, 26000] loss: 0.060\n",
      "[2, 28000] loss: 0.060\n",
      "[2, 30000] loss: 0.060\n",
      "[2, 32000] loss: 0.059\n",
      "[2, 34000] loss: 0.059\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "class VectorDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      self.X = torch.from_numpy(X)\n",
    "      self.y = torch.from_numpy(y)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return self.X[i], self.y[i]\n",
    "      \n",
    "class NeuralNet(nn.Module):\n",
    " \n",
    "  def __init__(self,):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.nn = nn.Linear(project_from, project_to)\n",
    "   \n",
    "  def forward(self, inputs):\n",
    "    return self.nn(inputs)\n",
    "\n",
    "dataset = VectorDataSet(X, Y)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "  \n",
    "model = NeuralNet()\n",
    "\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(2): \n",
    "  \n",
    "  print(f'Starting epoch {epoch+1}')\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "      inputs, actual = data\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_function(outputs, actual)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      running_loss += loss.item()\n",
    "      if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "          print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "          running_loss = 0.0\n",
    "\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0027,  0.0077,  0.1022,  ..., -0.0713, -0.0463, -0.0419],\n",
       "        [ 0.0306,  0.0149,  0.1105,  ...,  0.0757,  0.0461, -0.0160],\n",
       "        [-0.0107, -0.0302, -0.0311,  ..., -0.0349,  0.0028,  0.0132],\n",
       "        ...,\n",
       "        [ 0.0434, -0.0684,  0.0307,  ...,  0.0212, -0.0505,  0.0130],\n",
       "        [-0.0511,  0.0018,  0.0839,  ...,  0.0124, -0.0109,  0.0302],\n",
       "        [-0.0634, -0.0398, -0.0474,  ..., -0.0900,  0.0526, -0.0636]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.nn.weight\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform OOV Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.8678e-03,  2.3735e-03,  1.8241e-01,  2.0889e-02, -6.8731e-02,\n",
       "         1.3860e-01,  1.0363e-01, -4.5243e-02, -6.7927e-03,  9.1357e-03,\n",
       "        -9.5829e-02, -1.6048e-03, -1.4016e-01,  6.4791e-02, -8.5503e-02,\n",
       "         2.9857e-02, -3.8317e-02, -9.7960e-02,  1.7813e-02,  1.9014e-02,\n",
       "         1.0002e-01,  7.5055e-04, -3.4619e-02,  4.2269e-02,  5.6960e-03,\n",
       "         3.8909e-02, -1.8595e-02, -1.3510e-01,  2.2122e-02,  8.4148e-02,\n",
       "         2.3929e-02, -9.6059e-02, -5.5370e-02, -6.0250e-02, -2.8113e-02,\n",
       "         1.0597e-01,  3.5426e-02, -9.2728e-03, -5.5328e-02, -4.9054e-02,\n",
       "        -5.9051e-03,  1.1898e-01,  7.2115e-03,  4.0465e-05, -2.2899e-01,\n",
       "        -1.1009e-01, -3.9917e-02,  5.5506e-02, -9.6712e-05,  1.3041e-01,\n",
       "        -7.8812e-02, -3.1766e-02, -2.0723e-02, -3.6732e-02,  1.0633e-02,\n",
       "        -7.6645e-02, -7.9169e-02,  6.6527e-02, -2.2412e-02,  6.6237e-02,\n",
       "         1.2647e-01,  2.1966e-02, -9.6587e-03, -1.2444e-01, -4.2160e-02,\n",
       "        -9.9926e-02,  1.0107e-03, -8.3764e-02, -4.0149e-02, -9.4625e-02,\n",
       "         1.9479e-02, -1.6877e-02,  5.4945e-03, -1.1725e-01, -4.3180e-03,\n",
       "        -1.0589e-01,  1.2605e-01, -1.2110e-02,  5.4665e-02, -5.4589e-02,\n",
       "         1.0380e-01,  1.1666e-01,  3.6135e-02, -2.1803e-02, -1.1949e-01,\n",
       "         9.0251e-02,  2.7561e-02,  5.7972e-02,  5.2247e-02, -1.0570e-01,\n",
       "        -8.0359e-03,  1.0254e-01,  2.2114e-02,  6.8369e-02, -1.8738e-02,\n",
       "         1.5666e-01,  7.1676e-03, -1.5169e-02, -1.8728e-01, -5.3126e-02,\n",
       "        -1.1925e-01,  3.6336e-03,  1.5627e-01, -1.3950e-01,  7.1623e-02,\n",
       "         7.9737e-02,  1.4745e-01,  2.4285e-02,  4.2964e-02, -4.4234e-02,\n",
       "        -2.8204e-02,  3.4339e-02, -1.8071e-01, -2.4029e-02,  1.1025e-01,\n",
       "         9.2312e-02,  6.1391e-02, -5.8917e-02,  4.4060e-06,  1.5475e-01,\n",
       "         1.3291e-01,  7.8162e-02,  4.5410e-03,  1.8195e-01,  5.1002e-02,\n",
       "         8.0372e-03,  1.1999e-02, -4.9895e-02,  7.9783e-02,  5.4739e-02,\n",
       "        -4.8167e-02, -5.8194e-02, -4.0817e-02, -3.0786e-02, -2.6887e-02,\n",
       "         8.3025e-02, -2.3554e-02, -4.3713e-02, -6.3887e-02, -1.5168e-01,\n",
       "        -1.0290e-01,  2.3757e-02,  1.3440e-01,  2.8938e-02, -1.7297e-01,\n",
       "         3.3661e-02, -1.4582e-01, -6.0195e-02,  1.3383e-03,  1.6447e-01,\n",
       "        -8.4093e-02, -1.2287e-01, -1.3727e-01,  8.4394e-02, -9.2653e-02,\n",
       "        -4.8127e-02,  1.6632e-01,  9.8637e-02, -4.6948e-03,  1.6806e-03,\n",
       "         1.8999e-01,  4.1449e-02, -1.2752e-01, -3.4202e-02,  4.2337e-02,\n",
       "         3.3051e-03, -2.1132e-02, -5.4753e-02, -1.8204e-01, -6.8955e-02,\n",
       "         6.0944e-02,  7.8795e-02, -1.9657e-01,  1.3411e-01,  7.5740e-02,\n",
       "         1.1068e-01, -1.5020e-01,  4.4851e-02,  1.6196e-01, -6.9733e-03,\n",
       "         3.0540e-02, -1.2478e-01,  8.5533e-02, -1.6302e-01,  1.5120e-01,\n",
       "         2.4842e-02, -5.0327e-02, -2.0468e-01, -9.6368e-03, -6.2474e-02,\n",
       "        -5.0568e-02,  2.2367e-02,  2.3134e-02,  4.9509e-03,  1.5014e-01,\n",
       "        -4.0404e-02, -1.8066e-02,  2.4992e-01, -9.7245e-02, -4.7512e-02,\n",
       "        -4.4062e-02,  6.7716e-02,  1.0513e-01, -3.5173e-01,  1.8939e-01,\n",
       "         1.9594e-01, -1.0429e-01, -1.8556e-02, -5.4941e-02, -7.0766e-02,\n",
       "        -4.2553e-02, -3.3546e-02, -4.7170e-02, -1.5023e-01, -9.1704e-02,\n",
       "         3.5383e-02,  8.9883e-02, -4.5784e-02,  1.1131e-01,  1.1009e-02,\n",
       "         7.4802e-02, -8.6140e-03, -1.9961e-01,  3.0996e-02,  5.3381e-02,\n",
       "        -7.2021e-02, -6.3265e-03, -7.5939e-03,  1.2917e-02, -8.0635e-02,\n",
       "         3.6222e-02, -1.2658e-01,  1.1528e-02,  5.6453e-02, -7.5767e-02,\n",
       "        -8.2273e-03, -1.2724e-01,  4.2188e-02,  2.9725e-02, -5.6976e-02,\n",
       "        -1.2486e-02, -8.7844e-02, -7.2248e-02,  1.0712e-02, -1.1469e-01,\n",
       "         1.5549e-01, -1.5696e-01, -1.0302e-01,  6.0090e-03,  1.5954e-01,\n",
       "        -1.0647e-01,  3.8086e-02,  1.6595e-01,  2.5678e-02,  1.8091e-01,\n",
       "         2.5104e-02, -4.2125e-02, -7.3971e-02,  6.5104e-03, -4.5706e-02,\n",
       "         9.5281e-02,  5.0686e-02,  9.4532e-02,  1.1317e-01, -1.1901e-01,\n",
       "         1.6594e-01, -6.2853e-02, -1.9038e-02, -5.3003e-02, -5.9639e-02,\n",
       "         1.5501e-02, -6.8081e-03, -2.8557e-04, -1.3214e-01, -6.5367e-02,\n",
       "         2.1838e-01, -2.1617e-02, -6.0026e-02, -2.2901e-01,  1.6653e-02,\n",
       "         3.8856e-02,  6.2683e-02,  6.2572e-02, -8.4543e-02, -4.8427e-02,\n",
       "        -1.4791e-02, -1.9792e-01,  1.1761e-01, -9.4116e-02,  8.3242e-02,\n",
       "         1.2470e-01, -1.6095e-01, -8.0436e-02, -8.1391e-02, -2.0123e-02,\n",
       "        -1.1744e-01, -1.0295e-01,  1.9787e-01,  1.2575e-01,  2.1088e-02,\n",
       "         2.0294e-02, -7.1114e-02,  6.1228e-03,  6.3719e-03, -1.6443e-01,\n",
       "         2.9758e-02, -1.6969e-01,  1.4018e-01, -8.3358e-02, -4.7479e-02,\n",
       "        -1.1656e-02,  9.0841e-02, -1.4558e-02, -2.0481e-02,  9.5578e-02,\n",
       "        -1.3794e-01,  4.1016e-02,  1.2581e-01,  4.2755e-03,  1.1257e-01,\n",
       "         3.3575e-02, -5.3061e-03,  3.4505e-02,  3.6563e-02,  9.4956e-02,\n",
       "         9.6725e-02, -2.5399e-02,  4.5183e-02,  9.7410e-02, -1.0269e-01,\n",
       "        -5.8908e-02, -1.3395e-02, -2.7433e-02,  1.5202e-02, -1.6686e-02,\n",
       "         1.6479e-02, -1.1142e-01,  1.7956e-03,  7.0486e-02,  5.9210e-03,\n",
       "         2.4185e-01, -1.9449e-02, -2.2785e-02,  1.3351e-02, -7.5951e-03,\n",
       "         1.2775e-02,  4.2369e-02,  1.5304e-02,  2.9628e-01, -1.5538e-03,\n",
       "         5.5838e-02,  7.6295e-02,  6.4107e-02,  1.0004e-02, -9.8763e-02,\n",
       "        -1.8913e-01,  1.3794e-01, -3.0967e-02,  4.6216e-02,  4.2710e-02,\n",
       "        -1.0139e-01, -9.7001e-02, -2.7697e-02,  1.2042e-02,  1.5210e-01,\n",
       "        -6.9976e-02,  4.8092e-02, -1.2341e-01, -1.7663e-03, -4.9497e-02,\n",
       "         3.3928e-02,  1.1842e-01,  1.2379e-01,  1.1060e-02, -1.1682e-01,\n",
       "        -5.5312e-02,  8.1836e-02,  7.1335e-02,  3.8672e-02, -1.1502e-01,\n",
       "         8.9883e-02,  1.1425e-01, -2.7503e-02, -1.1169e-01,  1.7711e-02,\n",
       "        -6.7436e-02,  1.1250e-01,  1.5053e-01,  5.2534e-02,  1.1019e-01,\n",
       "         2.0485e-02,  5.6653e-02,  1.1015e-03,  2.0891e-01, -5.9485e-02,\n",
       "         3.4045e-02, -9.7959e-02,  1.2785e-01,  1.6322e-01,  3.1075e-02,\n",
       "        -6.3509e-02,  1.4709e-01, -1.0519e-01,  3.3789e-02, -7.9992e-02,\n",
       "         8.5022e-02, -1.0822e-01, -5.6422e-02, -2.6736e-02,  6.5743e-02,\n",
       "        -1.4629e-02, -3.2706e-02,  5.1379e-02, -5.1109e-02,  2.1117e-02,\n",
       "        -1.7297e-01,  4.1587e-02, -1.8188e-02,  6.5841e-02,  1.8578e-02,\n",
       "         2.4631e-02, -1.2738e-01,  2.0088e-01, -1.9011e-01, -4.6321e-02,\n",
       "        -1.3456e-02,  1.4678e-02, -1.3256e-01,  3.9502e-02, -2.3689e-02,\n",
       "         3.7583e-02,  1.0574e-01, -1.8024e-02, -7.0153e-03, -1.2226e-01,\n",
       "         3.3383e-02, -5.7461e-02,  6.6125e-02, -1.5193e-01, -1.3154e-01,\n",
       "         5.7407e-02,  1.2217e-01,  3.3190e-02, -1.0557e-01, -1.1621e-01,\n",
       "         4.6303e-02, -4.3810e-02,  3.6007e-02, -5.6428e-02, -5.9173e-02,\n",
       "         1.3573e-01,  6.2483e-02, -1.3499e-01, -6.8528e-02, -1.5329e-02,\n",
       "        -4.5181e-02, -2.1530e-01,  5.1241e-02,  1.0688e-01, -1.8740e-02,\n",
       "        -1.5530e-02,  1.3753e-01, -5.8571e-02,  1.7355e-01, -6.2421e-02,\n",
       "         2.9860e-03,  1.1463e-01, -1.3894e-02, -8.0820e-02, -3.7704e-02,\n",
       "         6.8729e-02, -1.6386e-01,  1.8746e-01,  6.6074e-03,  5.1776e-02,\n",
       "        -1.2815e-01,  7.9435e-02,  8.6347e-02, -7.4288e-02, -4.2825e-02,\n",
       "         5.9562e-03,  2.5276e-02,  3.7258e-02, -1.7343e-02,  2.4623e-01,\n",
       "         1.9361e-02, -4.0749e-02,  3.7097e-02, -7.1059e-02,  1.3571e-01,\n",
       "        -4.9609e-02,  4.4874e-02, -8.1191e-02,  1.1035e-01, -9.5811e-03,\n",
       "         7.8758e-02, -3.3244e-02, -1.1469e-01,  7.9699e-02, -1.8928e-01,\n",
       "         9.6522e-02,  7.6966e-02,  4.1825e-02, -1.7246e-03, -6.1968e-02,\n",
       "        -2.7557e-01,  6.2646e-02, -3.5466e-04, -7.1005e-04,  7.2274e-02,\n",
       "        -4.9327e-02,  6.2422e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = torch.from_numpy(orignal_egy[\"ابرا كادبرا\"])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3970e-01,  9.1253e-02,  9.8216e-02,  1.3309e-01, -8.9674e-03,\n",
       "        -1.7742e-01, -3.7670e-02,  9.4546e-02,  1.9565e-01, -1.1822e-03,\n",
       "         7.2320e-02,  1.1083e-01,  3.3814e-02, -2.6407e-01, -1.4682e-01,\n",
       "        -1.1834e-01,  8.4182e-02, -3.3805e-01, -1.7135e-01, -1.3218e-02,\n",
       "        -8.4594e-02, -8.7626e-02, -6.4661e-02,  2.8481e-01, -2.3850e-01,\n",
       "        -1.4856e-01,  1.2736e-01,  3.5994e-01, -1.1005e-01,  5.1463e-02,\n",
       "         1.4001e-01, -3.7012e-02, -2.2374e-01, -8.7048e-03,  1.1310e-01,\n",
       "        -2.4818e-01, -6.8081e-02,  1.2651e-01, -1.5978e-01, -6.6077e-02,\n",
       "         7.8119e-02, -7.0779e-02, -1.6016e-01,  6.9317e-02, -3.9864e-02,\n",
       "         1.1742e-02,  7.5328e-02,  1.9741e-04,  4.5231e-02, -3.4390e-02,\n",
       "         7.1819e-02,  1.9203e-02,  1.8718e-01, -5.7936e-02,  2.9158e-02,\n",
       "        -5.6911e-03,  3.4707e-02,  1.8340e-01, -4.3190e-02,  1.2007e-01,\n",
       "        -1.1225e-01, -6.8057e-02,  1.5107e-01,  1.9450e-01,  1.7726e-01,\n",
       "         9.0552e-02,  3.0664e-02,  1.4105e-01, -1.8089e-01, -2.1224e-01,\n",
       "         1.3916e-01, -7.7602e-02,  2.4589e-02,  1.4395e-01, -9.0894e-02,\n",
       "         1.8770e-02,  1.0304e-01,  1.8693e-01,  1.6123e-01,  4.2272e-03,\n",
       "        -1.6726e-01,  4.2114e-02, -5.3082e-02, -1.4412e-01,  4.0228e-02,\n",
       "         5.6364e-02,  7.3957e-02, -1.7694e-02, -8.2324e-02,  1.5473e-02,\n",
       "        -2.1832e-02,  1.2519e-01,  1.3744e-01,  9.0729e-02,  7.3632e-03,\n",
       "        -8.4001e-02, -2.0026e-01,  1.5981e-01, -6.7627e-02,  2.0371e-01,\n",
       "         6.5638e-02, -1.3345e-01,  1.8955e-01, -3.2727e-02, -2.3815e-01,\n",
       "         1.5293e-01, -3.3759e-02, -4.9239e-02,  6.1576e-02, -7.0774e-02,\n",
       "         6.6842e-02, -1.2823e-01,  1.6739e-02,  2.6266e-01,  1.8437e-01,\n",
       "         6.8087e-02, -6.8394e-02,  4.9975e-02,  8.4086e-02,  3.0722e-02,\n",
       "         1.8622e-01, -6.6729e-02,  8.1534e-02,  2.9795e-02, -1.3318e-01,\n",
       "         2.8629e-02, -3.6528e-02,  4.4590e-02, -2.5947e-01, -2.1573e-01,\n",
       "        -2.1008e-01,  7.0998e-02, -3.9882e-02,  2.6326e-02,  1.1677e-01,\n",
       "        -6.7085e-02,  4.7571e-02,  2.4549e-01, -1.0468e-01, -6.9744e-02,\n",
       "        -1.2166e-01, -5.6627e-02, -2.8672e-02, -3.0234e-02, -6.3403e-02,\n",
       "        -1.4157e-01,  1.0795e-01, -4.6608e-02, -1.6493e-01, -7.9003e-03,\n",
       "        -3.7891e-02, -5.2697e-02, -5.8791e-02, -1.0486e-02,  3.5648e-02,\n",
       "        -3.4461e-02, -7.3550e-03, -9.7861e-02, -2.1072e-02,  1.4873e-01,\n",
       "        -2.6391e-02, -1.6010e-01,  5.8333e-02, -7.5298e-02,  1.0400e-01,\n",
       "         5.5965e-02,  9.3761e-02, -2.7718e-02,  5.4274e-02, -7.4128e-02,\n",
       "        -5.4355e-02,  3.4041e-01, -4.7834e-02,  4.8518e-02,  5.7129e-02,\n",
       "         7.6156e-02,  2.3626e-01,  1.0072e-01, -1.1539e-02,  2.6249e-02,\n",
       "         6.4799e-02,  1.4738e-03,  7.3648e-02, -1.2151e-01, -2.8953e-02,\n",
       "        -6.8861e-02,  1.1794e-01,  2.1164e-01,  5.0306e-02, -1.0054e-02,\n",
       "        -2.6443e-02,  8.2292e-03,  8.4114e-02, -6.6985e-02,  5.1823e-03,\n",
       "         5.6326e-02, -2.7343e-02, -1.1766e-01, -9.0033e-02, -1.2372e-01,\n",
       "        -2.5478e-02,  1.0198e-01, -7.5990e-02,  7.9607e-02, -1.5047e-02,\n",
       "         1.3584e-02, -2.6041e-01, -1.6323e-01, -7.8002e-02, -1.6610e-01,\n",
       "         7.2969e-02, -1.5629e-01, -2.7909e-01,  5.5476e-03, -3.2011e-02,\n",
       "        -1.8215e-01, -1.2105e-01, -1.7887e-02, -8.9925e-02, -8.1448e-02,\n",
       "        -7.9428e-02, -2.1118e-01,  6.6812e-02, -9.2342e-02,  5.2649e-02,\n",
       "        -1.0321e-01,  1.4077e-01,  3.1935e-02, -5.6155e-02, -8.5528e-02,\n",
       "        -8.5743e-04, -3.5776e-02, -5.5454e-02,  2.0248e-01,  5.4806e-02,\n",
       "        -4.4125e-02,  3.9678e-02,  4.1805e-02, -8.9990e-02,  1.4683e-01,\n",
       "        -6.4514e-02, -8.2189e-02,  9.4912e-03,  2.6657e-01,  1.9248e-02,\n",
       "        -4.7184e-02, -3.0053e-02,  6.9090e-02, -7.2777e-02, -5.8628e-02,\n",
       "         1.5462e-02,  9.4005e-02, -3.0494e-02,  9.6069e-02,  1.0835e-01,\n",
       "         2.7292e-02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_egy = torch.nn.Linear(project_from, project_to, bias=False)\n",
    "#to_reload = torch.from_numpy(torch.load('best_mapping.pth'))\n",
    "mapping_egy.weight.data.copy_(weights.type_as(mapping_egy.weight.data))\n",
    "translated_vector = mapping_egy(vector)\n",
    "translated_vector.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "    \n",
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))\n",
    "\n",
    "def get_nn_oov(word, word_emb, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'vectors-EGY.txt'\n",
    "tgt_path = 'vectors-GLF.txt'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "1.0000 - باشا\n",
      "0.7421 - يباشا\n",
      "0.7217 - اباشا\n",
      "0.6005 - ياباشا\n",
      "0.5972 - حاشا\n"
     ]
    }
   ],
   "source": [
    "# EGY -> EGY\n",
    "get_nn(\"باشا\", src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.4100 - رح\n",
      "0.3553 - جحفلي\n",
      "0.3519 - دماغك\n",
      "0.3513 - وبكدا\n",
      "0.3499 - وعشان\n"
     ]
    }
   ],
   "source": [
    "# EGY -> GLF\n",
    "get_nn(\"باشا\", src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"تسذا\":\n",
      "0.5657 - تاعبني\n",
      "0.5567 - فأملأي\n",
      "0.5515 - بيضحكني\n",
      "0.5455 - تهزقني\n",
      "0.5424 - ستاموني\n"
     ]
    }
   ],
   "source": [
    "# GLF -> EGY\n",
    "get_nn(\"تسذا\", tgt_embeddings, tgt_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"وينك\":\n",
      "0.6053 - سلامتك\n",
      "0.4998 - توحشتك\n",
      "0.4944 - هفاجئك\n",
      "0.4943 - بصيرتك\n",
      "0.4902 - ينورلك\n"
     ]
    }
   ],
   "source": [
    "# GLF -> EGY\n",
    "get_nn(\"وينك\", tgt_embeddings, tgt_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"والصلاة\":\n",
      "0.5644 - اشقائكم\n",
      "0.5584 - للإخوة\n",
      "0.5441 - للأخوة\n",
      "0.5376 - للاخوة\n",
      "0.5351 - للہ\n"
     ]
    }
   ],
   "source": [
    "# GLF -> EGY\n",
    "get_nn(\"والصلاة\", tgt_embeddings, tgt_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"تسذا\":\n",
      "1.0000 - تسذا\n",
      "0.8123 - شذا\n",
      "0.7703 - كيذا\n",
      "0.7524 - وشذا\n",
      "0.7409 - خن\n"
     ]
    }
   ],
   "source": [
    "# GLF -> GLF\n",
    "get_nn(\"تسذا\", tgt_embeddings, tgt_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"طائرة حربية \":\n",
      "0.3770 - وعشان\n",
      "0.3749 - رح\n",
      "0.3689 - جحفلي\n",
      "0.3648 - مارضي\n",
      "0.3643 - ﻻن\n"
     ]
    }
   ],
   "source": [
    "get_nn_oov(\"طائرة حربية \", translated_vector.data, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy Of The Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.4100 - رح\n",
      "0.3553 - جحفلي\n",
      "0.3519 - دماغك\n",
      "0.3513 - وبكدا\n",
      "0.3499 - وعشان\n"
     ]
    }
   ],
   "source": [
    "get_nn(\"باشا\", src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.3770 - وعشان\n",
      "0.3749 - رح\n",
      "0.3689 - جحفلي\n",
      "0.3648 - مارضي\n",
      "0.3643 - ﻻن\n"
     ]
    }
   ],
   "source": [
    "vector = torch.from_numpy(orignal_egy[\"باشا\"])\n",
    "translated_vector = mapping_egy(vector)\n",
    "get_nn_oov(\"باشا\", translated_vector.data, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graduation_project')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
