{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import system\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from pickle import dump, load\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import tensorboard_analysis \n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = True \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.02 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "# model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model_name: str = \"aubmindlab/bert-large-arabertv2\"\n",
    "from_pretrained_classifier: bool = True\n",
    "pretrained_classifier_name: str = \"2021-12-09-train-0.963134765625\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 10\n",
    "warmup_ratio: float = 0.1\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = False\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't touch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# General\n",
    "date = str(datetime.today().date())\n",
    "\n",
    "# Preprocessing\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model \n",
    "pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "\n",
    "# Paths\n",
    "train_path= f\"./models/{date}-train\"\n",
    "search_path = f\"./models/{date}-search\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < data_proportion <= 1, \"data_proportion must be right side inclusively between 0 and 1\"\n",
    "assert 0 <= warmup_ratio <= 1, \"warmup_ratio must be inclusively between 0 and 1\"\n",
    "assert 0 < test_validation_proportion < 1, \"test_validation_proportion must be exclusively between 0 and 1\"\n",
    "assert 0 < sequence_length, \"sequence_length must be positive\"\n",
    "assert 0 < epochs, \"epochs must be positive\"\n",
    "assert 0 < batch_size, \"batch_size must be positive\"\n",
    "assert 0 < validation_size, \"validation_size must be positive\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe ~(2.9 s)\n",
    "df = get_SMADC_folder_data(code_folder_path)\n",
    "df = df.sample(frac=data_proportion)\n",
    "\n",
    "# Encode Y ~(307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    # Preprocess X ~(16min 22s)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "    # split and ~(323ms)\n",
    "    train, test = train_test_split(df, test_size=test_validation_proportion, random_state=seed)\n",
    "    validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=seed)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Tokenize\n",
    "    if tokenize_in_batches:\n",
    "        validate_encoding = batch_tokenize(tokenizer, validate[\"Text\"], 10, sequence_length)\n",
    "        test_encoding = batch_tokenize(tokenizer, test[\"Text\"], 100, sequence_length)\n",
    "        train_encoding = batch_tokenize(tokenizer, train[\"Text\"], 500, sequence_length)\n",
    "    else:\n",
    "        validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "        test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "        train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "    # Make Dataset \n",
    "    validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "    test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "    train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "    save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "    save_preprocessed_data(train_dataset, \"preprocessed_train\")\n",
    "\n",
    "if load_data:\n",
    "    # ~(3mins)\n",
    "    validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "    test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "    train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_args(output_dir, epochs=5, do_warmup=True, warmup_ratio=0.05, save_model=True, eval_while_training=True):\n",
    "    training_args = TrainingArguments(output_dir)\n",
    "\n",
    "    training_args.adam_epsilon = 1e-8\n",
    "    training_args.learning_rate = learning_rate\n",
    "\n",
    "    training_args.fp16 = True\n",
    "\n",
    "    training_args.per_device_train_batch_size = batch_size\n",
    "    training_args.per_device_eval_batch_size = batch_size\n",
    "\n",
    "    training_args.gradient_accumulation_steps = 1\n",
    "    \n",
    "    if epochs:\n",
    "        training_args.num_train_epochs = epochs\n",
    "\n",
    "    steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "    total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "    \n",
    "    if do_warmup:\n",
    "        warmup_ratio = warmup_ratio\n",
    "        training_args.warmup_steps = total_steps * warmup_ratio \n",
    "    \n",
    "    training_args.logging_steps = 10 ** 4\n",
    "    \n",
    "    if eval_while_training:\n",
    "        training_args.evaluation_strategy = \"steps\"\n",
    "        training_args.evaluate_during_training = True\n",
    "        training_args.load_best_model_at_end = True\n",
    "        training_args.eval_steps = 10 ** 4 # defaults to logging_steps\n",
    "        training_args.metric_for_best_model = \"macro_f1\"\n",
    "    \n",
    "    if save_model:\n",
    "        training_args.save_steps = 10 ** 4\n",
    "        training_args.save_total_limit = 120\n",
    "        training_args.save_strategy = \"steps\"\n",
    "\n",
    "    training_args.seed = seed\n",
    "\n",
    "    # training_args.lr_scheduler_type = 'cosine'\n",
    "\n",
    "\n",
    "    return training_args\n",
    "\n",
    "training_args = generate_training_args(\n",
    "                    train_path, epochs=epochs, do_warmup=do_warmup, \n",
    "                    warmup_ratio=warmup_ratio, save_model=save_model_while_training, \n",
    "                    eval_while_training=eval_while_training\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_pretrained_classifier:\n",
    "    trainer = Trainer(\n",
    "    pretrained_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validate_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow listening on http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard:\n",
    "    from tensorboard import program\n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', join(code_folder_path, f\"models\")])\n",
    "    print(f\"Tensorflow listening on {tb.launch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10000/215670 [1:10:07<22:48:25,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8199, 'learning_rate': 4.6313641843642774e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10009/215670 [1:10:10<22:54:43,  2.49it/s]ERROR:tensorboard:File models\\2021-12-08-train\\runs\\Dec08_13-51-22_DESKTOP-QN3OJOT\\events.out.tfevents.1638960710.DESKTOP-QN3OJOT.3312.0 updated even though the current file is models\\2021-12-08-train\\runs\\Dec08_13-51-22_DESKTOP-QN3OJOT\\events.out.tfevents.1638960710.DESKTOP-QN3OJOT.3312.2\n",
      "  9%|▉         | 20000/215670 [2:20:47<23:27:04,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5325, 'learning_rate': 9.267365297227119e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 30000/215670 [3:28:57<22:14:07,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4682, 'learning_rate': 9.566418002720192e-06, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 40000/215670 [4:38:28<20:52:30,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4365, 'learning_rate': 9.051333305856654e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 50000/215670 [5:49:57<20:15:31,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3917, 'learning_rate': 8.536351646539999e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 60000/215670 [6:59:50<17:41:38,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3699, 'learning_rate': 8.021266949676463e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 70000/215670 [8:09:08<16:26:06,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3384, 'learning_rate': 7.506285290359808e-06, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 80000/215670 [9:16:22<15:12:23,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3145, 'learning_rate': 6.991252112269712e-06, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 90000/215670 [10:24:15<13:55:23,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2995, 'learning_rate': 6.476321971726498e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 100000/215670 [11:30:48<12:48:31,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2722, 'learning_rate': 5.961237274862961e-06, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 110000/215670 [12:37:21<11:43:44,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2615, 'learning_rate': 5.446307134319747e-06, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 120000/215670 [13:44:05<10:37:38,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2353, 'learning_rate': 4.9313769937765325e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 130000/215670 [14:50:39<9:29:35,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2349, 'learning_rate': 4.416292296912996e-06, 'epoch': 6.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 140000/215670 [15:57:17<8:24:23,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.208, 'learning_rate': 3.9012591188229e-06, 'epoch': 6.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 150000/215670 [17:03:49<7:16:59,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2078, 'learning_rate': 3.3862259407328037e-06, 'epoch': 6.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 160000/215670 [18:10:23<6:10:46,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1863, 'learning_rate': 2.8711927626427074e-06, 'epoch': 7.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 170000/215670 [19:17:00<5:03:26,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1874, 'learning_rate': 2.356159584552611e-06, 'epoch': 7.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 180000/215670 [20:23:44<3:58:36,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1731, 'learning_rate': 1.841126406462515e-06, 'epoch': 8.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 190000/215670 [21:31:53<3:07:40,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1684, 'learning_rate': 1.32614474714586e-06, 'epoch': 8.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 200000/215670 [22:40:13<1:45:50,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1619, 'learning_rate': 8.11163087829205e-07, 'epoch': 9.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 210000/215670 [23:51:17<1:16:42,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1577, 'learning_rate': 2.9623294728599104e-07, 'epoch': 9.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215670/215670 [24:58:54<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 89935.0351, 'train_samples_per_second': 153.476, 'train_steps_per_second': 2.398, 'train_loss': 0.3019960471451429, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=215670, training_loss=0.3019960471451429, metrics={'train_runtime': 89935.0351, 'train_samples_per_second': 153.476, 'train_steps_per_second': 2.398, 'train_loss': 0.3019960471451429, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    " \n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███       | 66052/215670 [7:54:12<17:32:41,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19428153336048126, 'eval_macro_f1': 0.9271287626651207, 'eval_macro_precision': 0.9325099078535887, 'eval_macro_recall': 0.9221597530877496, 'eval_accuracy': 0.9421782836254881, 'eval_runtime': 44.1897, 'eval_samples_per_second': 544.788, 'eval_steps_per_second': 8.531, 'epoch': 3.06}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19428153336048126,\n",
       " 'eval_macro_f1': 0.9271287626651207,\n",
       " 'eval_macro_precision': 0.9325099078535887,\n",
       " 'eval_macro_recall': 0.9221597530877496,\n",
       " 'eval_accuracy': 0.9421782836254881,\n",
       " 'eval_runtime': 44.1897,\n",
       " 'eval_samples_per_second': 544.788,\n",
       " 'eval_steps_per_second': 8.531,\n",
       " 'epoch': 3.06}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███       | 66052/215670 [7:21:13<17:32:41,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19872522354125977, 'eval_macro_f1': 0.9268216135235413, 'eval_macro_precision': 0.9336408905837104, 'eval_macro_recall': 0.9205536321367577, 'eval_accuracy': 0.943115234375, 'eval_runtime': 7.3076, 'eval_samples_per_second': 560.514, 'eval_steps_per_second': 8.758, 'epoch': 3.06}\n"
     ]
    }
   ],
   "source": [
    "if save_model_after_finish:\n",
    "    trainer.save_model(f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{trainer.evaluate()[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):     \n",
    "    \n",
    "#     training_args = generate_training_args(search_path, epochs=None, do_warmup=False, save_model=False, eval_while_training=False)\n",
    "#     training_args.learning_rate= trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01)\n",
    "#     training_args.weight_decay= trial.suggest_loguniform('weight_decay', 4e-5, 0.01)\n",
    "#     training_args.num_train_epochs= trial.suggest_int('num_train_epochs', low=2, high=5)\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validate_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[TensorBoardCallback()]\n",
    "#     )\n",
    "    \n",
    "#     result = trainer.train()     \n",
    "    \n",
    "#     return result.training_loss # Or result.training_loss[\"metric_name\"] ps: change direction in study if necessary\n",
    "   \n",
    "# We want to minimize the loss! \n",
    "# study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize') \n",
    "# study.optimize(func=objective, n_trials=15)\n",
    "\n",
    "# print(study.best_value) \n",
    "# print(study.best_params) \n",
    "# print(study.best_trial)\n",
    "\n",
    "# trainer.hyperparameter_search(n_trials=10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2deab0c0d56e72ca5db1d97fb88fe0c0ab6b5e910fcaec703ad36be267b2091"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
