{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2021-09-12 23:16:36,765 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "device = torch.device(\"cuda\")\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SMADC_folder_data():\n",
    "    \"\"\"Returns a dataframe with Text and Region columns. Requires tree like this data/SMADC/*.txt\"\"\"\n",
    "    files = glob(\"data/SMADC/*.txt\")\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        region = file[-7:-4]\n",
    "        temp_df = pd.read_csv(file, encoding=\"utf8\", delimiter=\"\\n\", names=[\"Text\"])\n",
    "        temp_df[\"Region\"] = region\n",
    "        dataframes.append(temp_df)\n",
    "        \n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "def tokenize(batch, tokenizer):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch,\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        max_length=sequence_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    batch[\"input_ids\"].to(device)\n",
    "    batch[\"attention_mask\"].to(device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def preprocess_sample(sample, tokenizer):\n",
    "    \"\"\"Sample list of strings\"\"\"\n",
    "    return tokenize(list(arabert_prep.preprocess(text) for text in sample), tokenizer)\n",
    "\n",
    "\n",
    "def compute_metrics(p): \n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    assert len(preds) == len(p.label_ids)\n",
    "\n",
    "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "    macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "    macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "    acc = accuracy_score(p.label_ids,preds)\n",
    "    return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "    }\n",
    "\n",
    "\n",
    "def model_init(model_name, num_labels):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Data logic (load, arabic_preprocess, tokenize, dataset)\n",
    "\n",
    "# Date to dataframe (2.9 s)\n",
    "df = get_SMADC_folder_data()\n",
    "\n",
    "# Encode Y (307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "all_labels = torch.tensor(df[\"Region\"].map(class_to_index.get).values)\n",
    "all_labels.to(device)\n",
    "\n",
    "# Preprocess X (16min 22s)\n",
    "df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "# tokenize and split (2min 26s)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"Text\"], all_labels, random_state=1)\n",
    "x_train, x_test = tokenize(x_train.to_list(), tokenizer), tokenize(x_test.to_list(), tokenizer)\n",
    "\n",
    "# Dataset class\n",
    "class Dialect_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super(Dialect_dataset).__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return InputFeatures(self.X[\"input_ids\"][key], self.X[\"attention_mask\"][key], label=self.Y[key])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "# Make Dataset\n",
    "train_data = Dialect_dataset(x_train, y_train)\n",
    "test_data = Dialect_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_classifier = model_init(model_name, len(classes))\n",
    "dialect_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"./train\")\n",
    "training_args.evaluate_during_training = True\n",
    "# training_args.adam_epsilon = 1e-8\n",
    "# training_args.learning_rate = 5e-5\n",
    "# training_args.fp16 = True\n",
    "training_args.per_device_train_batch_size = 32\n",
    "training_args.per_device_eval_batch_size = 32\n",
    "# training_args.gradient_accumulation_steps = 2\n",
    "training_args.num_train_epochs= 5\n",
    "\n",
    "\n",
    "steps_per_epoch = len(training_data) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(steps_per_epoch)\n",
    "print(total_steps)\n",
    "#Warmup_ratio\n",
    "# warmup_ratio = 0.1\n",
    "# training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
    "\n",
    "training_args.logging_steps = 200\n",
    "training_args.save_steps = 10000\n",
    "training_args.save_total = 10 \n",
    "training_args.seed = 1\n",
    "training_args.lr_scheduler_type = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=dialect_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
