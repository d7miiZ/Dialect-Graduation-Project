{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shelve\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at aubmindlab/bert-base-arabertv2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "[2021-09-30 06:43:08,793 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/vocab.txt from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\af6611146fcbc110b4f831c0428e1fbacc46834e6be67ad005d38282b3a55e56.92809ffe5e568c38fb02e34451b1f9b856d049d10a8f967626ebace17c6bc1c9\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\2c79f1586b3719b467d24700b10ea39810a708e15a96d07faac98e6de8e583d2.40f02d215737071e47e240eb2941705eb18edf27b0126deabe245f3f19f2ee24\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/special_tokens_map.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\eadd29c2f1c1a9561439797b9dc30dd8a23f506fd13fde84b93b5fa3bde392f7.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/tokenizer_config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\9b92fc3002bc77d6c3214a307a407638a9ac0ecb7045096be9599828a5dd2126.8d69c2d6da3751176a19c831b3642f8679a3ff9825be1c07f365a65e652e865c\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Data params\n",
    "validation_size = 4096\n",
    "\n",
    "# Model params\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Preprocessing params\n",
    "sequence_length = 128\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Etc\n",
    "model_string = f\"./models/{str(datetime.today().date())}-train\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SMADC_folder_data():\n",
    "    \"\"\"Returns a dataframe with Text and Region columns. Requires tree like this data/SMADC/*.txt\"\"\"\n",
    "    files = glob(\"data/SMADC/*.txt\")\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        region = file[-7:-4]\n",
    "        temp_df = pd.read_csv(file, encoding=\"utf8\", delimiter=\"\\n\", names=[\"Text\"])\n",
    "        temp_df[\"Region\"] = region\n",
    "        dataframes.append(temp_df)\n",
    "        \n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "def get_music_df():\n",
    "    files = [\"GLF\",\"LEV\",\"NOR\",\"IRQ\"]\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in files:\n",
    "        temp_df = pd.read_csv(f'../extra_data/d7_data/{file}.txt', encoding=\"utf8\", delimiter=\"\\n\", names=[\"Text\"])\n",
    "        temp_df[\"Region\"] = file\n",
    "        dataframes.append(temp_df)\n",
    "    \n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of strings\n",
    "    \"\"\"\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        batch,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=sequence_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def batch_tokenize_iter(data, batch_size):\n",
    "    len_data = len(data)\n",
    "    batch_num = len_data // batch_size\n",
    "    batch_rest = len_data / batch_size - batch_num\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        yield tokenize(data[i * batch_num:(i+1) * batch_num].to_list())\n",
    "        \n",
    "    if batch_rest:\n",
    "        yield tokenize(data[batch_num:].to_list())\n",
    "\n",
    "\n",
    "def batch_tokenize(data, batch_size):\n",
    "    bt = batch_tokenize_iter(data, batch_size)\n",
    "    for i, tokenization in enumerate(bt):\n",
    "        if not i:\n",
    "            encoding = tokenization\n",
    "            continue\n",
    "        encoding[\"input_ids\"] = torch.cat([encoding[\"input_ids\"], tokenization[\"input_ids\"]])\n",
    "        encoding[\"attention_mask\"] = torch.cat([encoding[\"attention_mask\"], tokenization[\"attention_mask\"]])\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    \"\"\"Sample list of strings\"\"\"\n",
    "    return tokenize(list(arabert_prep.preprocess(text) for text in sample))\n",
    "\n",
    "\n",
    "def save_preprocessed_data(dataset, dataset_name):\n",
    "    with open(f\"preprocessed_data/{dataset_name}.pkl\", \"wb\") as file:\n",
    "        dump(dataset, file)\n",
    "        \n",
    "def load_preprocessed_data(dataset_name):\n",
    "    with open(f\"preprocessed_data/{dataset_name}.pkl\", \"rb\") as file:\n",
    "        temp = load(file)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def compute_metrics(p): \n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    assert len(preds) == len(p.label_ids)\n",
    "\n",
    "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "    macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "    macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "    acc = accuracy_score(p.label_ids,preds)\n",
    "    return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "    }\n",
    "\n",
    "\n",
    "def model_init(model_name, num_labels, label2id, id2label):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dataset class\n",
    "class Dialect_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super(Dialect_dataset).__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return InputFeatures(self.X[\"input_ids\"][key], self.X[\"attention_mask\"][key], label=self.Y[key])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe (2.9 s)\n",
    "df = get_SMADC_folder_data()\n",
    "\n",
    "# Encode Y (307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing (If you want to load data, skip until loading section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Preprocess X (16min 22s)\n",
    "df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "# split and (323ms)\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=1)\n",
    "validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=1)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "validate.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize everything at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validate_encoding = tokenize(validate[\"Text\"].to_list())\n",
    "test_encoding = tokenize(test[\"Text\"].to_list())\n",
    "train_encoding = tokenize(list(train[\"Text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# validate_encoding = batch_tokenize(validate[\"Text\"], 10)\n",
    "# test_encoding = batch_tokenize(test[\"Text\"], 100)\n",
    "# train_encoding = batch_tokenize(train[\"Text\"], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "%%time    \n",
    "# Make Dataset \n",
    "validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "# save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "# save_preprocessed_data(train_dataset, \"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "# test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "# train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 19806. Total steps: 99030\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(model_string)\n",
    "\n",
    "# training_args.adam_epsilon = 1e-8\n",
    "# training_args.learning_rate = 5e-5\n",
    "\n",
    "training_args.fp16 = True\n",
    "\n",
    "training_args.per_device_train_batch_size = 32\n",
    "training_args.per_device_eval_batch_size = 32\n",
    "\n",
    "training_args.gradient_accumulation_steps = 2\n",
    "\n",
    "training_args.num_train_epochs= 5\n",
    "\n",
    "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(f\"Steps per epoch: {steps_per_epoch}. Total steps: {total_steps}\")\n",
    "\n",
    "warmup_ratio = 0.05\n",
    "training_args.warmup_steps = total_steps * warmup_ratio \n",
    "\n",
    "training_args.logging_steps = 1000\n",
    "training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "training_args.save_strategy = EvaluationStrategy.STEPS\n",
    "training_args.eval_strategy = EvaluationStrategy.STEPS\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.eval_steps = 1000 # defaults to logging_steps\n",
    " \n",
    "training_args.greater_is_better = False # Loss lower is better\n",
    "\n",
    "training_args.save_steps = 1000\n",
    "training_args.save_total_limit = 10\n",
    "\n",
    "training_args.seed = 1\n",
    "\n",
    "training_args.lr_scheduler_type = 'cosine'\n",
    "\n",
    "training_args.metric_for_best_model= \"eval_loss\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "EarlyStoppingCallback\n",
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1267610\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 99030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2044' max='99030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2044/99030 15:51 < 12:33:16, 2.15 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(trainer.args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.501278042793274,\n",
       " 'eval_macro_f1': 0.20058725173315345,\n",
       " 'eval_macro_precision': 0.22795545972387604,\n",
       " 'eval_macro_recall': 0.23674922271704166,\n",
       " 'eval_accuracy': 0.412109375}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eval_loss', 'eval_macro_f1', 'eval_macro_precision',\n",
       "       'eval_macro_recall', 'eval_accuracy', 'eval_runtime',\n",
       "       'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'step'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_macro_precision</th>\n",
       "      <th>eval_macro_recall</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.501278</td>\n",
       "      <td>0.200587</td>\n",
       "      <td>0.227955</td>\n",
       "      <td>0.236749</td>\n",
       "      <td>0.412109</td>\n",
       "      <td>12.5894</td>\n",
       "      <td>325.352</td>\n",
       "      <td>10.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_macro_f1  eval_macro_precision  eval_macro_recall  \\\n",
       "0   1.501278       0.200587              0.227955           0.236749   \n",
       "\n",
       "   eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0       0.412109       12.5894                  325.352   \n",
       "\n",
       "   eval_steps_per_second  epoch  step  \n",
       "0                 10.167    0.0    89  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value of 'y' is not the name of a column in 'data_frame'. Expected one of ['eval_loss', 'eval_macro_f1', 'eval_macro_precision', 'eval_macro_recall', 'eval_accuracy', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'step'] but received: loss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-3864a9d076c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"step\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"plotly\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Steps over loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;31m# when using another backend, get out of the way\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"pandas.plotting._matplotlib\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_kinds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\plotly\\__init__.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(data_frame, kind, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"line\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"area\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\plotly\\express\\_chart_types.py\u001b[0m in \u001b[0;36mline\u001b[1;34m(data_frame, x, y, line_group, color, line_dash, hover_name, hover_data, custom_data, text, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, error_x, error_x_minus, error_y, error_y_minus, animation_frame, animation_group, category_orders, labels, orientation, color_discrete_sequence, color_discrete_map, line_dash_sequence, line_dash_map, log_x, log_y, range_x, range_y, line_shape, render_mode, title, template, width, height)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mpolyline\u001b[0m \u001b[0mmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmake_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\plotly\\express\\_core.py\u001b[0m in \u001b[0;36mmake_figure\u001b[1;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     \u001b[0mapply_default_cascade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreemap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSunburst\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"path\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_dataframe_hierarchy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\plotly\\express\\_core.py\u001b[0m in \u001b[0;36mbuild_dataframe\u001b[1;34m(args, constructor)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[1;31m# now that things have been prepped, we do the systematic rewriting of `args`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m     df_output, wide_id_vars = process_args_into_dataframe(\n\u001b[0m\u001b[0;32m   1378\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwide_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m     )\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\plotly\\express\\_core.py\u001b[0m in \u001b[0;36mprocess_args_into_dataframe\u001b[1;34m(args, wide_mode, var_name, value_name)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0margument\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n To use the index, pass it in directly as `df.index`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                     raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Value of 'y' is not the name of a column in 'data_frame'. Expected one of ['eval_loss', 'eval_macro_f1', 'eval_macro_precision', 'eval_macro_recall', 'eval_accuracy', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'step'] but received: loss"
     ]
    }
   ],
   "source": [
    "history.plot(x=\"step\", y=\"loss\", backend=\"plotly\", title=\"Steps over loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "rounded_epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "xaxis": "x",
         "y": [
          0.3647,
          0.2209,
          0.1991,
          0.1894,
          0.1842,
          0.177,
          0.1736,
          0.1664,
          0.1641,
          0.1627,
          0.16,
          0.1586,
          0.1565,
          0.1536,
          0.1526,
          0.1506,
          0.1483,
          0.1477
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "autosize": true,
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Steps over loss"
        },
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "title": {
          "text": "rounded_epoch"
         },
         "type": "linear"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          0.13564444444444443,
          0.37675555555555557
         ],
         "title": {
          "text": "loss"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"71e68c58-d7d7-4772-9ce4-f6f1e75a0589\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"71e68c58-d7d7-4772-9ce4-f6f1e75a0589\")) {                    Plotly.newPlot(                        \"71e68c58-d7d7-4772-9ce4-f6f1e75a0589\",                        [{\"hovertemplate\": \"rounded_epoch=%{x}<br>loss=%{y}<extra></extra>\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \"xaxis\": \"x\", \"y\": [0.3647, 0.2209, 0.1991, 0.1894, 0.1842, 0.177, 0.1736, 0.1664, 0.1641, 0.1627, 0.16, 0.1586, 0.1565, 0.1536, 0.1526, 0.1506, 0.1483, 0.1477], \"yaxis\": \"y\"}],                        {\"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Steps over loss\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"rounded_epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('71e68c58-d7d7-4772-9ce4-f6f1e75a0589');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history[\"rounded_epoch\"] = round(history[\"epoch\"])\n",
    "history.plot(x=\"rounded_epoch\", y=\"loss\", backend=\"plotly\", title=\"Steps over loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.line(history, x=\"step\", y=\"loss\", title=\"Steps over loss\")\n",
    "# fig.add_trace(\n",
    "#     next(px.line(history, x=\"step\", y=\"\").select_traces())\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5),\n",
    "        \"gradient_accumulation_steps\" : trail.suggest_int(\"gradient_accumulation_steps\" , 1 , 10)\n",
    "    }\n",
    "\n",
    "search_space = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5),\n",
    "        \"gradient_accumulation_steps\" : trail.suggest_int(\"gradient_accumulation_steps\" , 1 , 10)\n",
    "}\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics['eval_accuracy'] # or try \"accuracy\" if didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_model = trainer.hyperparameter_search(direction=\"maximize\",\n",
    "                                         hp_space=hp_space,\n",
    "                                         compute_objective=my_objective,\n",
    "                                         n_trials=None,\n",
    "                                         pruner=optuna.pruners.NopPruner(),\n",
    "                                         sampler=optuna.samplers.GridSampler(search_space),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-15 20:31:54,311]\u001b[0m A new study created in memory with name: no-name-3b5ad9cf-10eb-4575-b942-a0ce741ac9e2\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1267610\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475353' max='475353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475353/475353 17:22:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning:\n",
      "\n",
      "Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "\u001b[32m[I 2021-09-16 13:54:54,627]\u001b[0m Trial 0 finished with value: 3.4530827368971555 and parameters: {'learning_rate': 1.380855075991113e-05, 'num_train_epochs': 3, 'seed': 37, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 3.4530827368971555.\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1267610\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 29709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29709' max='29709' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29709/29709 5:13:12, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning:\n",
      "\n",
      "Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-c02adcc4533f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameter_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[1;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m         \u001b[0mrun_hp_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_hp_search_optuna\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mHPSearchBackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTUNA\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrun_hp_search_ray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mbest_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp_search_backend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\integrations.py\u001b[0m in \u001b[0;36mrun_hp_search_optuna\u001b[1;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_jobs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_objective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\integrations.py\u001b[0m in \u001b[0;36m_objective\u001b[1;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[0;32m    138\u001b[0m                     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;31m# If there hasn't been any evaluation during the training loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1282\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1800\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwarez\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#trainer.hyperparameter_search(n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelWrapper(torch.nn.Module):\n",
    "#     def __init__(self, model: torch.nn.Module):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, input_x: torch.Tensor):\n",
    "#         data = self.model(input_x)\n",
    "\n",
    "#         if isinstance(data, dict):\n",
    "#             data_named_tuple = namedtuple(\"ModelEndpoints\", sorted(data.keys())) \n",
    "#             data = data_named_tuple(**data)\n",
    "\n",
    "#         elif isinstance(data, list):\n",
    "#             data = tuple(data)\n",
    "\n",
    "#         return data\n",
    "    \n",
    "# writer = SummaryWriter()\n",
    "# _, tensorboard = train_test_split(validate, test_size=100, random_state=1)\n",
    "# costume_data = validate_dataset.X[\"input_ids\"][:100].cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
