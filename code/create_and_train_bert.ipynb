{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import system\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from pickle import dump, load\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import tensorboard_analysis \n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "seed = 1\n",
    "\n",
    "# Data params\n",
    "data_proportion = 1.0 # propotion of data to be loaded in df\n",
    "load_data = True \n",
    "save_data = False\n",
    "test_validation_proportion = 0.1 # test and validation proportion from df\n",
    "\n",
    "# Model params\n",
    "# model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model_name = \"aubmindlab/bert-large-arabertv2\"\n",
    "    \n",
    "# Preprocessing params\n",
    "sequence_length = 64\n",
    "tokenize_in_batches = False # Helps reduce memory footprint\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Paths\n",
    "code_folder_path = \"\"\n",
    "train_path= f\"./models/{str(datetime.today().date())}-train\"\n",
    "search_path = f\"./models/{str(datetime.today().date())}-search\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)\n",
    "\n",
    "# Training params\n",
    "validation_size = 4096\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "epochs = 5\n",
    "warmup_ratio = 0.1\n",
    "save_model_while_training = True # doesn't work, transformers is terrible\n",
    "do_warmup = True\n",
    "eval_while_training = True\n",
    "save_model_after_finish = True\n",
    "\n",
    "# Etc\n",
    "open_tensorboard = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe ~(2.9 s)\n",
    "df = get_SMADC_folder_data(code_folder_path)\n",
    "df = df.sample(frac=data_proportion)\n",
    "\n",
    "# Encode Y ~(307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    # Preprocess X ~(16min 22s)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "    # split and ~(323ms)\n",
    "    train, test = train_test_split(df, test_size=test_validation_proportion, random_state=seed)\n",
    "    validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=seed)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Tokenize\n",
    "    if tokenize_in_batches:\n",
    "        validate_encoding = batch_tokenize(tokenizer, validate[\"Text\"], 10, sequence_length)\n",
    "        test_encoding = batch_tokenize(tokenizer, test[\"Text\"], 100, sequence_length)\n",
    "        train_encoding = batch_tokenize(tokenizer, train[\"Text\"], 500, sequence_length)\n",
    "    else:\n",
    "        validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "        test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "        train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "    # Make Dataset \n",
    "    validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "    test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "    train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "if save_data:\n",
    "    save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "    save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "    save_preprocessed_data(train_dataset, \"preprocessed_train\")\n",
    "\n",
    "if load_data:\n",
    "    # ~(3mins)\n",
    "    validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "    test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "    train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_args(output_dir, epochs=5, do_warmup=True, warmup_ratio=0.05, save_model=True, eval_while_training=True):\n",
    "    training_args = TrainingArguments(output_dir)\n",
    "\n",
    "    training_args.adam_epsilon = 1e-8\n",
    "    training_args.learning_rate = learning_rate\n",
    "\n",
    "    training_args.fp16 = True\n",
    "\n",
    "    training_args.per_device_train_batch_size = batch_size\n",
    "    training_args.per_device_eval_batch_size = batch_size\n",
    "\n",
    "    training_args.gradient_accumulation_steps = 1\n",
    "    \n",
    "    if epochs:\n",
    "        training_args.num_train_epochs = epochs\n",
    "\n",
    "    steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "    total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "    \n",
    "    if do_warmup:\n",
    "        warmup_ratio = warmup_ratio\n",
    "        training_args.warmup_steps = total_steps * warmup_ratio \n",
    "    \n",
    "    training_args.logging_steps = 1024\n",
    "    \n",
    "    if eval_while_training:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "        training_args.eval_strategy = EvaluationStrategy.STEPS\n",
    "        training_args.evaluate_during_training = True\n",
    "        training_args.load_best_model_at_end = True\n",
    "        training_args.eval_steps = 1024 # defaults to logging_steps\n",
    "        training_args.metric_for_best_model= \"eval_loss\"\n",
    "    \n",
    "    if save_model:\n",
    "        training_args.save_steps = 1024\n",
    "        training_args.save_total_limit = 10\n",
    "        training_args.save_strategy = \"steps\"\n",
    "\n",
    "\n",
    "    training_args.seed = seed\n",
    "\n",
    "    training_args.lr_scheduler_type = 'cosine'\n",
    "\n",
    "    return training_args\n",
    "\n",
    "training_args = generate_training_args(\n",
    "                    train_path, epochs=epochs, do_warmup=do_warmup, \n",
    "                    warmup_ratio=warmup_ratio, save_model=save_model_while_training, \n",
    "                    eval_while_training=eval_while_training\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow listening on http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard:\n",
    "    from tensorboard import program\n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', join(code_folder_path, f\"models\")])\n",
    "    print(f\"Tensorflow listening on {tb.launch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/198065 [01:05<24:19:41,  2.26it/s]"
     ]
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    " \n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 21588/198065 [3:34:24<21:11:39,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49099254608154297, 'eval_macro_f1': 0.7990344686502608, 'eval_macro_precision': 0.82856805555997, 'eval_macro_recall': 0.7783906309962003, 'eval_accuracy': 0.831787109375, 'eval_runtime': 14.9573, 'eval_samples_per_second': 273.845, 'eval_steps_per_second': 8.558, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49099254608154297,\n",
       " 'eval_macro_f1': 0.7990344686502608,\n",
       " 'eval_macro_precision': 0.82856805555997,\n",
       " 'eval_macro_recall': 0.7783906309962003,\n",
       " 'eval_accuracy': 0.831787109375,\n",
       " 'eval_runtime': 14.9573,\n",
       " 'eval_samples_per_second': 273.845,\n",
       " 'eval_steps_per_second': 8.558,\n",
       " 'epoch': 0.54}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:13<00:00,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "if save_model_after_finish:\n",
    "    trainer.save_model(f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{trainer.evaluate()[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):     \n",
    "    \n",
    "#     training_args = generate_training_args(search_path, epochs=None, do_warmup=False, save_model=False, eval_while_training=False)\n",
    "#     training_args.learning_rate= trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01)\n",
    "#     training_args.weight_decay= trial.suggest_loguniform('weight_decay', 4e-5, 0.01)\n",
    "#     training_args.num_train_epochs= trial.suggest_int('num_train_epochs', low=2, high=5)\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validate_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[TensorBoardCallback()]\n",
    "#     )\n",
    "    \n",
    "#     result = trainer.train()     \n",
    "    \n",
    "#     return result.training_loss # Or result.training_loss[\"metric_name\"] ps: change direction in study if necessary\n",
    "   \n",
    "# We want to minimize the loss! \n",
    "# study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize') \n",
    "# study.optimize(func=objective, n_trials=15)\n",
    "\n",
    "# print(study.best_value) \n",
    "# print(study.best_params) \n",
    "# print(study.best_trial)\n",
    "\n",
    "# trainer.hyperparameter_search(n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2deab0c0d56e72ca5db1d97fb88fe0c0ab6b5e910fcaec703ad36be267b2091"
  },
  "kernelspec": {
   "display_name": "graduation_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
