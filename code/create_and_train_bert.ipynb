{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import system\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from pickle import dump, load\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import tensorboard_analysis \n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = True \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.02 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "# model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model_name: str = \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "from_pretrained_classifier: bool = True\n",
    "pretrained_classifier_name: str = \"2021-12-09-train-0.963134765625\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 6\n",
    "warmup_ratio: float = 0.1\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = False\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't touch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 476/476 [00:00<00:00, 476kB/s]\n",
      "Downloading: 100%|██████████| 733k/733k [00:02<00:00, 361kB/s]\n",
      "Downloading: 100%|██████████| 1.19M/1.19M [00:03<00:00, 365kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 89.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# General\n",
    "date = str(datetime.today().date())\n",
    "\n",
    "# Data\n",
    "labels = [\"EGY\", \"GLF\", \"IRQ\", \"LEV\", \"NOR\"]\n",
    "\n",
    "# Preprocessing\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model \n",
    "if from_pretrained_classifier:\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "\n",
    "# Paths\n",
    "train_path= f\"./models/{date}-train\"\n",
    "search_path = f\"./models/{date}-search\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < data_proportion <= 1, \"data_proportion must be right side inclusively between 0 and 1\"\n",
    "assert 0 <= warmup_ratio <= 1, \"warmup_ratio must be inclusively between 0 and 1\"\n",
    "assert 0 < test_validation_proportion < 1, \"test_validation_proportion must be exclusively between 0 and 1\"\n",
    "assert 0 < sequence_length, \"sequence_length must be positive\"\n",
    "assert 0 < epochs, \"epochs must be positive\"\n",
    "assert 0 < batch_size, \"batch_size must be positive\"\n",
    "assert 0 < validation_size, \"validation_size must be positive\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.31 s\n",
      "Compiler : 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe ~(2.9 s)\n",
    "df = get_SMADC_folder_data(code_folder_path)\n",
    "df = df.sample(frac=data_proportion)\n",
    "\n",
    "# Encode Y ~(307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    # Preprocess X ~(16min 22s)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "    # split and ~(323ms)\n",
    "    train, test = train_test_split(df, test_size=test_validation_proportion, random_state=seed)\n",
    "    validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=seed)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Tokenize\n",
    "    if tokenize_in_batches:\n",
    "        validate_encoding = batch_tokenize(tokenizer, validate[\"Text\"], 10, sequence_length)\n",
    "        test_encoding = batch_tokenize(tokenizer, test[\"Text\"], 100, sequence_length)\n",
    "        train_encoding = batch_tokenize(tokenizer, train[\"Text\"], 500, sequence_length)\n",
    "    else:\n",
    "        validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "        test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "        train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "    # Make Dataset \n",
    "    validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "    test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "    train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "    save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "    save_preprocessed_data(train_dataset, \"preprocessed_train\")\n",
    "\n",
    "if load_data:\n",
    "    # ~(3mins)\n",
    "    validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "    test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "    train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = generate_training_args(\n",
    "                    train_path, epochs=epochs, do_warmup=do_warmup, warmup_ratio=warmup_ratio, \n",
    "                    save_model=save_model_while_training, eval_while_training=eval_while_training, \n",
    "                    learning_rate=learning_rate,batch_size=batch_size, \n",
    "                    train_dataset_length=len(train_dataset), seed=seed\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_pretrained_classifier:\n",
    "    trainer = Trainer(\n",
    "    pretrained_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validate_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow listening on http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard:\n",
    "    from tensorboard import program\n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', join(code_folder_path, f\"models\")])\n",
    "    print(f\"Tensorflow listening on {tb.launch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 10000/64701 [1:29:06<6:04:16,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8545, 'learning_rate': 8.455974405341495e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 10011/64701 [1:29:11<6:05:49,  2.49it/s]ERROR:tensorboard:File models\\2022-01-29-train\\runs\\Jan29_20-36-22_At-Low-desktop\\events.out.tfevents.1643477870.At-Low-desktop.29816.0 updated even though the current file is models\\2022-01-29-train\\runs\\Jan29_20-36-22_At-Low-desktop\\events.out.tfevents.1643477870.At-Low-desktop.29816.2\n",
      " 31%|███       | 20000/64701 [2:39:09<4:58:44,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6552, 'learning_rate': 6.911021467983494e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 30000/64701 [4:07:23<14:00:16,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5868, 'learning_rate': 5.3657594163923276e-06, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 40000/64701 [6:12:46<2:44:03,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5584, 'learning_rate': 3.820806479034327e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 50000/64701 [7:19:09<1:37:31,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5219, 'learning_rate': 2.2756989845597446e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 60000/64701 [8:25:32<31:13,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5091, 'learning_rate': 7.304369329685785e-07, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64701/64701 [8:56:45<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32205.17, 'train_samples_per_second': 128.577, 'train_steps_per_second': 2.009, 'train_loss': 0.6062105185053168, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=64701, training_loss=0.6062105185053168, metrics={'train_runtime': 32205.17, 'train_samples_per_second': 128.577, 'train_steps_per_second': 2.009, 'train_loss': 0.6062105185053168, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    " \n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:39<00:00,  9.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5220786333084106,\n",
       " 'eval_macro_f1': 0.7832360568610907,\n",
       " 'eval_macro_precision': 0.7910669284220544,\n",
       " 'eval_macro_recall': 0.7766100128430244,\n",
       " 'eval_accuracy': 0.8221317604054166,\n",
       " 'eval_runtime': 40.2447,\n",
       " 'eval_samples_per_second': 598.19,\n",
       " 'eval_steps_per_second': 9.368,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:06<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "if save_model_after_finish:\n",
    "    trainer.save_model(f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{trainer.evaluate()[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):     \n",
    "    \n",
    "#     training_args = generate_training_args(search_path, epochs=None, do_warmup=False, save_model=False, eval_while_training=False)\n",
    "#     training_args.learning_rate= trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01)\n",
    "#     training_args.weight_decay= trial.suggest_loguniform('weight_decay', 4e-5, 0.01)\n",
    "#     training_args.num_train_epochs= trial.suggest_int('num_train_epochs', low=2, high=5)\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validate_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[TensorBoardCallback()]\n",
    "#     )\n",
    "    \n",
    "#     result = trainer.train()     \n",
    "    \n",
    "#     return result.training_loss # Or result.training_loss[\"metric_name\"] ps: change direction in study if necessary\n",
    "   \n",
    "# We want to minimize the loss! \n",
    "# study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize') \n",
    "# study.optimize(func=objective, n_trials=15)\n",
    "\n",
    "# print(study.best_value) \n",
    "# print(study.best_params) \n",
    "# print(study.best_trial)\n",
    "\n",
    "# trainer.hyperparameter_search(n_trials=10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graduation_project')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
