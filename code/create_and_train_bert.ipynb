{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import system\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from pickle import dump, load\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import tensorboard_analysis \n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = True \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.02 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "# model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model_name: str = \"aubmindlab/bert-large-arabertv2\"\n",
    "from_pretrained_classifier: bool = True\n",
    "pretrained_classifier_name: str = \"2021-12-09-train-0.963134765625\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 10\n",
    "warmup_ratio: float = 0.1\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = False\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't touch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# General\n",
    "date = str(datetime.today().date())\n",
    "\n",
    "# Data\n",
    "labels = [\"EGY\", \"GLF\", \"IRQ\", \"LEV\", \"NOR\"]\n",
    "\n",
    "# Preprocessing\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model \n",
    "if from_pretrained_classifier:\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "\n",
    "# Paths\n",
    "train_path= f\"./models/{date}-train\"\n",
    "search_path = f\"./models/{date}-search\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < data_proportion <= 1, \"data_proportion must be right side inclusively between 0 and 1\"\n",
    "assert 0 <= warmup_ratio <= 1, \"warmup_ratio must be inclusively between 0 and 1\"\n",
    "assert 0 < test_validation_proportion < 1, \"test_validation_proportion must be exclusively between 0 and 1\"\n",
    "assert 0 < sequence_length, \"sequence_length must be positive\"\n",
    "assert 0 < epochs, \"epochs must be positive\"\n",
    "assert 0 < batch_size, \"batch_size must be positive\"\n",
    "assert 0 < validation_size, \"validation_size must be positive\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe ~(2.9 s)\n",
    "df = get_SMADC_folder_data(code_folder_path)\n",
    "df = df.sample(frac=data_proportion)\n",
    "\n",
    "# Encode Y ~(307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    # Preprocess X ~(16min 22s)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "    # split and ~(323ms)\n",
    "    train, test = train_test_split(df, test_size=test_validation_proportion, random_state=seed)\n",
    "    validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=seed)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Tokenize\n",
    "    if tokenize_in_batches:\n",
    "        validate_encoding = batch_tokenize(tokenizer, validate[\"Text\"], 10, sequence_length)\n",
    "        test_encoding = batch_tokenize(tokenizer, test[\"Text\"], 100, sequence_length)\n",
    "        train_encoding = batch_tokenize(tokenizer, train[\"Text\"], 500, sequence_length)\n",
    "    else:\n",
    "        validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "        test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "        train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "    # Make Dataset \n",
    "    validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "    test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "    train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "    save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "    save_preprocessed_data(train_dataset, \"preprocessed_train\")\n",
    "\n",
    "if load_data:\n",
    "    # ~(3mins)\n",
    "    validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "    test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "    train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_training_args() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25544/1543161958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m training_args = generate_training_args(\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_warmup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_warmup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarmup_ratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0msave_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_model_while_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_while_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_while_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mtrain_dataset_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_training_args() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "training_args = generate_training_args(\n",
    "                    train_path, epochs=epochs, do_warmup=do_warmup, warmup_ratio=warmup_ratio, \n",
    "                    save_model=save_model_while_training, eval_while_training=eval_while_training, \n",
    "                    learning_rate=learning_rate,batch_size=batch_size, \n",
    "                    train_dataset_length=len(train_dataset), seed=seed\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_pretrained_classifier:\n",
    "    trainer = Trainer(\n",
    "    pretrained_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validate_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow listening on http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard:\n",
    "    from tensorboard import program\n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', join(code_folder_path, f\"models\")])\n",
    "    print(f\"Tensorflow listening on {tb.launch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10000/215670 [1:08:05<22:42:46,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0585, 'learning_rate': 9.53660685306255e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10011/215670 [1:08:09<22:58:19,  2.49it/s]ERROR:tensorboard:File models\\2021-12-11-train\\runs\\Dec11_02-21-18_DESKTOP-QN3OJOT\\events.out.tfevents.1639179045.DESKTOP-QN3OJOT.25544.0 updated even though the current file is models\\2021-12-11-train\\runs\\Dec11_02-21-18_DESKTOP-QN3OJOT\\events.out.tfevents.1639179045.DESKTOP-QN3OJOT.25544.2\n",
      "  9%|▉         | 20000/215670 [2:14:19<21:37:15,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0638, 'learning_rate': 9.073120971855149e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 30000/215670 [3:20:34<20:30:02,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0597, 'learning_rate': 8.60963509064775e-06, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 40000/215670 [4:26:48<19:23:18,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0619, 'learning_rate': 8.146102842305374e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 50000/215670 [5:33:02<18:17:44,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0571, 'learning_rate': 7.682616961097974e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 60000/215670 [6:39:16<17:13:36,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0567, 'learning_rate': 7.219223814160523e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 70000/215670 [7:46:26<17:16:15,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0541, 'learning_rate': 6.755645198683174e-06, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 80000/215670 [8:58:55<16:24:10,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.053, 'learning_rate': 6.2921593174757735e-06, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 90000/215670 [10:10:07<14:42:31,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0516, 'learning_rate': 5.828673436268374e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 100000/215670 [11:18:38<12:51:16,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0485, 'learning_rate': 5.3651411879259985e-06, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 110000/215670 [12:25:50<11:55:27,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0485, 'learning_rate': 4.901608939583624e-06, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 120000/215670 [13:33:27<10:32:29,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0448, 'learning_rate': 4.438076691241249e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 130000/215670 [14:41:31<9:46:43,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0465, 'learning_rate': 3.9746835443037975e-06, 'epoch': 6.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 140000/215670 [15:52:22<8:45:53,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0421, 'learning_rate': 3.511104928826448e-06, 'epoch': 6.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 150000/215670 [16:59:40<7:16:22,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0436, 'learning_rate': 3.0475726804840732e-06, 'epoch': 6.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 160000/215670 [18:11:19<6:36:43,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0401, 'learning_rate': 2.5841331664116476e-06, 'epoch': 7.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 170000/215670 [19:21:43<5:04:27,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0407, 'learning_rate': 2.120647285204247e-06, 'epoch': 7.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 180000/215670 [20:29:08<3:59:36,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0398, 'learning_rate': 1.6570686697268978e-06, 'epoch': 8.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 190000/215670 [21:36:52<2:57:08,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0408, 'learning_rate': 1.1936291556544722e-06, 'epoch': 8.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 200000/215670 [23:19:35<1:49:20,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0404, 'learning_rate': 7.301896415820467e-07, 'epoch': 9.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 210000/215670 [24:27:52<37:50,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0426, 'learning_rate': 2.667037603746465e-07, 'epoch': 9.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215670/215670 [25:05:35<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 90335.7877, 'train_samples_per_second': 152.795, 'train_steps_per_second': 2.387, 'train_loss': 0.049227373370177095, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=215670, training_loss=0.049227373370177095, metrics={'train_runtime': 90335.7877, 'train_samples_per_second': 152.795, 'train_steps_per_second': 2.387, 'train_loss': 0.049227373370177095, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    " \n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:39<00:00,  9.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.23767970502376556,\n",
       " 'eval_macro_f1': 0.9432873249466805,\n",
       " 'eval_macro_precision': 0.9478332012719948,\n",
       " 'eval_macro_recall': 0.9389531666017248,\n",
       " 'eval_accuracy': 0.9558444795214754,\n",
       " 'eval_runtime': 39.6862,\n",
       " 'eval_samples_per_second': 606.61,\n",
       " 'eval_steps_per_second': 9.5,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:06<00:00,  9.86it/s]\n"
     ]
    }
   ],
   "source": [
    "if save_model_after_finish:\n",
    "    trainer.save_model(f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{trainer.evaluate()[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):     \n",
    "    \n",
    "#     training_args = generate_training_args(search_path, epochs=None, do_warmup=False, save_model=False, eval_while_training=False)\n",
    "#     training_args.learning_rate= trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01)\n",
    "#     training_args.weight_decay= trial.suggest_loguniform('weight_decay', 4e-5, 0.01)\n",
    "#     training_args.num_train_epochs= trial.suggest_int('num_train_epochs', low=2, high=5)\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validate_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[TensorBoardCallback()]\n",
    "#     )\n",
    "    \n",
    "#     result = trainer.train()     \n",
    "    \n",
    "#     return result.training_loss # Or result.training_loss[\"metric_name\"] ps: change direction in study if necessary\n",
    "   \n",
    "# We want to minimize the loss! \n",
    "# study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize') \n",
    "# study.optimize(func=objective, n_trials=15)\n",
    "\n",
    "# print(study.best_value) \n",
    "# print(study.best_params) \n",
    "# print(study.best_trial)\n",
    "\n",
    "# trainer.hyperparameter_search(n_trials=10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2deab0c0d56e72ca5db1d97fb88fe0c0ab6b5e910fcaec703ad36be267b2091"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
