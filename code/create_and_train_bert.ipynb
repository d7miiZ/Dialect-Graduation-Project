{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from os import system\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from pickle import dump, load\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import tensorboard_analysis \n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed: int = 1\n",
    "\n",
    "# Data \n",
    "data_proportion: float = 1.0 # propotion of data to be loaded in df\n",
    "load_data: bool = True \n",
    "save_data: bool = False\n",
    "test_validation_proportion: float = 0.02 # test and validation proportion from df\n",
    "\n",
    "# Model \n",
    "model_name = \"aubmindlab/bert-large-arabertv2\"\n",
    "from_pretrained_classifier: bool = True\n",
    "pretrained_classifier_name: str = \"2021-12-05-train-0.898193359375\"\n",
    "    \n",
    "# Preprocessing \n",
    "sequence_length: int = 32\n",
    "tokenize_in_batches: bool = False # Helps reduce memory footprint\n",
    "\n",
    "# Paths\n",
    "code_folder_path: str = \"\"\n",
    "\n",
    "# Training \n",
    "validation_size: int = 4096\n",
    "batch_size: int = 64\n",
    "learning_rate: float = 1e-5\n",
    "epochs: int = 6\n",
    "warmup_ratio: float = 0.1\n",
    "save_model_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "do_warmup: bool = False\n",
    "eval_while_training: bool = True # maybe doesn't work, transformers is terrible\n",
    "save_model_after_finish: bool = True # maybe doesn't work, transformers is terrible\n",
    "\n",
    "# Etc\n",
    "open_tensorboard: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't touch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# General\n",
    "date = str(datetime.today().date())\n",
    "\n",
    "# Data\n",
    "labels = [\"EGY\", \"GLF\", \"IRQ\", \"LEV\", \"NOR\"]\n",
    "\n",
    "# Preprocessing\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model \n",
    "if from_pretrained_classifier:\n",
    "    pretrained_classifier_path = join(code_folder_path, \"models\", \"finalized_models\", pretrained_classifier_name)\n",
    "    pretrained_classifier = AutoModelForSequenceClassification.from_pretrained(pretrained_classifier_path)\n",
    "\n",
    "# Paths\n",
    "train_path= f\"./models/{date}-train\"\n",
    "search_path = f\"./models/{date}-search\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < data_proportion <= 1, \"data_proportion must be right side inclusively between 0 and 1\"\n",
    "assert 0 <= warmup_ratio <= 1, \"warmup_ratio must be inclusively between 0 and 1\"\n",
    "assert 0 < test_validation_proportion < 1, \"test_validation_proportion must be exclusively between 0 and 1\"\n",
    "assert 0 < sequence_length, \"sequence_length must be positive\"\n",
    "assert 0 < epochs, \"epochs must be positive\"\n",
    "assert 0 < batch_size, \"batch_size must be positive\"\n",
    "assert 0 < validation_size, \"validation_size must be positive\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe ~(2.9 s)\n",
    "df = get_SMADC_folder_data(code_folder_path)\n",
    "df = df.sample(frac=data_proportion)\n",
    "\n",
    "# Encode Y ~(307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    # Preprocess X ~(16min 22s)\n",
    "    df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "    # split and ~(323ms)\n",
    "    train, test = train_test_split(df, test_size=test_validation_proportion, random_state=seed)\n",
    "    validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=seed)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Tokenize\n",
    "    if tokenize_in_batches:\n",
    "        validate_encoding = batch_tokenize(tokenizer, validate[\"Text\"], 10, sequence_length)\n",
    "        test_encoding = batch_tokenize(tokenizer, test[\"Text\"], 100, sequence_length)\n",
    "        train_encoding = batch_tokenize(tokenizer, train[\"Text\"], 500, sequence_length)\n",
    "    else:\n",
    "        validate_encoding = tokenize(tokenizer, validate[\"Text\"].to_list(), sequence_length)\n",
    "        test_encoding = tokenize(tokenizer, test[\"Text\"].to_list(), sequence_length)\n",
    "        train_encoding = tokenize(tokenizer, list(train[\"Text\"]), sequence_length)\n",
    "\n",
    "    # Make Dataset \n",
    "    validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "    test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "    train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "    save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "    save_preprocessed_data(train_dataset, \"preprocessed_train\")\n",
    "\n",
    "if load_data:\n",
    "    # ~(3mins)\n",
    "    validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "    test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "    train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = generate_training_args(\n",
    "                    train_path, epochs=epochs, do_warmup=do_warmup, warmup_ratio=warmup_ratio, \n",
    "                    save_model=save_model_while_training, eval_while_training=eval_while_training, \n",
    "                    learning_rate=learning_rate,batch_size=batch_size, \n",
    "                    train_dataset_length=len(train_dataset), seed=seed\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_pretrained_classifier:\n",
    "    trainer = Trainer(\n",
    "    pretrained_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validate_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow listening on http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "if open_tensorboard:\n",
    "    from tensorboard import program\n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', join(code_folder_path, f\"models\")])\n",
    "    print(f\"Tensorflow listening on {tb.launch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10000/129402 [1:15:37<14:13:36,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.265, 'learning_rate': 9.227832645554166e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10003/129402 [1:15:39<15:07:57,  2.19it/s]ERROR:tensorboard:File models\\2022-03-23-train\\runs\\Mar23_18-49-33_At-Low-desktop\\events.out.tfevents.1648050632.At-Low-desktop.25436.0 updated even though the current file is models\\2022-03-23-train\\runs\\Mar23_18-49-33_At-Low-desktop\\events.out.tfevents.1648050633.At-Low-desktop.25436.2\n",
      " 15%|█▌        | 20000/129402 [2:27:08<13:03:07,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.232, 'learning_rate': 8.455433455433457e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 30000/129402 [3:38:40<11:53:12,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1971, 'learning_rate': 7.682879708196165e-06, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 40000/129402 [4:50:12<10:38:12,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1924, 'learning_rate': 6.9103259609588725e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 50000/129402 [6:05:44<9:43:23,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1709, 'learning_rate': 6.13769493516329e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 60000/129402 [7:19:24<8:31:24,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1639, 'learning_rate': 5.36521846648429e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 70000/129402 [8:33:02<7:17:51,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1501, 'learning_rate': 4.592664719246998e-06, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 80000/129402 [9:46:41<6:05:17,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1403, 'learning_rate': 3.820188250567998e-06, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 90000/129402 [11:03:42<4:53:46,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1331, 'learning_rate': 3.047634503330706e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 100000/129402 [12:17:36<4:24:59,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1232, 'learning_rate': 2.2751580346517057e-06, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 110000/129402 [13:29:03<2:16:24,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1217, 'learning_rate': 1.5027588445309965e-06, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 120000/129402 [14:41:55<1:13:30,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1115, 'learning_rate': 7.302050972937049e-07, 'epoch': 5.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129402/129402 [15:50:20<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 57021.1176, 'train_samples_per_second': 145.239, 'train_steps_per_second': 2.269, 'train_loss': 0.16270215116187106, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129402, training_loss=0.16270215116187106, metrics={'train_runtime': 57021.1176, 'train_samples_per_second': 145.239, 'train_steps_per_second': 2.269, 'train_loss': 0.16270215116187106, 'epoch': 6.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    " \n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:46<00:00,  8.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19884486496448517,\n",
       " 'eval_macro_f1': 0.9284600252295803,\n",
       " 'eval_macro_precision': 0.9328351681831611,\n",
       " 'eval_macro_recall': 0.9243442769402341,\n",
       " 'eval_accuracy': 0.9446705989864584,\n",
       " 'eval_report': {'NOR': {'precision': 0.940982058545798,\n",
       "   'recall': 0.9081795397584871,\n",
       "   'f1-score': 0.9242898550724638,\n",
       "   'support': 4389},\n",
       "  'IRQ': {'precision': 0.9096385542168675,\n",
       "   'recall': 0.9086122602482136,\n",
       "   'f1-score': 0.9091251175917215,\n",
       "   'support': 2659},\n",
       "  'LEV': {'precision': 0.9055965559655597,\n",
       "   'recall': 0.8806818181818182,\n",
       "   'f1-score': 0.8929654335961189,\n",
       "   'support': 3344},\n",
       "  'EGY': {'precision': 0.9674431401320617,\n",
       "   'recall': 0.9926602051378564,\n",
       "   'f1-score': 0.9798894617063767,\n",
       "   'support': 10627},\n",
       "  'GLF': {'precision': 0.9405155320555189,\n",
       "   'recall': 0.9315875613747954,\n",
       "   'f1-score': 0.9360302581812201,\n",
       "   'support': 3055},\n",
       "  'accuracy': 0.9446705989864584,\n",
       "  'macro avg': {'precision': 0.9328351681831611,\n",
       "   'recall': 0.9243442769402341,\n",
       "   'f1-score': 0.9284600252295803,\n",
       "   'support': 24074},\n",
       "  'weighted avg': {'precision': 0.9442264374172145,\n",
       "   'recall': 0.9446705989864584,\n",
       "   'f1-score': 0.9442970017376483,\n",
       "   'support': 24074}},\n",
       " 'eval_runtime': 47.1344,\n",
       " 'eval_samples_per_second': 510.752,\n",
       " 'eval_steps_per_second': 7.998,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = trainer.evaluate(test_dataset)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:07<00:00,  8.36it/s]\n"
     ]
    }
   ],
   "source": [
    "if save_model_after_finish:\n",
    "    save_path = f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{model_name[model_name.rfind(\"/\")+1:]}-{trainer.evaluate()[\"eval_accuracy\"]}'\n",
    "    trainer.save_model(save_path)\n",
    "    with open(join(save_path, \"evaluation.json\"), \"w\") as file:\n",
    "        file.write(json.dumps(evaluation))\n",
    "        \n",
    "    with open(join(save_path, \"arguments.txt\"), \"w\") as file:\n",
    "        file.write(str(training_args))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial: optuna.Trial):     \n",
    "    \n",
    "#     training_args = generate_training_args(search_path, epochs=None, do_warmup=False, save_model=False, eval_while_training=False)\n",
    "#     training_args.learning_rate= trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01)\n",
    "#     training_args.weight_decay= trial.suggest_loguniform('weight_decay', 4e-5, 0.01)\n",
    "#     training_args.num_train_epochs= trial.suggest_int('num_train_epochs', low=2, high=5)\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validate_dataset,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         callbacks=[TensorBoardCallback()]\n",
    "#     )\n",
    "    \n",
    "#     result = trainer.train()     \n",
    "    \n",
    "#     return result.training_loss # Or result.training_loss[\"metric_name\"] ps: change direction in study if necessary\n",
    "   \n",
    "# We want to minimize the loss! \n",
    "# study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize') \n",
    "# study.optimize(func=objective, n_trials=15)\n",
    "\n",
    "# print(study.best_value) \n",
    "# print(study.best_params) \n",
    "# print(study.best_trial)\n",
    "\n",
    "# trainer.hyperparameter_search(n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a11609d3c9a3d6d9d27250456fa90a271920de06fcd2ad5e9bde9ece7a63280"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('graduation_project')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
