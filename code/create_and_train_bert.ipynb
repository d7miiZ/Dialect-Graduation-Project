{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer, EarlyStoppingCallback, BatchEncoding\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "import optuna \n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2021-09-30 10:02:21,082 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Data params\n",
    "validation_size = 4096\n",
    "\n",
    "# Model params\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Preprocessing params\n",
    "sequence_length = 128\n",
    "arabert_prep = ArabertPreprocessor(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Etc\n",
    "model_string = f\"./models/{str(datetime.today().date())}-train\"\n",
    "dataset_string = \"{}_dataset-seqlen\" + str(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SMADC_folder_data():\n",
    "    \"\"\"Returns a dataframe with Text and Region columns. Requires tree like this data/SMADC/*.txt\"\"\"\n",
    "    files = glob(\"data/SMADC/*.txt\")\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        region = file[-7:-4]\n",
    "        temp_df = pd.read_csv(file, encoding=\"utf8\", delimiter=\"\\n\", names=[\"Text\"])\n",
    "        temp_df[\"Region\"] = region\n",
    "        dataframes.append(temp_df)\n",
    "        \n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "def get_music_df():\n",
    "    files = [\"GLF\",\"LEV\",\"NOR\",\"IRQ\"]\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in files:\n",
    "        temp_df = pd.read_csv(f'../extra_data/d7_data/{file}.txt', encoding=\"utf8\", delimiter=\"\\n\", names=[\"Text\"])\n",
    "        temp_df[\"Region\"] = file\n",
    "        dataframes.append(temp_df)\n",
    "    \n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of strings\n",
    "    \"\"\"\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        batch,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=sequence_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def batch_tokenize_iter(data, batch_size):\n",
    "    len_data = len(data)\n",
    "    batch_num = len_data // batch_size\n",
    "    batch_rest = len_data / batch_size - batch_num\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        yield tokenize(data[i * batch_num:(i+1) * batch_num].to_list())\n",
    "        \n",
    "    if batch_rest:\n",
    "        yield tokenize(data[batch_num:].to_list())\n",
    "\n",
    "\n",
    "def batch_tokenize(data, batch_size):\n",
    "    bt = batch_tokenize_iter(data, batch_size)\n",
    "    for i, tokenization in enumerate(bt):\n",
    "        if not i:\n",
    "            encoding = tokenization\n",
    "            continue\n",
    "        encoding[\"input_ids\"] = torch.cat([encoding[\"input_ids\"], tokenization[\"input_ids\"]])\n",
    "        encoding[\"attention_mask\"] = torch.cat([encoding[\"attention_mask\"], tokenization[\"attention_mask\"]])\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    \"\"\"Sample list of strings\"\"\"\n",
    "    return tokenize(list(arabert_prep.preprocess(text) for text in sample))\n",
    "\n",
    "\n",
    "def save_preprocessed_data(dataset, dataset_name):\n",
    "    with open(f\"preprocessed_data/{dataset_name}.pkl\", \"wb\") as file:\n",
    "        dump(dataset, file)\n",
    "        \n",
    "def load_preprocessed_data(dataset_name):\n",
    "    with open(f\"preprocessed_data/{dataset_name}.pkl\", \"rb\") as file:\n",
    "        temp = load(file)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def compute_metrics(p): \n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    assert len(preds) == len(p.label_ids)\n",
    "\n",
    "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "    macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "    macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "    acc = accuracy_score(p.label_ids,preds)\n",
    "    return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "    }\n",
    "\n",
    "\n",
    "def model_init(model_name, num_labels, label2id, id2label):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dataset class\n",
    "class Dialect_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super(Dialect_dataset).__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return InputFeatures(self.X[\"input_ids\"][key], self.X[\"attention_mask\"][key], label=self.Y[key])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (loading, preprocessing, tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Date to dataframe (2.9 s)\n",
    "df = get_SMADC_folder_data()\n",
    "\n",
    "# Encode Y (307 ms)\n",
    "classes = df[\"Region\"].unique()\n",
    "num_labels = len(classes)\n",
    "class_to_index = {class_:index for class_, index in zip(classes, range(len(classes)))}\n",
    "index_to_class = {index:class_ for class_, index in zip(classes, range(len(classes)))}\n",
    "df[\"Labels\"] = df[\"Region\"].apply(class_to_index.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing (If you want to load data, skip until loading section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Preprocess X (16min 22s)\n",
    "df[\"Text\"] = df[\"Text\"].apply(arabert_prep.preprocess)\n",
    "\n",
    "# split and (323ms)\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=1)\n",
    "validate, test = train_test_split(test, test_size=len(test)-validation_size, random_state=1)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "validate.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize everything at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validate_encoding = tokenize(validate[\"Text\"].to_list())\n",
    "test_encoding = tokenize(test[\"Text\"].to_list())\n",
    "train_encoding = tokenize(list(train[\"Text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# validate_encoding = batch_tokenize(validate[\"Text\"], 10)\n",
    "# test_encoding = batch_tokenize(test[\"Text\"], 100)\n",
    "# train_encoding = batch_tokenize(train[\"Text\"], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 746 ms\n"
     ]
    }
   ],
   "source": [
    "%%time    \n",
    "# Make Dataset \n",
    "validate_dataset = Dialect_dataset(validate_encoding, validate[\"Labels\"].to_list())\n",
    "test_dataset = Dialect_dataset(test_encoding, test[\"Labels\"].to_list())\n",
    "train_dataset = Dialect_dataset(train_encoding, train[\"Labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save_preprocessed_data(validate_dataset, \"preprocessed_validation\")\n",
    "# save_preprocessed_data(test_dataset, \"preprocessed_test\")\n",
    "# save_preprocessed_data(train_dataset, \"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# validate_dataset = load_preprocessed_data(\"preprocessed_validation\")\n",
    "# test_dataset = load_preprocessed_data(\"preprocessed_test\")\n",
    "# train_dataset = load_preprocessed_data(\"preprocessed_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 39612. Total steps: 198060\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(model_string)\n",
    "\n",
    "training_args.adam_epsilon = 1e-8\n",
    "training_args.learning_rate = 5e-5\n",
    "\n",
    "training_args.fp16 = True\n",
    "\n",
    "training_args.per_device_train_batch_size = 32\n",
    "training_args.per_device_eval_batch_size = 32\n",
    "\n",
    "training_args.gradient_accumulation_steps = 1\n",
    "\n",
    "training_args.num_train_epochs = 5\n",
    "\n",
    "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(f\"Steps per epoch: {steps_per_epoch}. Total steps: {total_steps}\")\n",
    "\n",
    "warmup_ratio = 0.05\n",
    "training_args.warmup_steps = total_steps * warmup_ratio \n",
    "\n",
    "training_args.logging_steps = 1024\n",
    "training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "# training_args.save_strategy = EvaluationStrategy.STEPS\n",
    "training_args.eval_strategy = EvaluationStrategy.STEPS\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.eval_steps = 1024 # defaults to logging_steps\n",
    " \n",
    "# training_args.greater_is_better = False # Loss lower is better\n",
    "\n",
    "training_args.save_steps = 1024\n",
    "training_args.save_total_limit = 10\n",
    "\n",
    "training_args.seed = 1\n",
    "\n",
    "training_args.lr_scheduler_type = 'cosine'\n",
    "\n",
    "training_args.metric_for_best_model= \"eval_loss\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "EarlyStoppingCallback\n",
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=lambda:model_init(model_name, len(classes), label2id=class_to_index, id2label=index_to_class),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/config.json from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\fcf7a61cc072840ad32e1a2e8eb230a79b33db68e3f965f8014a52915cab999f.2f0d0092105af7b8b42b899ffb7f801dc48e93516d509483f6cfbd86155d49ea\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin from cache at C:\\Users\\mohnd/.cache\\huggingface\\transformers\\da598d10a62ed68f0b95e0c032d813a008518ba8fe1d02fb191884f844c818ce.97462e17e0f13709a0a977021298c2733cda0cb6787facbeeb0b53199a7e73bf\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1267610\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197633' max='198065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [197633/198065 16:35:35 < 02:10, 3.31 it/s, Epoch 4.99/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>1.111600</td>\n",
       "      <td>0.740110</td>\n",
       "      <td>0.691882</td>\n",
       "      <td>0.707830</td>\n",
       "      <td>0.680998</td>\n",
       "      <td>0.748535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.727800</td>\n",
       "      <td>0.622861</td>\n",
       "      <td>0.746602</td>\n",
       "      <td>0.757428</td>\n",
       "      <td>0.738106</td>\n",
       "      <td>0.787354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3072</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.573454</td>\n",
       "      <td>0.759787</td>\n",
       "      <td>0.765894</td>\n",
       "      <td>0.754840</td>\n",
       "      <td>0.798096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4096</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.557097</td>\n",
       "      <td>0.766516</td>\n",
       "      <td>0.787411</td>\n",
       "      <td>0.751631</td>\n",
       "      <td>0.803467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.558398</td>\n",
       "      <td>0.769603</td>\n",
       "      <td>0.788661</td>\n",
       "      <td>0.760189</td>\n",
       "      <td>0.804199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6144</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>0.511577</td>\n",
       "      <td>0.791133</td>\n",
       "      <td>0.799179</td>\n",
       "      <td>0.784054</td>\n",
       "      <td>0.823975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7168</td>\n",
       "      <td>0.568200</td>\n",
       "      <td>0.518530</td>\n",
       "      <td>0.793587</td>\n",
       "      <td>0.806826</td>\n",
       "      <td>0.790178</td>\n",
       "      <td>0.827881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8192</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.537362</td>\n",
       "      <td>0.782648</td>\n",
       "      <td>0.786702</td>\n",
       "      <td>0.781989</td>\n",
       "      <td>0.817871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9216</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.503549</td>\n",
       "      <td>0.795891</td>\n",
       "      <td>0.806537</td>\n",
       "      <td>0.790116</td>\n",
       "      <td>0.830078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10240</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>0.512387</td>\n",
       "      <td>0.797208</td>\n",
       "      <td>0.798469</td>\n",
       "      <td>0.796485</td>\n",
       "      <td>0.826416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11264</td>\n",
       "      <td>0.535400</td>\n",
       "      <td>0.500640</td>\n",
       "      <td>0.796578</td>\n",
       "      <td>0.818806</td>\n",
       "      <td>0.780618</td>\n",
       "      <td>0.826660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12288</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.505055</td>\n",
       "      <td>0.799454</td>\n",
       "      <td>0.812905</td>\n",
       "      <td>0.791827</td>\n",
       "      <td>0.827881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13312</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.493740</td>\n",
       "      <td>0.802146</td>\n",
       "      <td>0.827595</td>\n",
       "      <td>0.785113</td>\n",
       "      <td>0.833740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14336</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.475262</td>\n",
       "      <td>0.803504</td>\n",
       "      <td>0.803323</td>\n",
       "      <td>0.803980</td>\n",
       "      <td>0.832275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15360</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.463648</td>\n",
       "      <td>0.809390</td>\n",
       "      <td>0.817781</td>\n",
       "      <td>0.801951</td>\n",
       "      <td>0.837891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16384</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.813701</td>\n",
       "      <td>0.827661</td>\n",
       "      <td>0.801850</td>\n",
       "      <td>0.842529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17408</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.474356</td>\n",
       "      <td>0.807345</td>\n",
       "      <td>0.804767</td>\n",
       "      <td>0.812898</td>\n",
       "      <td>0.838867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18432</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.458248</td>\n",
       "      <td>0.813916</td>\n",
       "      <td>0.825276</td>\n",
       "      <td>0.804880</td>\n",
       "      <td>0.843506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19456</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.461546</td>\n",
       "      <td>0.805553</td>\n",
       "      <td>0.815418</td>\n",
       "      <td>0.800788</td>\n",
       "      <td>0.835449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20480</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.471867</td>\n",
       "      <td>0.808308</td>\n",
       "      <td>0.825036</td>\n",
       "      <td>0.794338</td>\n",
       "      <td>0.836426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21504</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.451559</td>\n",
       "      <td>0.820604</td>\n",
       "      <td>0.825776</td>\n",
       "      <td>0.816889</td>\n",
       "      <td>0.846436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22528</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.459534</td>\n",
       "      <td>0.809493</td>\n",
       "      <td>0.822869</td>\n",
       "      <td>0.800698</td>\n",
       "      <td>0.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23552</td>\n",
       "      <td>0.476600</td>\n",
       "      <td>0.448737</td>\n",
       "      <td>0.817769</td>\n",
       "      <td>0.834109</td>\n",
       "      <td>0.804071</td>\n",
       "      <td>0.846191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24576</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.460094</td>\n",
       "      <td>0.821055</td>\n",
       "      <td>0.846714</td>\n",
       "      <td>0.800430</td>\n",
       "      <td>0.847656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.437511</td>\n",
       "      <td>0.817236</td>\n",
       "      <td>0.837470</td>\n",
       "      <td>0.803407</td>\n",
       "      <td>0.848633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26624</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.456098</td>\n",
       "      <td>0.814572</td>\n",
       "      <td>0.827034</td>\n",
       "      <td>0.807530</td>\n",
       "      <td>0.842041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27648</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.445886</td>\n",
       "      <td>0.823019</td>\n",
       "      <td>0.844831</td>\n",
       "      <td>0.804905</td>\n",
       "      <td>0.849365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28672</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.439804</td>\n",
       "      <td>0.823387</td>\n",
       "      <td>0.839856</td>\n",
       "      <td>0.814094</td>\n",
       "      <td>0.852295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29696</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.431952</td>\n",
       "      <td>0.829078</td>\n",
       "      <td>0.843504</td>\n",
       "      <td>0.817961</td>\n",
       "      <td>0.855957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30720</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.432111</td>\n",
       "      <td>0.827400</td>\n",
       "      <td>0.840554</td>\n",
       "      <td>0.818059</td>\n",
       "      <td>0.854736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31744</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>0.420246</td>\n",
       "      <td>0.837644</td>\n",
       "      <td>0.848041</td>\n",
       "      <td>0.829151</td>\n",
       "      <td>0.862061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32768</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.427110</td>\n",
       "      <td>0.832069</td>\n",
       "      <td>0.841624</td>\n",
       "      <td>0.823896</td>\n",
       "      <td>0.856934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33792</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.425007</td>\n",
       "      <td>0.832043</td>\n",
       "      <td>0.847869</td>\n",
       "      <td>0.819092</td>\n",
       "      <td>0.856201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34816</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.417593</td>\n",
       "      <td>0.830303</td>\n",
       "      <td>0.842135</td>\n",
       "      <td>0.820030</td>\n",
       "      <td>0.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35840</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.419679</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>0.838129</td>\n",
       "      <td>0.832013</td>\n",
       "      <td>0.858154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36864</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.418850</td>\n",
       "      <td>0.832943</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.856934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37888</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.409880</td>\n",
       "      <td>0.836542</td>\n",
       "      <td>0.854957</td>\n",
       "      <td>0.820933</td>\n",
       "      <td>0.859863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38912</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.397202</td>\n",
       "      <td>0.837843</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.832826</td>\n",
       "      <td>0.862549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39936</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.405830</td>\n",
       "      <td>0.843807</td>\n",
       "      <td>0.848915</td>\n",
       "      <td>0.839914</td>\n",
       "      <td>0.866211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40960</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>0.424149</td>\n",
       "      <td>0.841267</td>\n",
       "      <td>0.853525</td>\n",
       "      <td>0.833357</td>\n",
       "      <td>0.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41984</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.831380</td>\n",
       "      <td>0.836838</td>\n",
       "      <td>0.826377</td>\n",
       "      <td>0.857422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43008</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>0.832328</td>\n",
       "      <td>0.835581</td>\n",
       "      <td>0.830665</td>\n",
       "      <td>0.858154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44032</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.427725</td>\n",
       "      <td>0.832371</td>\n",
       "      <td>0.848536</td>\n",
       "      <td>0.819652</td>\n",
       "      <td>0.857910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45056</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.406161</td>\n",
       "      <td>0.836440</td>\n",
       "      <td>0.839559</td>\n",
       "      <td>0.835203</td>\n",
       "      <td>0.862305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46080</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.403445</td>\n",
       "      <td>0.834590</td>\n",
       "      <td>0.838217</td>\n",
       "      <td>0.831346</td>\n",
       "      <td>0.859863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47104</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.409632</td>\n",
       "      <td>0.842463</td>\n",
       "      <td>0.844872</td>\n",
       "      <td>0.840280</td>\n",
       "      <td>0.865967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48128</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.397133</td>\n",
       "      <td>0.842147</td>\n",
       "      <td>0.844750</td>\n",
       "      <td>0.840200</td>\n",
       "      <td>0.865479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49152</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.409083</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.830213</td>\n",
       "      <td>0.863037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50176</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.405185</td>\n",
       "      <td>0.842282</td>\n",
       "      <td>0.856259</td>\n",
       "      <td>0.830262</td>\n",
       "      <td>0.866211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.411596</td>\n",
       "      <td>0.847390</td>\n",
       "      <td>0.867493</td>\n",
       "      <td>0.831781</td>\n",
       "      <td>0.871094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52224</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.397430</td>\n",
       "      <td>0.843277</td>\n",
       "      <td>0.850564</td>\n",
       "      <td>0.836878</td>\n",
       "      <td>0.868164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53248</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.394505</td>\n",
       "      <td>0.843139</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>0.830431</td>\n",
       "      <td>0.868164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54272</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.396239</td>\n",
       "      <td>0.848517</td>\n",
       "      <td>0.853676</td>\n",
       "      <td>0.844036</td>\n",
       "      <td>0.870117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55296</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.392887</td>\n",
       "      <td>0.843082</td>\n",
       "      <td>0.848163</td>\n",
       "      <td>0.839470</td>\n",
       "      <td>0.868164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56320</td>\n",
       "      <td>0.378100</td>\n",
       "      <td>0.399503</td>\n",
       "      <td>0.839072</td>\n",
       "      <td>0.845974</td>\n",
       "      <td>0.832845</td>\n",
       "      <td>0.863037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57344</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.404994</td>\n",
       "      <td>0.842304</td>\n",
       "      <td>0.849839</td>\n",
       "      <td>0.835777</td>\n",
       "      <td>0.866211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58368</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.402448</td>\n",
       "      <td>0.842024</td>\n",
       "      <td>0.845943</td>\n",
       "      <td>0.838788</td>\n",
       "      <td>0.866699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59392</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>0.397214</td>\n",
       "      <td>0.845069</td>\n",
       "      <td>0.857296</td>\n",
       "      <td>0.834869</td>\n",
       "      <td>0.870361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60416</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.403704</td>\n",
       "      <td>0.839640</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.827708</td>\n",
       "      <td>0.864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61440</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.404664</td>\n",
       "      <td>0.843478</td>\n",
       "      <td>0.851263</td>\n",
       "      <td>0.837045</td>\n",
       "      <td>0.867676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62464</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.400298</td>\n",
       "      <td>0.846015</td>\n",
       "      <td>0.852935</td>\n",
       "      <td>0.840995</td>\n",
       "      <td>0.871582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63488</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.382426</td>\n",
       "      <td>0.850678</td>\n",
       "      <td>0.861998</td>\n",
       "      <td>0.840465</td>\n",
       "      <td>0.874268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64512</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.380377</td>\n",
       "      <td>0.847637</td>\n",
       "      <td>0.855320</td>\n",
       "      <td>0.841607</td>\n",
       "      <td>0.872803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65536</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.372614</td>\n",
       "      <td>0.854626</td>\n",
       "      <td>0.865689</td>\n",
       "      <td>0.845135</td>\n",
       "      <td>0.877686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66560</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.382113</td>\n",
       "      <td>0.852204</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.841765</td>\n",
       "      <td>0.876465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67584</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.392447</td>\n",
       "      <td>0.849399</td>\n",
       "      <td>0.862214</td>\n",
       "      <td>0.838406</td>\n",
       "      <td>0.874023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68608</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.381121</td>\n",
       "      <td>0.850869</td>\n",
       "      <td>0.858524</td>\n",
       "      <td>0.843914</td>\n",
       "      <td>0.875977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69632</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.386045</td>\n",
       "      <td>0.852089</td>\n",
       "      <td>0.865902</td>\n",
       "      <td>0.840884</td>\n",
       "      <td>0.876465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70656</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.386652</td>\n",
       "      <td>0.844734</td>\n",
       "      <td>0.849525</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.871094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71680</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.385917</td>\n",
       "      <td>0.845677</td>\n",
       "      <td>0.855597</td>\n",
       "      <td>0.837437</td>\n",
       "      <td>0.870850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72704</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.384616</td>\n",
       "      <td>0.852359</td>\n",
       "      <td>0.868933</td>\n",
       "      <td>0.838669</td>\n",
       "      <td>0.876953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73728</td>\n",
       "      <td>0.357100</td>\n",
       "      <td>0.377423</td>\n",
       "      <td>0.846502</td>\n",
       "      <td>0.854430</td>\n",
       "      <td>0.839772</td>\n",
       "      <td>0.873291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74752</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.372645</td>\n",
       "      <td>0.851573</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.839689</td>\n",
       "      <td>0.875244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75776</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.370533</td>\n",
       "      <td>0.851978</td>\n",
       "      <td>0.866718</td>\n",
       "      <td>0.840702</td>\n",
       "      <td>0.876221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76800</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.380969</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.868286</td>\n",
       "      <td>0.838371</td>\n",
       "      <td>0.876465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77824</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.376976</td>\n",
       "      <td>0.855162</td>\n",
       "      <td>0.872531</td>\n",
       "      <td>0.840608</td>\n",
       "      <td>0.878418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78848</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.363060</td>\n",
       "      <td>0.863513</td>\n",
       "      <td>0.873117</td>\n",
       "      <td>0.855068</td>\n",
       "      <td>0.885742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79872</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.388818</td>\n",
       "      <td>0.854940</td>\n",
       "      <td>0.860495</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.878174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80896</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.383738</td>\n",
       "      <td>0.853077</td>\n",
       "      <td>0.858863</td>\n",
       "      <td>0.847806</td>\n",
       "      <td>0.878174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81920</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.386699</td>\n",
       "      <td>0.857570</td>\n",
       "      <td>0.867245</td>\n",
       "      <td>0.849761</td>\n",
       "      <td>0.880859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82944</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>0.851332</td>\n",
       "      <td>0.856529</td>\n",
       "      <td>0.846599</td>\n",
       "      <td>0.875977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83968</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.380941</td>\n",
       "      <td>0.850063</td>\n",
       "      <td>0.859739</td>\n",
       "      <td>0.841666</td>\n",
       "      <td>0.875488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84992</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.401772</td>\n",
       "      <td>0.848471</td>\n",
       "      <td>0.855007</td>\n",
       "      <td>0.842966</td>\n",
       "      <td>0.875488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86016</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.406804</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>0.869823</td>\n",
       "      <td>0.842238</td>\n",
       "      <td>0.878906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87040</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.387044</td>\n",
       "      <td>0.858940</td>\n",
       "      <td>0.864371</td>\n",
       "      <td>0.855016</td>\n",
       "      <td>0.882080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88064</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.400960</td>\n",
       "      <td>0.855659</td>\n",
       "      <td>0.862813</td>\n",
       "      <td>0.849675</td>\n",
       "      <td>0.880615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89088</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.378088</td>\n",
       "      <td>0.861768</td>\n",
       "      <td>0.872503</td>\n",
       "      <td>0.852122</td>\n",
       "      <td>0.885254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90112</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.390086</td>\n",
       "      <td>0.855136</td>\n",
       "      <td>0.860309</td>\n",
       "      <td>0.850627</td>\n",
       "      <td>0.879395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91136</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.385499</td>\n",
       "      <td>0.850290</td>\n",
       "      <td>0.858411</td>\n",
       "      <td>0.843993</td>\n",
       "      <td>0.876709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92160</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.383221</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.864832</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.878906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93184</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.384328</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>0.867879</td>\n",
       "      <td>0.853004</td>\n",
       "      <td>0.883057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94208</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.389537</td>\n",
       "      <td>0.860374</td>\n",
       "      <td>0.870840</td>\n",
       "      <td>0.851249</td>\n",
       "      <td>0.884521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95232</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.384762</td>\n",
       "      <td>0.858582</td>\n",
       "      <td>0.868660</td>\n",
       "      <td>0.850347</td>\n",
       "      <td>0.883545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96256</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.380120</td>\n",
       "      <td>0.858336</td>\n",
       "      <td>0.869682</td>\n",
       "      <td>0.848370</td>\n",
       "      <td>0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97280</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>0.368021</td>\n",
       "      <td>0.861816</td>\n",
       "      <td>0.872019</td>\n",
       "      <td>0.852756</td>\n",
       "      <td>0.886963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98304</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.366686</td>\n",
       "      <td>0.862297</td>\n",
       "      <td>0.874910</td>\n",
       "      <td>0.853338</td>\n",
       "      <td>0.886475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99328</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.366997</td>\n",
       "      <td>0.862159</td>\n",
       "      <td>0.872054</td>\n",
       "      <td>0.853161</td>\n",
       "      <td>0.885742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100352</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.377693</td>\n",
       "      <td>0.861626</td>\n",
       "      <td>0.869244</td>\n",
       "      <td>0.855013</td>\n",
       "      <td>0.885742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101376</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.355127</td>\n",
       "      <td>0.860351</td>\n",
       "      <td>0.867868</td>\n",
       "      <td>0.853758</td>\n",
       "      <td>0.884766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102400</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.357643</td>\n",
       "      <td>0.859076</td>\n",
       "      <td>0.866828</td>\n",
       "      <td>0.852852</td>\n",
       "      <td>0.884766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103424</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.367125</td>\n",
       "      <td>0.866134</td>\n",
       "      <td>0.878920</td>\n",
       "      <td>0.854748</td>\n",
       "      <td>0.888184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104448</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.357329</td>\n",
       "      <td>0.866040</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.857536</td>\n",
       "      <td>0.889648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105472</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.352824</td>\n",
       "      <td>0.865645</td>\n",
       "      <td>0.872679</td>\n",
       "      <td>0.859286</td>\n",
       "      <td>0.889404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106496</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.365405</td>\n",
       "      <td>0.864412</td>\n",
       "      <td>0.878600</td>\n",
       "      <td>0.852862</td>\n",
       "      <td>0.888184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107520</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.356116</td>\n",
       "      <td>0.866791</td>\n",
       "      <td>0.877198</td>\n",
       "      <td>0.857668</td>\n",
       "      <td>0.890381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108544</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.356397</td>\n",
       "      <td>0.863896</td>\n",
       "      <td>0.873419</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>0.888672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109568</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.352346</td>\n",
       "      <td>0.870762</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.865280</td>\n",
       "      <td>0.894043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110592</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.352693</td>\n",
       "      <td>0.862721</td>\n",
       "      <td>0.869083</td>\n",
       "      <td>0.857459</td>\n",
       "      <td>0.888916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111616</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.354478</td>\n",
       "      <td>0.862755</td>\n",
       "      <td>0.872146</td>\n",
       "      <td>0.854971</td>\n",
       "      <td>0.888428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112640</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.351685</td>\n",
       "      <td>0.865455</td>\n",
       "      <td>0.873382</td>\n",
       "      <td>0.858239</td>\n",
       "      <td>0.888428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113664</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.352343</td>\n",
       "      <td>0.865345</td>\n",
       "      <td>0.874724</td>\n",
       "      <td>0.857041</td>\n",
       "      <td>0.891113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114688</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.350704</td>\n",
       "      <td>0.866996</td>\n",
       "      <td>0.876088</td>\n",
       "      <td>0.858857</td>\n",
       "      <td>0.891602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115712</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.354198</td>\n",
       "      <td>0.864691</td>\n",
       "      <td>0.874744</td>\n",
       "      <td>0.855899</td>\n",
       "      <td>0.889404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116736</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.352558</td>\n",
       "      <td>0.861793</td>\n",
       "      <td>0.870117</td>\n",
       "      <td>0.854499</td>\n",
       "      <td>0.886719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117760</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.345580</td>\n",
       "      <td>0.866453</td>\n",
       "      <td>0.875119</td>\n",
       "      <td>0.859018</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118784</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.336125</td>\n",
       "      <td>0.870039</td>\n",
       "      <td>0.878692</td>\n",
       "      <td>0.862218</td>\n",
       "      <td>0.893555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119808</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.390468</td>\n",
       "      <td>0.868212</td>\n",
       "      <td>0.875189</td>\n",
       "      <td>0.861694</td>\n",
       "      <td>0.891846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120832</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.378877</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.877712</td>\n",
       "      <td>0.855431</td>\n",
       "      <td>0.889893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121856</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.384837</td>\n",
       "      <td>0.867217</td>\n",
       "      <td>0.876652</td>\n",
       "      <td>0.858920</td>\n",
       "      <td>0.891846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122880</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.381233</td>\n",
       "      <td>0.866688</td>\n",
       "      <td>0.874592</td>\n",
       "      <td>0.859503</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123904</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.395922</td>\n",
       "      <td>0.868449</td>\n",
       "      <td>0.875627</td>\n",
       "      <td>0.862104</td>\n",
       "      <td>0.892578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124928</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.390197</td>\n",
       "      <td>0.870791</td>\n",
       "      <td>0.878616</td>\n",
       "      <td>0.863881</td>\n",
       "      <td>0.893799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125952</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.391014</td>\n",
       "      <td>0.868379</td>\n",
       "      <td>0.875026</td>\n",
       "      <td>0.862576</td>\n",
       "      <td>0.892578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126976</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.392865</td>\n",
       "      <td>0.864528</td>\n",
       "      <td>0.874507</td>\n",
       "      <td>0.855540</td>\n",
       "      <td>0.888916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.390412</td>\n",
       "      <td>0.867584</td>\n",
       "      <td>0.878518</td>\n",
       "      <td>0.857780</td>\n",
       "      <td>0.891357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129024</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.387089</td>\n",
       "      <td>0.869108</td>\n",
       "      <td>0.876131</td>\n",
       "      <td>0.862603</td>\n",
       "      <td>0.892822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130048</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.378376</td>\n",
       "      <td>0.870239</td>\n",
       "      <td>0.878860</td>\n",
       "      <td>0.862418</td>\n",
       "      <td>0.894043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131072</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.374090</td>\n",
       "      <td>0.873992</td>\n",
       "      <td>0.881431</td>\n",
       "      <td>0.867331</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132096</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.368769</td>\n",
       "      <td>0.865269</td>\n",
       "      <td>0.871813</td>\n",
       "      <td>0.859342</td>\n",
       "      <td>0.891113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133120</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.381448</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.876249</td>\n",
       "      <td>0.860700</td>\n",
       "      <td>0.893311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134144</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.388152</td>\n",
       "      <td>0.868437</td>\n",
       "      <td>0.879382</td>\n",
       "      <td>0.858705</td>\n",
       "      <td>0.892822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135168</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.375609</td>\n",
       "      <td>0.871802</td>\n",
       "      <td>0.881692</td>\n",
       "      <td>0.863112</td>\n",
       "      <td>0.896240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136192</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.376113</td>\n",
       "      <td>0.870933</td>\n",
       "      <td>0.879847</td>\n",
       "      <td>0.863317</td>\n",
       "      <td>0.895020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137216</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.373619</td>\n",
       "      <td>0.868977</td>\n",
       "      <td>0.878665</td>\n",
       "      <td>0.860770</td>\n",
       "      <td>0.894287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138240</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.378340</td>\n",
       "      <td>0.867652</td>\n",
       "      <td>0.874662</td>\n",
       "      <td>0.861747</td>\n",
       "      <td>0.893066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139264</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.379208</td>\n",
       "      <td>0.872410</td>\n",
       "      <td>0.880536</td>\n",
       "      <td>0.865029</td>\n",
       "      <td>0.896484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140288</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.377502</td>\n",
       "      <td>0.871641</td>\n",
       "      <td>0.879553</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.896484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141312</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.377456</td>\n",
       "      <td>0.873472</td>\n",
       "      <td>0.882546</td>\n",
       "      <td>0.865511</td>\n",
       "      <td>0.897461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142336</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.368793</td>\n",
       "      <td>0.865764</td>\n",
       "      <td>0.873531</td>\n",
       "      <td>0.858771</td>\n",
       "      <td>0.892334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143360</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.363853</td>\n",
       "      <td>0.872195</td>\n",
       "      <td>0.878914</td>\n",
       "      <td>0.865976</td>\n",
       "      <td>0.896240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144384</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.375134</td>\n",
       "      <td>0.870880</td>\n",
       "      <td>0.878764</td>\n",
       "      <td>0.863630</td>\n",
       "      <td>0.895020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145408</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.368452</td>\n",
       "      <td>0.873220</td>\n",
       "      <td>0.881848</td>\n",
       "      <td>0.865300</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146432</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.375493</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.880907</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>0.895264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147456</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.365477</td>\n",
       "      <td>0.872995</td>\n",
       "      <td>0.881020</td>\n",
       "      <td>0.865535</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148480</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.375856</td>\n",
       "      <td>0.875305</td>\n",
       "      <td>0.886616</td>\n",
       "      <td>0.865166</td>\n",
       "      <td>0.898926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149504</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.374553</td>\n",
       "      <td>0.874399</td>\n",
       "      <td>0.885153</td>\n",
       "      <td>0.864917</td>\n",
       "      <td>0.898193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150528</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.366026</td>\n",
       "      <td>0.875445</td>\n",
       "      <td>0.886017</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>0.899414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151552</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.368311</td>\n",
       "      <td>0.872673</td>\n",
       "      <td>0.883325</td>\n",
       "      <td>0.863407</td>\n",
       "      <td>0.896240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152576</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.372001</td>\n",
       "      <td>0.870042</td>\n",
       "      <td>0.880690</td>\n",
       "      <td>0.860537</td>\n",
       "      <td>0.895020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153600</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.360755</td>\n",
       "      <td>0.877181</td>\n",
       "      <td>0.886651</td>\n",
       "      <td>0.868566</td>\n",
       "      <td>0.900146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154624</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.369243</td>\n",
       "      <td>0.874274</td>\n",
       "      <td>0.885950</td>\n",
       "      <td>0.863887</td>\n",
       "      <td>0.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155648</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.370427</td>\n",
       "      <td>0.873324</td>\n",
       "      <td>0.884201</td>\n",
       "      <td>0.863580</td>\n",
       "      <td>0.897705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156672</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.358905</td>\n",
       "      <td>0.874634</td>\n",
       "      <td>0.881847</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.898926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157696</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.366010</td>\n",
       "      <td>0.876129</td>\n",
       "      <td>0.886161</td>\n",
       "      <td>0.867143</td>\n",
       "      <td>0.900635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158720</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.387502</td>\n",
       "      <td>0.874162</td>\n",
       "      <td>0.885351</td>\n",
       "      <td>0.864136</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159744</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.393638</td>\n",
       "      <td>0.874267</td>\n",
       "      <td>0.883273</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160768</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.409664</td>\n",
       "      <td>0.872708</td>\n",
       "      <td>0.884437</td>\n",
       "      <td>0.862416</td>\n",
       "      <td>0.896973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161792</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.398954</td>\n",
       "      <td>0.874050</td>\n",
       "      <td>0.882891</td>\n",
       "      <td>0.865975</td>\n",
       "      <td>0.899170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162816</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.405335</td>\n",
       "      <td>0.872379</td>\n",
       "      <td>0.880872</td>\n",
       "      <td>0.864611</td>\n",
       "      <td>0.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163840</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.401820</td>\n",
       "      <td>0.873077</td>\n",
       "      <td>0.883255</td>\n",
       "      <td>0.863914</td>\n",
       "      <td>0.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164864</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.885589</td>\n",
       "      <td>0.864633</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165888</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.407441</td>\n",
       "      <td>0.871357</td>\n",
       "      <td>0.880354</td>\n",
       "      <td>0.863253</td>\n",
       "      <td>0.896240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166912</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.399722</td>\n",
       "      <td>0.871115</td>\n",
       "      <td>0.880451</td>\n",
       "      <td>0.862711</td>\n",
       "      <td>0.896729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167936</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.396717</td>\n",
       "      <td>0.875353</td>\n",
       "      <td>0.884408</td>\n",
       "      <td>0.867111</td>\n",
       "      <td>0.899902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168960</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.401450</td>\n",
       "      <td>0.872567</td>\n",
       "      <td>0.882468</td>\n",
       "      <td>0.863701</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169984</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.393740</td>\n",
       "      <td>0.873732</td>\n",
       "      <td>0.880983</td>\n",
       "      <td>0.867025</td>\n",
       "      <td>0.899170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171008</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.399896</td>\n",
       "      <td>0.875423</td>\n",
       "      <td>0.882948</td>\n",
       "      <td>0.868451</td>\n",
       "      <td>0.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172032</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.403523</td>\n",
       "      <td>0.875830</td>\n",
       "      <td>0.886151</td>\n",
       "      <td>0.866730</td>\n",
       "      <td>0.900146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173056</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.402561</td>\n",
       "      <td>0.874998</td>\n",
       "      <td>0.883976</td>\n",
       "      <td>0.866891</td>\n",
       "      <td>0.899902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174080</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.404237</td>\n",
       "      <td>0.872656</td>\n",
       "      <td>0.882652</td>\n",
       "      <td>0.863722</td>\n",
       "      <td>0.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175104</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.393283</td>\n",
       "      <td>0.875650</td>\n",
       "      <td>0.884454</td>\n",
       "      <td>0.867643</td>\n",
       "      <td>0.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176128</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.394449</td>\n",
       "      <td>0.874371</td>\n",
       "      <td>0.884240</td>\n",
       "      <td>0.865428</td>\n",
       "      <td>0.899170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177152</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.876109</td>\n",
       "      <td>0.884980</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.900635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178176</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.397396</td>\n",
       "      <td>0.874229</td>\n",
       "      <td>0.883210</td>\n",
       "      <td>0.866013</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179200</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.397363</td>\n",
       "      <td>0.874863</td>\n",
       "      <td>0.884157</td>\n",
       "      <td>0.866473</td>\n",
       "      <td>0.899170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180224</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.395819</td>\n",
       "      <td>0.873848</td>\n",
       "      <td>0.882542</td>\n",
       "      <td>0.865947</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181248</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.397762</td>\n",
       "      <td>0.871873</td>\n",
       "      <td>0.880107</td>\n",
       "      <td>0.864352</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182272</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.393815</td>\n",
       "      <td>0.871976</td>\n",
       "      <td>0.880452</td>\n",
       "      <td>0.864257</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183296</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.395412</td>\n",
       "      <td>0.872689</td>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.864660</td>\n",
       "      <td>0.897705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184320</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.398034</td>\n",
       "      <td>0.873132</td>\n",
       "      <td>0.881798</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.898193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185344</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.397649</td>\n",
       "      <td>0.873311</td>\n",
       "      <td>0.881791</td>\n",
       "      <td>0.865551</td>\n",
       "      <td>0.898193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186368</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>0.872641</td>\n",
       "      <td>0.881371</td>\n",
       "      <td>0.864660</td>\n",
       "      <td>0.897705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187392</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.397677</td>\n",
       "      <td>0.872135</td>\n",
       "      <td>0.880862</td>\n",
       "      <td>0.864192</td>\n",
       "      <td>0.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188416</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.397569</td>\n",
       "      <td>0.873538</td>\n",
       "      <td>0.882813</td>\n",
       "      <td>0.865127</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189440</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.398939</td>\n",
       "      <td>0.872845</td>\n",
       "      <td>0.882171</td>\n",
       "      <td>0.864393</td>\n",
       "      <td>0.897949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190464</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.397593</td>\n",
       "      <td>0.873160</td>\n",
       "      <td>0.882448</td>\n",
       "      <td>0.864736</td>\n",
       "      <td>0.898193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191488</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.398145</td>\n",
       "      <td>0.873453</td>\n",
       "      <td>0.882855</td>\n",
       "      <td>0.864951</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192512</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.397894</td>\n",
       "      <td>0.873685</td>\n",
       "      <td>0.882931</td>\n",
       "      <td>0.865295</td>\n",
       "      <td>0.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193536</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.397909</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194560</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.397929</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195584</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.397962</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196608</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 92/128 00:09 < 00:03, 9.60 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-1024\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-1024\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-1024\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-2048\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-2048\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-2048\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-3072\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-3072\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-3072\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-4096\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-4096\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-4096\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-5120\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-5120\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-5120\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-6144\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-6144\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-6144\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-7168\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-7168\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-7168\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-8192\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-8192\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-8192\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-5120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-18432\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-18432\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-18432\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-8192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-20480\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-20480\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-20480\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-10240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-21504\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-21504\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-21504\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-11264] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-22528\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-22528\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-22528\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-12288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-23552\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-23552\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-23552\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-13312] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-24576\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-24576\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-24576\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-14336] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-25600\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-25600\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-25600\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-15360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-26624\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-26624\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-26624\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-16384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-27648\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-27648\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-27648\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-17408] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-28672\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-28672\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-28672\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-18432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-29696\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-29696\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-29696\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-19456] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-30720\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-30720\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-30720\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-20480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-31744\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-31744\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-31744\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-21504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-32768\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-32768\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-32768\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-22528] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-33792\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-33792\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-33792\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-23552] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-34816\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-34816\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-34816\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-24576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-35840\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-35840\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-35840\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-25600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-38912\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-38912\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-38912\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-28672] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-40960\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-40960\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-40960\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-30720] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-47104\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-47104\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-47104\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-36864] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-50176\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-50176\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-50176\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-39936] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-56320\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-56320\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-56320\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-46080] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-62464\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-62464\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-62464\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-52224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-66560\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-66560\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-66560\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-56320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-70656\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-70656\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-70656\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-60416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-73728\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-73728\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-73728\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-63488] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-78848\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-78848\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-78848\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-68608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-79872\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-79872\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-79872\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-69632] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-83968\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-83968\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-83968\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-73728] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-94208\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-94208\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-94208\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-83968] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-95232\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-95232\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-95232\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-84992] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-98304\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-98304\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-98304\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-88064] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-102400\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-102400\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-102400\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-92160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-110592\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-110592\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-110592\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-100352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-114688\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-114688\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-114688\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-104448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-118784\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-118784\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-118784\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-108544] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-120832\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-120832\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-120832\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-110592] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-126976\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-126976\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-126976\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-116736] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-131072\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-131072\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-131072\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-120832] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-135168\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-135168\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-135168\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-124928] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-143360\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-143360\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-143360\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-133120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-145408\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-145408\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-145408\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-135168] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-149504\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-149504\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-149504\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-139264] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-151552\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-151552\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-151552\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-141312] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-152576\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-152576\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-152576\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-142336] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-159744\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-159744\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-159744\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-149504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-165888\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-165888\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-165888\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-155648] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "C:\\Softwarez\\Anaconda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-179200\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-179200\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-179200\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-168960] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-188416\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-188416\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-188416\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-178176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./models/2021-09-30-train\\checkpoint-197632\n",
      "Configuration saved in ./models/2021-09-30-train\\checkpoint-197632\\config.json\n",
      "Model weights saved in ./models/2021-09-30-train\\checkpoint-197632\\pytorch_model.bin\n",
      "Deleting older checkpoint [models\\2021-09-30-train\\checkpoint-187392] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "class EvaluatePleaseCallback(transformers.TrainerCallback):\n",
    "    def on_save(self, args, state, control, model, **kwargs):\n",
    "        trainer.evaluate()\n",
    "\n",
    "trainer.add_callback(EvaluatePleaseCallback())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(f'models/finalized_models/{trainer.args.output_dir.split(\"/\")[-1]}-{trainer.evaluate()[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/finalized_models/2021-09-30-train-0.8921535648994515\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"EGY\",\n",
      "    \"1\": \"GLF\",\n",
      "    \"2\": \"IRQ\",\n",
      "    \"3\": \"LEV\",\n",
      "    \"4\": \"NOR\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"EGY\": 0,\n",
      "    \"GLF\": 1,\n",
      "    \"IRQ\": 2,\n",
      "    \"LEV\": 3,\n",
      "    \"NOR\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file models/finalized_models/2021-09-30-train-0.8921535648994515\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/finalized_models/2021-09-30-train-0.8921535648994515.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# loaded_model = AutoModelForSequenceClassification.from_pretrained(\"models/finalized_models/2021-09-30-train-0.8921535648994515\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4096\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8984375"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loss', 'learning_rate', 'epoch', 'step', 'eval_loss', 'eval_macro_f1',\n",
       "       'eval_macro_precision', 'eval_macro_recall', 'eval_accuracy',\n",
       "       'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second',\n",
       "       'train_runtime', 'train_samples_per_second', 'train_steps_per_second',\n",
       "       'total_flos', 'train_loss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_macro_precision</th>\n",
       "      <th>eval_macro_recall</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1116</td>\n",
       "      <td>5.149955e-06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.740110</td>\n",
       "      <td>0.691882</td>\n",
       "      <td>0.707830</td>\n",
       "      <td>0.680998</td>\n",
       "      <td>0.748535</td>\n",
       "      <td>12.3955</td>\n",
       "      <td>330.442</td>\n",
       "      <td>10.326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7278</td>\n",
       "      <td>1.031506e-05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.622861</td>\n",
       "      <td>0.746602</td>\n",
       "      <td>0.757428</td>\n",
       "      <td>0.738106</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>12.2617</td>\n",
       "      <td>334.047</td>\n",
       "      <td>10.439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6426</td>\n",
       "      <td>1.548016e-05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.96</td>\n",
       "      <td>196608</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>12.1431</td>\n",
       "      <td>337.311</td>\n",
       "      <td>10.541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.1594</td>\n",
       "      <td>8.921633e-10</td>\n",
       "      <td>4.99</td>\n",
       "      <td>197632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.99</td>\n",
       "      <td>197632</td>\n",
       "      <td>0.397923</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>13.3141</td>\n",
       "      <td>307.645</td>\n",
       "      <td>9.614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>198065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59869.5474</td>\n",
       "      <td>105.864</td>\n",
       "      <td>3.308</td>\n",
       "      <td>4.169140e+17</td>\n",
       "      <td>0.306587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>198065</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.873381</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.865016</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>13.0561</td>\n",
       "      <td>313.723</td>\n",
       "      <td>9.804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  learning_rate  epoch    step  eval_loss  eval_macro_f1  \\\n",
       "0    1.1116   5.149955e-06   0.03    1024        NaN            NaN   \n",
       "1       NaN            NaN   0.03    1024   0.740110       0.691882   \n",
       "2    0.7278   1.031506e-05   0.05    2048        NaN            NaN   \n",
       "3       NaN            NaN   0.05    2048   0.622861       0.746602   \n",
       "4    0.6426   1.548016e-05   0.08    3072        NaN            NaN   \n",
       "..      ...            ...    ...     ...        ...            ...   \n",
       "383     NaN            NaN   4.96  196608   0.397925       0.873381   \n",
       "384  0.1594   8.921633e-10   4.99  197632        NaN            NaN   \n",
       "385     NaN            NaN   4.99  197632   0.397923       0.873381   \n",
       "386     NaN            NaN   5.00  198065        NaN            NaN   \n",
       "387     NaN            NaN   5.00  198065   0.397925       0.873381   \n",
       "\n",
       "     eval_macro_precision  eval_macro_recall  eval_accuracy  eval_runtime  \\\n",
       "0                     NaN                NaN            NaN           NaN   \n",
       "1                0.707830           0.680998       0.748535       12.3955   \n",
       "2                     NaN                NaN            NaN           NaN   \n",
       "3                0.757428           0.738106       0.787354       12.2617   \n",
       "4                     NaN                NaN            NaN           NaN   \n",
       "..                    ...                ...            ...           ...   \n",
       "383              0.882589           0.865016       0.898438       12.1431   \n",
       "384                   NaN                NaN            NaN           NaN   \n",
       "385              0.882589           0.865016       0.898438       13.3141   \n",
       "386                   NaN                NaN            NaN           NaN   \n",
       "387              0.882589           0.865016       0.898438       13.0561   \n",
       "\n",
       "     eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                        NaN                    NaN            NaN   \n",
       "1                    330.442                 10.326            NaN   \n",
       "2                        NaN                    NaN            NaN   \n",
       "3                    334.047                 10.439            NaN   \n",
       "4                        NaN                    NaN            NaN   \n",
       "..                       ...                    ...            ...   \n",
       "383                  337.311                 10.541            NaN   \n",
       "384                      NaN                    NaN            NaN   \n",
       "385                  307.645                  9.614            NaN   \n",
       "386                      NaN                    NaN     59869.5474   \n",
       "387                  313.723                  9.804            NaN   \n",
       "\n",
       "     train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "0                         NaN                     NaN           NaN   \n",
       "1                         NaN                     NaN           NaN   \n",
       "2                         NaN                     NaN           NaN   \n",
       "3                         NaN                     NaN           NaN   \n",
       "4                         NaN                     NaN           NaN   \n",
       "..                        ...                     ...           ...   \n",
       "383                       NaN                     NaN           NaN   \n",
       "384                       NaN                     NaN           NaN   \n",
       "385                       NaN                     NaN           NaN   \n",
       "386                   105.864                   3.308  4.169140e+17   \n",
       "387                       NaN                     NaN           NaN   \n",
       "\n",
       "     train_loss  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "..          ...  \n",
       "383         NaN  \n",
       "384         NaN  \n",
       "385         NaN  \n",
       "386    0.306587  \n",
       "387         NaN  \n",
       "\n",
       "[388 rows x 17 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy = history[[\"eval_accuracy\", \"eval_macro_recall\", \"eval_macro_precision\"]].dropna()\n",
    "loss = history[\"loss\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=eval_accuracy<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "eval_accuracy",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "eval_accuracy",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          3,
          5,
          7,
          9,
          11,
          13,
          15,
          17,
          19,
          21,
          23,
          25,
          27,
          29,
          31,
          33,
          35,
          37,
          39,
          41,
          43,
          45,
          47,
          49,
          51,
          53,
          55,
          57,
          59,
          61,
          63,
          65,
          67,
          69,
          71,
          73,
          75,
          77,
          79,
          81,
          83,
          85,
          87,
          89,
          91,
          93,
          95,
          97,
          99,
          101,
          103,
          105,
          107,
          109,
          111,
          113,
          115,
          117,
          119,
          121,
          123,
          125,
          127,
          129,
          131,
          133,
          135,
          137,
          139,
          141,
          143,
          145,
          147,
          149,
          151,
          153,
          155,
          157,
          159,
          161,
          163,
          165,
          167,
          169,
          171,
          173,
          175,
          177,
          179,
          181,
          183,
          185,
          187,
          189,
          191,
          193,
          195,
          197,
          199,
          201,
          203,
          205,
          207,
          209,
          211,
          213,
          215,
          217,
          219,
          221,
          223,
          225,
          227,
          229,
          231,
          233,
          235,
          237,
          239,
          241,
          243,
          245,
          247,
          249,
          251,
          253,
          255,
          257,
          259,
          261,
          263,
          265,
          267,
          269,
          271,
          273,
          275,
          277,
          279,
          281,
          283,
          285,
          287,
          289,
          291,
          293,
          295,
          297,
          299,
          301,
          303,
          305,
          307,
          309,
          311,
          313,
          315,
          317,
          319,
          321,
          323,
          325,
          327,
          329,
          331,
          333,
          335,
          337,
          339,
          341,
          343,
          345,
          347,
          349,
          351,
          353,
          355,
          357,
          359,
          361,
          363,
          365,
          367,
          369,
          371,
          373,
          375,
          377,
          379,
          381,
          383,
          385,
          387
         ],
         "xaxis": "x",
         "y": [
          0.74853515625,
          0.787353515625,
          0.798095703125,
          0.803466796875,
          0.80419921875,
          0.823974609375,
          0.827880859375,
          0.81787109375,
          0.830078125,
          0.826416015625,
          0.82666015625,
          0.827880859375,
          0.833740234375,
          0.832275390625,
          0.837890625,
          0.842529296875,
          0.8388671875,
          0.843505859375,
          0.83544921875,
          0.83642578125,
          0.846435546875,
          0.839599609375,
          0.84619140625,
          0.84765625,
          0.8486328125,
          0.842041015625,
          0.849365234375,
          0.852294921875,
          0.85595703125,
          0.854736328125,
          0.862060546875,
          0.85693359375,
          0.856201171875,
          0.85546875,
          0.858154296875,
          0.85693359375,
          0.85986328125,
          0.862548828125,
          0.8662109375,
          0.865234375,
          0.857421875,
          0.858154296875,
          0.85791015625,
          0.8623046875,
          0.85986328125,
          0.865966796875,
          0.865478515625,
          0.863037109375,
          0.8662109375,
          0.87109375,
          0.8681640625,
          0.8681640625,
          0.8701171875,
          0.8681640625,
          0.863037109375,
          0.8662109375,
          0.86669921875,
          0.870361328125,
          0.86474609375,
          0.86767578125,
          0.87158203125,
          0.874267578125,
          0.872802734375,
          0.877685546875,
          0.87646484375,
          0.8740234375,
          0.8759765625,
          0.87646484375,
          0.87109375,
          0.870849609375,
          0.876953125,
          0.873291015625,
          0.875244140625,
          0.876220703125,
          0.87646484375,
          0.87841796875,
          0.8857421875,
          0.878173828125,
          0.878173828125,
          0.880859375,
          0.8759765625,
          0.87548828125,
          0.87548828125,
          0.87890625,
          0.882080078125,
          0.880615234375,
          0.88525390625,
          0.87939453125,
          0.876708984375,
          0.87890625,
          0.883056640625,
          0.884521484375,
          0.883544921875,
          0.8828125,
          0.886962890625,
          0.886474609375,
          0.8857421875,
          0.8857421875,
          0.884765625,
          0.884765625,
          0.88818359375,
          0.8896484375,
          0.889404296875,
          0.88818359375,
          0.890380859375,
          0.888671875,
          0.89404296875,
          0.888916015625,
          0.888427734375,
          0.888427734375,
          0.89111328125,
          0.8916015625,
          0.889404296875,
          0.88671875,
          0.890625,
          0.8935546875,
          0.891845703125,
          0.889892578125,
          0.891845703125,
          0.890625,
          0.892578125,
          0.893798828125,
          0.892578125,
          0.888916015625,
          0.891357421875,
          0.892822265625,
          0.89404296875,
          0.897216796875,
          0.89111328125,
          0.893310546875,
          0.892822265625,
          0.896240234375,
          0.89501953125,
          0.894287109375,
          0.89306640625,
          0.896484375,
          0.896484375,
          0.8974609375,
          0.892333984375,
          0.896240234375,
          0.89501953125,
          0.897216796875,
          0.895263671875,
          0.897216796875,
          0.89892578125,
          0.898193359375,
          0.8994140625,
          0.896240234375,
          0.89501953125,
          0.900146484375,
          0.89794921875,
          0.897705078125,
          0.89892578125,
          0.900634765625,
          0.898681640625,
          0.898681640625,
          0.89697265625,
          0.899169921875,
          0.89794921875,
          0.89794921875,
          0.8984375,
          0.896240234375,
          0.896728515625,
          0.89990234375,
          0.897216796875,
          0.899169921875,
          0.900390625,
          0.900146484375,
          0.89990234375,
          0.89794921875,
          0.900390625,
          0.899169921875,
          0.900634765625,
          0.898681640625,
          0.899169921875,
          0.898681640625,
          0.897216796875,
          0.897216796875,
          0.897705078125,
          0.898193359375,
          0.898193359375,
          0.897705078125,
          0.897216796875,
          0.898681640625,
          0.89794921875,
          0.898193359375,
          0.8984375,
          0.898681640625,
          0.8984375,
          0.8984375,
          0.8984375,
          0.8984375,
          0.8984375,
          0.8984375
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=eval_macro_recall<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "eval_macro_recall",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "eval_macro_recall",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          3,
          5,
          7,
          9,
          11,
          13,
          15,
          17,
          19,
          21,
          23,
          25,
          27,
          29,
          31,
          33,
          35,
          37,
          39,
          41,
          43,
          45,
          47,
          49,
          51,
          53,
          55,
          57,
          59,
          61,
          63,
          65,
          67,
          69,
          71,
          73,
          75,
          77,
          79,
          81,
          83,
          85,
          87,
          89,
          91,
          93,
          95,
          97,
          99,
          101,
          103,
          105,
          107,
          109,
          111,
          113,
          115,
          117,
          119,
          121,
          123,
          125,
          127,
          129,
          131,
          133,
          135,
          137,
          139,
          141,
          143,
          145,
          147,
          149,
          151,
          153,
          155,
          157,
          159,
          161,
          163,
          165,
          167,
          169,
          171,
          173,
          175,
          177,
          179,
          181,
          183,
          185,
          187,
          189,
          191,
          193,
          195,
          197,
          199,
          201,
          203,
          205,
          207,
          209,
          211,
          213,
          215,
          217,
          219,
          221,
          223,
          225,
          227,
          229,
          231,
          233,
          235,
          237,
          239,
          241,
          243,
          245,
          247,
          249,
          251,
          253,
          255,
          257,
          259,
          261,
          263,
          265,
          267,
          269,
          271,
          273,
          275,
          277,
          279,
          281,
          283,
          285,
          287,
          289,
          291,
          293,
          295,
          297,
          299,
          301,
          303,
          305,
          307,
          309,
          311,
          313,
          315,
          317,
          319,
          321,
          323,
          325,
          327,
          329,
          331,
          333,
          335,
          337,
          339,
          341,
          343,
          345,
          347,
          349,
          351,
          353,
          355,
          357,
          359,
          361,
          363,
          365,
          367,
          369,
          371,
          373,
          375,
          377,
          379,
          381,
          383,
          385,
          387
         ],
         "xaxis": "x",
         "y": [
          0.680997515897721,
          0.7381057149558276,
          0.7548403098577753,
          0.7516311399782558,
          0.7601887536882223,
          0.7840538594012834,
          0.7901779168847956,
          0.7819894690975264,
          0.7901162359500729,
          0.79648515202462,
          0.7806176601630901,
          0.7918267297394974,
          0.785113290411924,
          0.8039801968505593,
          0.8019510183188159,
          0.8018502146282215,
          0.8128984101999898,
          0.8048796766190008,
          0.8007876027725456,
          0.7943383940401467,
          0.816888815534203,
          0.8006978508421188,
          0.8040711232279737,
          0.800430264002931,
          0.8034066895114842,
          0.807529742794407,
          0.8049045447931367,
          0.8140937733596043,
          0.8179606029296884,
          0.818058705285585,
          0.829151028653077,
          0.8238959264978071,
          0.8190922241727264,
          0.820029628865016,
          0.8320131143855896,
          0.8288887601686901,
          0.8209328631347566,
          0.8328261785218112,
          0.8399137774482602,
          0.8333570823456109,
          0.8263765025916076,
          0.830665038368253,
          0.8196524202751314,
          0.8352034122007179,
          0.8313463380547106,
          0.8402804767579625,
          0.8402002426889428,
          0.8302134281542038,
          0.8302615471396706,
          0.8317813930292965,
          0.8368777658528161,
          0.830430707438112,
          0.8440355224875822,
          0.8394700998413139,
          0.8328447274434805,
          0.8357770055473607,
          0.8387876798100183,
          0.8348689850046259,
          0.8277077193605191,
          0.8370454410128934,
          0.8409949176326164,
          0.8404646148077541,
          0.8416072934320915,
          0.8451347204235298,
          0.8417648135394311,
          0.8384059853098409,
          0.8439135285326314,
          0.8408842614164854,
          0.8406217510590641,
          0.8374372263859868,
          0.838668942266182,
          0.8397723893014246,
          0.839688951432979,
          0.8407020104457329,
          0.8383705047769195,
          0.8406078953455254,
          0.8550684892300915,
          0.8498167252626165,
          0.847805961995924,
          0.8497609123029868,
          0.8465994541092996,
          0.8416661103352926,
          0.8429655522189157,
          0.8422375877346926,
          0.8550158713063982,
          0.8496752567129947,
          0.8521224077404626,
          0.8506269788459064,
          0.8439930866915187,
          0.8453614979768167,
          0.8530035560852978,
          0.8512494449304894,
          0.8503474912701288,
          0.8483699292351738,
          0.8527556541875125,
          0.8533381795943826,
          0.8531614931688685,
          0.8550132870217716,
          0.853757530436909,
          0.8528515367041403,
          0.8547476251290158,
          0.8575361337700764,
          0.8592864442408417,
          0.8528623569891671,
          0.8576680289076138,
          0.855754017406219,
          0.8652804082001753,
          0.8574592439470962,
          0.854970867163632,
          0.858239454456361,
          0.8570409623369727,
          0.85885650892865,
          0.8558994611617401,
          0.8544987928335687,
          0.859018221792339,
          0.8622177122362856,
          0.8616939580672286,
          0.8554313453848226,
          0.8589196766595292,
          0.8595028962349758,
          0.8621040204809715,
          0.8638807878080946,
          0.8625762937473154,
          0.8555397441371131,
          0.8577803925200123,
          0.8626033545880913,
          0.862418121971612,
          0.8673308448609138,
          0.8593419629996468,
          0.860700356513074,
          0.8587050708499119,
          0.8631122118456973,
          0.8633170562858983,
          0.8607701936326471,
          0.8617467479807595,
          0.8650292598523415,
          0.8644297861346886,
          0.8655105351375999,
          0.8587713942126672,
          0.8659761556159008,
          0.8636300860126891,
          0.8652996180660153,
          0.8608333017044247,
          0.8655352447280894,
          0.865165686110954,
          0.8649165264361969,
          0.8664604561176541,
          0.863406575135493,
          0.8605365825151731,
          0.8685661131178494,
          0.8638874080054604,
          0.863579661907864,
          0.8679254057774684,
          0.8671426700086645,
          0.8641355865607416,
          0.8660251930416518,
          0.8624155640980075,
          0.865975056957996,
          0.8646108788115378,
          0.8639135897871958,
          0.8646333449087977,
          0.8632530442049493,
          0.8627109006929345,
          0.8671114624841836,
          0.8637013926937538,
          0.8670252635408374,
          0.8684507212226604,
          0.8667296332281189,
          0.8668912243407512,
          0.8637216707211554,
          0.8676428225668641,
          0.865427528728314,
          0.8681315970561416,
          0.8660132311388631,
          0.8664726158343512,
          0.8659469170744464,
          0.8643522488554012,
          0.864256583307883,
          0.8646598079448108,
          0.8651958094080712,
          0.8655508767257416,
          0.8646598079448108,
          0.8641922703051377,
          0.8651266633673895,
          0.8643926417297765,
          0.8647362843414603,
          0.8649513009476536,
          0.8652949435593374,
          0.8650156139503988,
          0.8650156139503988,
          0.8650156139503988,
          0.8650156139503988,
          0.8650156139503988,
          0.8650156139503988
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=eval_macro_precision<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "eval_macro_precision",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "eval_macro_precision",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          3,
          5,
          7,
          9,
          11,
          13,
          15,
          17,
          19,
          21,
          23,
          25,
          27,
          29,
          31,
          33,
          35,
          37,
          39,
          41,
          43,
          45,
          47,
          49,
          51,
          53,
          55,
          57,
          59,
          61,
          63,
          65,
          67,
          69,
          71,
          73,
          75,
          77,
          79,
          81,
          83,
          85,
          87,
          89,
          91,
          93,
          95,
          97,
          99,
          101,
          103,
          105,
          107,
          109,
          111,
          113,
          115,
          117,
          119,
          121,
          123,
          125,
          127,
          129,
          131,
          133,
          135,
          137,
          139,
          141,
          143,
          145,
          147,
          149,
          151,
          153,
          155,
          157,
          159,
          161,
          163,
          165,
          167,
          169,
          171,
          173,
          175,
          177,
          179,
          181,
          183,
          185,
          187,
          189,
          191,
          193,
          195,
          197,
          199,
          201,
          203,
          205,
          207,
          209,
          211,
          213,
          215,
          217,
          219,
          221,
          223,
          225,
          227,
          229,
          231,
          233,
          235,
          237,
          239,
          241,
          243,
          245,
          247,
          249,
          251,
          253,
          255,
          257,
          259,
          261,
          263,
          265,
          267,
          269,
          271,
          273,
          275,
          277,
          279,
          281,
          283,
          285,
          287,
          289,
          291,
          293,
          295,
          297,
          299,
          301,
          303,
          305,
          307,
          309,
          311,
          313,
          315,
          317,
          319,
          321,
          323,
          325,
          327,
          329,
          331,
          333,
          335,
          337,
          339,
          341,
          343,
          345,
          347,
          349,
          351,
          353,
          355,
          357,
          359,
          361,
          363,
          365,
          367,
          369,
          371,
          373,
          375,
          377,
          379,
          381,
          383,
          385,
          387
         ],
         "xaxis": "x",
         "y": [
          0.7078302721017071,
          0.7574277592299664,
          0.765893655383137,
          0.7874105024981949,
          0.7886612503412185,
          0.7991794912815694,
          0.8068255700073698,
          0.7867021431262872,
          0.8065365576509823,
          0.7984686785302658,
          0.8188055315507159,
          0.812905202557897,
          0.8275947426125458,
          0.8033232804973442,
          0.8177807551353664,
          0.8276611308759494,
          0.8047674716388382,
          0.82527597250198,
          0.8154177538909076,
          0.8250355603448123,
          0.8257758264396365,
          0.8228694140898425,
          0.8341094115551831,
          0.8467141625344267,
          0.8374698698032598,
          0.8270336267402497,
          0.8448312979136151,
          0.8398558274323665,
          0.8435040697280582,
          0.8405538587677388,
          0.8480406977116293,
          0.8416237020220123,
          0.8478692700631477,
          0.8421353183608682,
          0.8381292017725116,
          0.8388975985597812,
          0.8549572371034705,
          0.8432291514061916,
          0.8489151453564592,
          0.853525257127882,
          0.8368382365778351,
          0.8355813801594397,
          0.8485355612437047,
          0.8395587286945606,
          0.8382173555723815,
          0.8448722230869701,
          0.8447504450222623,
          0.847943848493679,
          0.8562586878266114,
          0.8674927264076064,
          0.8505644018150388,
          0.8577450755950021,
          0.8536761983893377,
          0.8481629669396259,
          0.8459736819542181,
          0.849839487174225,
          0.8459429129940759,
          0.8572961788603404,
          0.8543210917902286,
          0.8512628773302449,
          0.8529351899158539,
          0.8619982158853536,
          0.8553204695712685,
          0.8656894966718337,
          0.8653849610494009,
          0.8622137010968629,
          0.8585235834772231,
          0.8659016339143577,
          0.8495252165797403,
          0.8555972539841695,
          0.8689330389455021,
          0.8544297292106607,
          0.865247874204288,
          0.8667180997281253,
          0.8682864779789652,
          0.8725306201457166,
          0.8731165374724895,
          0.8604949056681577,
          0.8588628479438236,
          0.8672451385443232,
          0.8565286224101122,
          0.8597392893646644,
          0.8550065578343842,
          0.8698233865684137,
          0.8643707466598863,
          0.8628128661936401,
          0.8725031950276223,
          0.860308915838535,
          0.8584112945449739,
          0.8648317345497603,
          0.8678793769125243,
          0.8708402165298992,
          0.8686599861471134,
          0.8696816831999392,
          0.8720192537060493,
          0.8749102588255399,
          0.8720540698775645,
          0.8692442464869966,
          0.8678681901118708,
          0.8668281282030279,
          0.8789196861606252,
          0.875599790599567,
          0.8726788506557105,
          0.878600042863571,
          0.8771984742370739,
          0.8734188376613773,
          0.8767273727761957,
          0.8690833587644281,
          0.8721455610498303,
          0.873381931917278,
          0.874724036646132,
          0.8760879245015285,
          0.8747441035874385,
          0.8701172354797138,
          0.8751192270098074,
          0.8786915499355178,
          0.8751892744517118,
          0.8777121655515823,
          0.8766515066957374,
          0.8745924013938515,
          0.8756267537016257,
          0.8786155622578692,
          0.8750259339348216,
          0.8745070443106737,
          0.8785181900940466,
          0.8761310619780825,
          0.878860092308767,
          0.8814311705050425,
          0.8718130284687874,
          0.8762486229471881,
          0.8793817153470804,
          0.881692190570859,
          0.8798473358079868,
          0.8786648114368946,
          0.874662384743862,
          0.8805355237617503,
          0.8795529483067558,
          0.8825463573998032,
          0.873530792138616,
          0.8789143773442263,
          0.8787636610099018,
          0.8818479765982493,
          0.8809070945521501,
          0.881020048977368,
          0.8866160768169152,
          0.8851525179511042,
          0.8860171744381707,
          0.8833250725432347,
          0.8806902133202901,
          0.8866509726624796,
          0.8859503883093861,
          0.8842011860482621,
          0.8818468011088557,
          0.8861607823542557,
          0.8853514937882292,
          0.8832729192772293,
          0.8844368082014589,
          0.8828914699553326,
          0.8808719686115483,
          0.8832545612842793,
          0.8855892579694796,
          0.8803535819107035,
          0.8804509271384798,
          0.884407983034891,
          0.8824676714827246,
          0.8809831080739834,
          0.8829478689363492,
          0.8861514077376788,
          0.8839758485870168,
          0.8826521994175615,
          0.8844536146707384,
          0.8842402152453837,
          0.8849797052166674,
          0.8832098271958391,
          0.8841568108868227,
          0.8825419611349028,
          0.8801069367412516,
          0.8804522378026582,
          0.8815083331391401,
          0.8817980835834713,
          0.8817909514157793,
          0.881371434189367,
          0.8808622745778576,
          0.8828126063099127,
          0.8821710303036687,
          0.8824477988004119,
          0.8828547257687702,
          0.8829312304395586,
          0.8825893278066724,
          0.8825893278066724,
          0.8825893278066724,
          0.8825893278066724,
          0.8825893278066724,
          0.8825893278066724
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "autosize": true,
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Steps over loss"
        },
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          1,
          387
         ],
         "title": {
          "text": "index"
         },
         "type": "linear"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          0.6687954464684277,
          0.9128368350542932
         ],
         "title": {
          "text": "value"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"49a8c4bc-3d45-41f6-b886-675fdfeb70c6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"49a8c4bc-3d45-41f6-b886-675fdfeb70c6\")) {                    Plotly.newPlot(                        \"49a8c4bc-3d45-41f6-b886-675fdfeb70c6\",                        [{\"hovertemplate\": \"variable=eval_accuracy<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"eval_accuracy\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"eval_accuracy\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363, 365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387], \"xaxis\": \"x\", \"y\": [0.74853515625, 0.787353515625, 0.798095703125, 0.803466796875, 0.80419921875, 0.823974609375, 0.827880859375, 0.81787109375, 0.830078125, 0.826416015625, 0.82666015625, 0.827880859375, 0.833740234375, 0.832275390625, 0.837890625, 0.842529296875, 0.8388671875, 0.843505859375, 0.83544921875, 0.83642578125, 0.846435546875, 0.839599609375, 0.84619140625, 0.84765625, 0.8486328125, 0.842041015625, 0.849365234375, 0.852294921875, 0.85595703125, 0.854736328125, 0.862060546875, 0.85693359375, 0.856201171875, 0.85546875, 0.858154296875, 0.85693359375, 0.85986328125, 0.862548828125, 0.8662109375, 0.865234375, 0.857421875, 0.858154296875, 0.85791015625, 0.8623046875, 0.85986328125, 0.865966796875, 0.865478515625, 0.863037109375, 0.8662109375, 0.87109375, 0.8681640625, 0.8681640625, 0.8701171875, 0.8681640625, 0.863037109375, 0.8662109375, 0.86669921875, 0.870361328125, 0.86474609375, 0.86767578125, 0.87158203125, 0.874267578125, 0.872802734375, 0.877685546875, 0.87646484375, 0.8740234375, 0.8759765625, 0.87646484375, 0.87109375, 0.870849609375, 0.876953125, 0.873291015625, 0.875244140625, 0.876220703125, 0.87646484375, 0.87841796875, 0.8857421875, 0.878173828125, 0.878173828125, 0.880859375, 0.8759765625, 0.87548828125, 0.87548828125, 0.87890625, 0.882080078125, 0.880615234375, 0.88525390625, 0.87939453125, 0.876708984375, 0.87890625, 0.883056640625, 0.884521484375, 0.883544921875, 0.8828125, 0.886962890625, 0.886474609375, 0.8857421875, 0.8857421875, 0.884765625, 0.884765625, 0.88818359375, 0.8896484375, 0.889404296875, 0.88818359375, 0.890380859375, 0.888671875, 0.89404296875, 0.888916015625, 0.888427734375, 0.888427734375, 0.89111328125, 0.8916015625, 0.889404296875, 0.88671875, 0.890625, 0.8935546875, 0.891845703125, 0.889892578125, 0.891845703125, 0.890625, 0.892578125, 0.893798828125, 0.892578125, 0.888916015625, 0.891357421875, 0.892822265625, 0.89404296875, 0.897216796875, 0.89111328125, 0.893310546875, 0.892822265625, 0.896240234375, 0.89501953125, 0.894287109375, 0.89306640625, 0.896484375, 0.896484375, 0.8974609375, 0.892333984375, 0.896240234375, 0.89501953125, 0.897216796875, 0.895263671875, 0.897216796875, 0.89892578125, 0.898193359375, 0.8994140625, 0.896240234375, 0.89501953125, 0.900146484375, 0.89794921875, 0.897705078125, 0.89892578125, 0.900634765625, 0.898681640625, 0.898681640625, 0.89697265625, 0.899169921875, 0.89794921875, 0.89794921875, 0.8984375, 0.896240234375, 0.896728515625, 0.89990234375, 0.897216796875, 0.899169921875, 0.900390625, 0.900146484375, 0.89990234375, 0.89794921875, 0.900390625, 0.899169921875, 0.900634765625, 0.898681640625, 0.899169921875, 0.898681640625, 0.897216796875, 0.897216796875, 0.897705078125, 0.898193359375, 0.898193359375, 0.897705078125, 0.897216796875, 0.898681640625, 0.89794921875, 0.898193359375, 0.8984375, 0.898681640625, 0.8984375, 0.8984375, 0.8984375, 0.8984375, 0.8984375, 0.8984375], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=eval_macro_recall<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"eval_macro_recall\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"eval_macro_recall\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363, 365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387], \"xaxis\": \"x\", \"y\": [0.680997515897721, 0.7381057149558276, 0.7548403098577753, 0.7516311399782558, 0.7601887536882223, 0.7840538594012834, 0.7901779168847956, 0.7819894690975264, 0.7901162359500729, 0.79648515202462, 0.7806176601630901, 0.7918267297394974, 0.785113290411924, 0.8039801968505593, 0.8019510183188159, 0.8018502146282215, 0.8128984101999898, 0.8048796766190008, 0.8007876027725456, 0.7943383940401467, 0.816888815534203, 0.8006978508421188, 0.8040711232279737, 0.800430264002931, 0.8034066895114842, 0.807529742794407, 0.8049045447931367, 0.8140937733596043, 0.8179606029296884, 0.818058705285585, 0.829151028653077, 0.8238959264978071, 0.8190922241727264, 0.820029628865016, 0.8320131143855896, 0.8288887601686901, 0.8209328631347566, 0.8328261785218112, 0.8399137774482602, 0.8333570823456109, 0.8263765025916076, 0.830665038368253, 0.8196524202751314, 0.8352034122007179, 0.8313463380547106, 0.8402804767579625, 0.8402002426889428, 0.8302134281542038, 0.8302615471396706, 0.8317813930292965, 0.8368777658528161, 0.830430707438112, 0.8440355224875822, 0.8394700998413139, 0.8328447274434805, 0.8357770055473607, 0.8387876798100183, 0.8348689850046259, 0.8277077193605191, 0.8370454410128934, 0.8409949176326164, 0.8404646148077541, 0.8416072934320915, 0.8451347204235298, 0.8417648135394311, 0.8384059853098409, 0.8439135285326314, 0.8408842614164854, 0.8406217510590641, 0.8374372263859868, 0.838668942266182, 0.8397723893014246, 0.839688951432979, 0.8407020104457329, 0.8383705047769195, 0.8406078953455254, 0.8550684892300915, 0.8498167252626165, 0.847805961995924, 0.8497609123029868, 0.8465994541092996, 0.8416661103352926, 0.8429655522189157, 0.8422375877346926, 0.8550158713063982, 0.8496752567129947, 0.8521224077404626, 0.8506269788459064, 0.8439930866915187, 0.8453614979768167, 0.8530035560852978, 0.8512494449304894, 0.8503474912701288, 0.8483699292351738, 0.8527556541875125, 0.8533381795943826, 0.8531614931688685, 0.8550132870217716, 0.853757530436909, 0.8528515367041403, 0.8547476251290158, 0.8575361337700764, 0.8592864442408417, 0.8528623569891671, 0.8576680289076138, 0.855754017406219, 0.8652804082001753, 0.8574592439470962, 0.854970867163632, 0.858239454456361, 0.8570409623369727, 0.85885650892865, 0.8558994611617401, 0.8544987928335687, 0.859018221792339, 0.8622177122362856, 0.8616939580672286, 0.8554313453848226, 0.8589196766595292, 0.8595028962349758, 0.8621040204809715, 0.8638807878080946, 0.8625762937473154, 0.8555397441371131, 0.8577803925200123, 0.8626033545880913, 0.862418121971612, 0.8673308448609138, 0.8593419629996468, 0.860700356513074, 0.8587050708499119, 0.8631122118456973, 0.8633170562858983, 0.8607701936326471, 0.8617467479807595, 0.8650292598523415, 0.8644297861346886, 0.8655105351375999, 0.8587713942126672, 0.8659761556159008, 0.8636300860126891, 0.8652996180660153, 0.8608333017044247, 0.8655352447280894, 0.865165686110954, 0.8649165264361969, 0.8664604561176541, 0.863406575135493, 0.8605365825151731, 0.8685661131178494, 0.8638874080054604, 0.863579661907864, 0.8679254057774684, 0.8671426700086645, 0.8641355865607416, 0.8660251930416518, 0.8624155640980075, 0.865975056957996, 0.8646108788115378, 0.8639135897871958, 0.8646333449087977, 0.8632530442049493, 0.8627109006929345, 0.8671114624841836, 0.8637013926937538, 0.8670252635408374, 0.8684507212226604, 0.8667296332281189, 0.8668912243407512, 0.8637216707211554, 0.8676428225668641, 0.865427528728314, 0.8681315970561416, 0.8660132311388631, 0.8664726158343512, 0.8659469170744464, 0.8643522488554012, 0.864256583307883, 0.8646598079448108, 0.8651958094080712, 0.8655508767257416, 0.8646598079448108, 0.8641922703051377, 0.8651266633673895, 0.8643926417297765, 0.8647362843414603, 0.8649513009476536, 0.8652949435593374, 0.8650156139503988, 0.8650156139503988, 0.8650156139503988, 0.8650156139503988, 0.8650156139503988, 0.8650156139503988], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=eval_macro_precision<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"eval_macro_precision\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"eval_macro_precision\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363, 365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387], \"xaxis\": \"x\", \"y\": [0.7078302721017071, 0.7574277592299664, 0.765893655383137, 0.7874105024981949, 0.7886612503412185, 0.7991794912815694, 0.8068255700073698, 0.7867021431262872, 0.8065365576509823, 0.7984686785302658, 0.8188055315507159, 0.812905202557897, 0.8275947426125458, 0.8033232804973442, 0.8177807551353664, 0.8276611308759494, 0.8047674716388382, 0.82527597250198, 0.8154177538909076, 0.8250355603448123, 0.8257758264396365, 0.8228694140898425, 0.8341094115551831, 0.8467141625344267, 0.8374698698032598, 0.8270336267402497, 0.8448312979136151, 0.8398558274323665, 0.8435040697280582, 0.8405538587677388, 0.8480406977116293, 0.8416237020220123, 0.8478692700631477, 0.8421353183608682, 0.8381292017725116, 0.8388975985597812, 0.8549572371034705, 0.8432291514061916, 0.8489151453564592, 0.853525257127882, 0.8368382365778351, 0.8355813801594397, 0.8485355612437047, 0.8395587286945606, 0.8382173555723815, 0.8448722230869701, 0.8447504450222623, 0.847943848493679, 0.8562586878266114, 0.8674927264076064, 0.8505644018150388, 0.8577450755950021, 0.8536761983893377, 0.8481629669396259, 0.8459736819542181, 0.849839487174225, 0.8459429129940759, 0.8572961788603404, 0.8543210917902286, 0.8512628773302449, 0.8529351899158539, 0.8619982158853536, 0.8553204695712685, 0.8656894966718337, 0.8653849610494009, 0.8622137010968629, 0.8585235834772231, 0.8659016339143577, 0.8495252165797403, 0.8555972539841695, 0.8689330389455021, 0.8544297292106607, 0.865247874204288, 0.8667180997281253, 0.8682864779789652, 0.8725306201457166, 0.8731165374724895, 0.8604949056681577, 0.8588628479438236, 0.8672451385443232, 0.8565286224101122, 0.8597392893646644, 0.8550065578343842, 0.8698233865684137, 0.8643707466598863, 0.8628128661936401, 0.8725031950276223, 0.860308915838535, 0.8584112945449739, 0.8648317345497603, 0.8678793769125243, 0.8708402165298992, 0.8686599861471134, 0.8696816831999392, 0.8720192537060493, 0.8749102588255399, 0.8720540698775645, 0.8692442464869966, 0.8678681901118708, 0.8668281282030279, 0.8789196861606252, 0.875599790599567, 0.8726788506557105, 0.878600042863571, 0.8771984742370739, 0.8734188376613773, 0.8767273727761957, 0.8690833587644281, 0.8721455610498303, 0.873381931917278, 0.874724036646132, 0.8760879245015285, 0.8747441035874385, 0.8701172354797138, 0.8751192270098074, 0.8786915499355178, 0.8751892744517118, 0.8777121655515823, 0.8766515066957374, 0.8745924013938515, 0.8756267537016257, 0.8786155622578692, 0.8750259339348216, 0.8745070443106737, 0.8785181900940466, 0.8761310619780825, 0.878860092308767, 0.8814311705050425, 0.8718130284687874, 0.8762486229471881, 0.8793817153470804, 0.881692190570859, 0.8798473358079868, 0.8786648114368946, 0.874662384743862, 0.8805355237617503, 0.8795529483067558, 0.8825463573998032, 0.873530792138616, 0.8789143773442263, 0.8787636610099018, 0.8818479765982493, 0.8809070945521501, 0.881020048977368, 0.8866160768169152, 0.8851525179511042, 0.8860171744381707, 0.8833250725432347, 0.8806902133202901, 0.8866509726624796, 0.8859503883093861, 0.8842011860482621, 0.8818468011088557, 0.8861607823542557, 0.8853514937882292, 0.8832729192772293, 0.8844368082014589, 0.8828914699553326, 0.8808719686115483, 0.8832545612842793, 0.8855892579694796, 0.8803535819107035, 0.8804509271384798, 0.884407983034891, 0.8824676714827246, 0.8809831080739834, 0.8829478689363492, 0.8861514077376788, 0.8839758485870168, 0.8826521994175615, 0.8844536146707384, 0.8842402152453837, 0.8849797052166674, 0.8832098271958391, 0.8841568108868227, 0.8825419611349028, 0.8801069367412516, 0.8804522378026582, 0.8815083331391401, 0.8817980835834713, 0.8817909514157793, 0.881371434189367, 0.8808622745778576, 0.8828126063099127, 0.8821710303036687, 0.8824477988004119, 0.8828547257687702, 0.8829312304395586, 0.8825893278066724, 0.8825893278066724, 0.8825893278066724, 0.8825893278066724, 0.8825893278066724, 0.8825893278066724], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Steps over loss\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('49a8c4bc-3d45-41f6-b886-675fdfeb70c6');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_accuracy.plot(backend=\"plotly\", title=\"Steps over loss\")\n",
    "# fig.add_trace(next(fig.add_trace[].plot(backend=\"plotly\").select_traces(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5),\n",
    "        \"gradient_accumulation_steps\" : trail.suggest_int(\"gradient_accumulation_steps\" , 1 , 10)\n",
    "    }\n",
    "\n",
    "search_space = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5),\n",
    "        \"gradient_accumulation_steps\" : trail.suggest_int(\"gradient_accumulation_steps\" , 1 , 10)\n",
    "}\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics['eval_accuracy'] # or try \"accuracy\" if didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_model = trainer.hyperparameter_search(direction=\"maximize\",\n",
    "                                         hp_space=hp_space,\n",
    "                                         compute_objective=my_objective,\n",
    "                                         n_trials=None,\n",
    "                                         pruner=optuna.pruners.NopPruner(),\n",
    "                                         sampler=optuna.samplers.GridSampler(search_space),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.hyperparameter_search(n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelWrapper(torch.nn.Module):\n",
    "#     def __init__(self, model: torch.nn.Module):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, input_x: torch.Tensor):\n",
    "#         data = self.model(input_x)\n",
    "\n",
    "#         if isinstance(data, dict):\n",
    "#             data_named_tuple = namedtuple(\"ModelEndpoints\", sorted(data.keys())) \n",
    "#             data = data_named_tuple(**data)\n",
    "\n",
    "#         elif isinstance(data, list):\n",
    "#             data = tuple(data)\n",
    "\n",
    "#         return data\n",
    "    \n",
    "# writer = SummaryWriter()\n",
    "# _, tensorboard = train_test_split(validate, test_size=100, random_state=1)\n",
    "# costume_data = validate_dataset.X[\"input_ids\"][:100].cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
