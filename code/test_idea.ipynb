{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import torch\n",
    "from torch import nn\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "orignal_egy = fasttext.load_model(\"C:/Users/Abdul/Desktop/Grad Project/code/Embeddings/embeddings-20220313T080929Z-001/embeddings/embedding_EGY.bin\")\n",
    "mapped_egy = KeyedVectors.load_word2vec_format(\"C:/Users/Abdul/Desktop/Grad Project/code/Embeddings/embeddings-20220313T080929Z-001/embeddings/vectors-EGY.txt\")\n",
    "assert orignal_egy.get_words() == mapped_egy.index_to_key\n",
    "word_list = orignal_egy.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_from = len(orignal_egy[\"تست\"])\n",
    "project_to = mapped_egy.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([orignal_egy.get_word_vector(word) for word in word_list])\n",
    "Y = np.array([mapped_egy.get_vector(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "[1,  2000] loss: 0.298\n",
      "[1,  4000] loss: 0.226\n",
      "[1,  6000] loss: 0.173\n",
      "[1,  8000] loss: 0.127\n",
      "[1, 10000] loss: 0.090\n",
      "[1, 12000] loss: 0.056\n",
      "[1, 14000] loss: 0.026\n",
      "[1, 16000] loss: 0.007\n",
      "[1, 18000] loss: 0.003\n",
      "[1, 20000] loss: 0.002\n",
      "[1, 22000] loss: 0.002\n",
      "[1, 24000] loss: 0.001\n",
      "[1, 26000] loss: 0.001\n",
      "[1, 28000] loss: 0.001\n",
      "[1, 30000] loss: 0.001\n",
      "[1, 32000] loss: 0.001\n",
      "[1, 34000] loss: 0.001\n",
      "Starting epoch 2\n",
      "[2,  2000] loss: 0.001\n",
      "[2,  4000] loss: 0.001\n",
      "[2,  6000] loss: 0.001\n",
      "[2,  8000] loss: 0.001\n",
      "[2, 10000] loss: 0.001\n",
      "[2, 12000] loss: 0.001\n",
      "[2, 14000] loss: 0.001\n",
      "[2, 16000] loss: 0.001\n",
      "[2, 18000] loss: 0.001\n",
      "[2, 20000] loss: 0.001\n",
      "[2, 22000] loss: 0.001\n",
      "[2, 24000] loss: 0.001\n",
      "[2, 26000] loss: 0.001\n",
      "[2, 28000] loss: 0.001\n",
      "[2, 30000] loss: 0.001\n",
      "[2, 32000] loss: 0.001\n",
      "[2, 34000] loss: 0.001\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "class VectorDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      self.X = torch.from_numpy(X)\n",
    "      self.y = torch.from_numpy(y)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return self.X[i], self.y[i]\n",
    "      \n",
    "class NeuralNet(nn.Module):\n",
    " \n",
    "  def __init__(self,):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.nn = nn.Linear(project_from, project_to)\n",
    "   \n",
    "  def forward(self, inputs):\n",
    "    return self.nn(inputs)\n",
    "\n",
    "dataset = VectorDataSet(X, Y)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "  \n",
    "model = NeuralNet()\n",
    "\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(2): \n",
    "  \n",
    "  print(f'Starting epoch {epoch+1}')\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "      inputs, actual = data\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_function(outputs, actual)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      running_loss += loss.item()\n",
    "      if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "          print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "          running_loss = 0.0\n",
    "\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1961,  0.0173,  0.0794,  ...,  0.0104, -0.0121, -0.0661],\n",
       "        [-0.0055,  0.1659,  0.1194,  ..., -0.0704, -0.0691,  0.1322],\n",
       "        [-0.1752,  0.0294, -0.1327,  ..., -0.1015, -0.0077,  0.1027],\n",
       "        ...,\n",
       "        [-0.0349, -0.0809,  0.0773,  ..., -0.1100,  0.0220, -0.0740],\n",
       "        [ 0.0607, -0.0275, -0.0053,  ...,  0.0777,  0.1975,  0.0988],\n",
       "        [ 0.0288, -0.1798,  0.0052,  ...,  0.1797, -0.1421, -0.0617]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.nn.weight\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform OOV Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.2040e-02,  1.4865e-01, -5.6032e-02,  3.7975e-02,  7.0395e-02,\n",
       "         1.2474e-01, -4.7408e-02,  9.4362e-02,  3.5227e-01, -3.7987e-02,\n",
       "        -1.9843e-01,  1.0184e-01, -4.8749e-02,  7.5740e-02,  2.6891e-01,\n",
       "         6.1159e-02, -1.5976e-02, -1.6184e-02,  2.3527e-01, -5.5144e-02,\n",
       "         1.9678e-01,  3.2300e-01,  6.2339e-02, -1.0238e-01, -1.2260e-01,\n",
       "         5.7452e-03,  1.1809e-01, -4.1456e-02, -2.1094e-01,  3.4244e-01,\n",
       "        -8.4845e-04, -2.8098e-02, -1.7438e-02, -7.1443e-02, -2.5656e-01,\n",
       "        -5.7791e-02,  4.4908e-02, -2.4819e-01, -1.4585e-01,  5.2531e-02,\n",
       "        -1.6236e-01,  2.5883e-01,  7.1127e-02,  2.4198e-01, -8.9583e-02,\n",
       "        -1.9114e-02,  9.8471e-02, -1.0518e-02, -6.6584e-02, -2.2069e-01,\n",
       "         1.1617e-02, -5.5783e-02, -3.3539e-01,  6.7654e-02,  8.4350e-03,\n",
       "         9.7909e-02, -3.2134e-01, -2.9755e-01, -1.4918e-03, -9.0681e-02,\n",
       "         1.8118e-01,  3.5747e-01,  5.1818e-02, -2.9018e-01,  1.7030e-02,\n",
       "         2.9945e-01, -2.9328e-03, -1.7895e-02,  3.4473e-03, -2.6543e-02,\n",
       "        -2.7462e-02,  2.0504e-01, -3.2658e-02, -3.2920e-01, -2.2204e-01,\n",
       "        -1.4486e-01,  6.0587e-02,  4.8276e-02,  3.2509e-02, -1.9458e-01,\n",
       "        -2.4458e-02, -2.4870e-01,  7.4766e-02, -2.3440e-02, -1.2520e-01,\n",
       "         2.5374e-01, -5.0399e-02, -1.3608e-04,  2.5210e-01, -1.4027e-01,\n",
       "         4.7465e-02, -2.3372e-01,  3.8629e-02, -5.4209e-02, -1.3813e-02,\n",
       "         7.5917e-02, -1.5300e-01,  3.4444e-01, -1.4548e-01,  2.5310e-02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = torch.from_numpy(orignal_egy[\"ابرا كادبرا\"])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0328,  0.1184,  0.0196,  0.1928, -0.0652, -0.0742, -0.0329,  0.0404,\n",
       "        -0.1578,  0.3530, -0.0938,  0.2301, -0.3129,  0.2493, -0.1146, -0.0506,\n",
       "         0.1905, -0.0193,  0.1866,  0.1487,  0.0945,  0.1833, -0.0334, -0.1684,\n",
       "        -0.3585, -0.1886,  0.1418,  0.0276,  0.2461,  0.2100,  0.1094,  0.0983,\n",
       "        -0.3934, -0.0558,  0.2301, -0.0235, -0.1950,  0.1035, -0.1355,  0.1216,\n",
       "        -0.0964,  0.0355,  0.1662,  0.1063, -0.0174,  0.1232,  0.2988,  0.1953,\n",
       "        -0.0836,  0.1050, -0.0803, -0.0747, -0.0875, -0.1717,  0.0876,  0.1360,\n",
       "        -0.1636, -0.0818,  0.1198,  0.2259, -0.2240, -0.0136,  0.1107,  0.0980,\n",
       "        -0.0221, -0.1472,  0.2532, -0.1975, -0.1256, -0.2077,  0.2116,  0.1570,\n",
       "        -0.0740,  0.0456,  0.0062,  0.1324, -0.1616,  0.0403,  0.3018, -0.0264,\n",
       "         0.1925,  0.2399,  0.0640,  0.2653, -0.0307, -0.0484, -0.3235,  0.1310,\n",
       "        -0.0336, -0.1031, -0.1970,  0.1981, -0.0824,  0.0857, -0.0777,  0.1374,\n",
       "        -0.1464, -0.0430, -0.0428, -0.0204])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_egy = torch.nn.Linear(project_from, project_to, bias=False)\n",
    "#to_reload = torch.from_numpy(torch.load('best_mapping.pth'))\n",
    "mapping_egy.weight.data.copy_(weights.type_as(mapping_egy.weight.data))\n",
    "translated_vector = mapping_egy(vector)\n",
    "translated_vector.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "    \n",
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))\n",
    "\n",
    "def get_nn_oov(word, word_emb, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'C:/Users/Abdul/Desktop/Grad Project/code/Embeddings/embeddings-20220313T080929Z-001/embeddings/vectors-EGY.txt'\n",
    "tgt_path = 'C:/Users/Abdul/Desktop/Grad Project/code/Embeddings/embeddings-20220313T080929Z-001/embeddings/vectors-GLF.txt'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "1.0000 - باشا\n",
      "0.7139 - يباشا\n",
      "0.7020 - اباشا\n",
      "0.6532 - الباشا\n",
      "0.5911 - حاشا\n"
     ]
    }
   ],
   "source": [
    "# EGY -> EGY\n",
    "get_nn(\"باشا\", src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.5828 - الميداني\n",
      "0.5297 - يثني\n",
      "0.5139 - تأليف\n",
      "0.5138 - وحمل\n",
      "0.5138 - أفتكر\n"
     ]
    }
   ],
   "source": [
    "# EGY -> GLF\n",
    "get_nn(\"باشا\", src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"تسذا\":\n",
      "0.6616 - يغضضن\n",
      "0.6415 - تظن\n",
      "0.6356 - وليضربن\n",
      "0.6310 - بحزمة\n",
      "0.6293 - فريقا\n"
     ]
    }
   ],
   "source": [
    "# GLF -> EGY\n",
    "get_nn(\"تسذا\", tgt_embeddings, tgt_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"تسذا\":\n",
      "1.0000 - تسذا\n",
      "0.8349 - شذا\n",
      "0.7860 - كيذا\n",
      "0.7541 - وشذا\n",
      "0.7523 - خن\n"
     ]
    }
   ],
   "source": [
    "# GLF -> GLF\n",
    "get_nn(\"تسذا\", tgt_embeddings, tgt_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"ابرا كادبرا\":\n",
      "0.6401 - هولاكو\n",
      "0.6060 - رآي\n",
      "0.6047 - أردوغان\n",
      "0.6030 - إدمان\n",
      "0.5985 - لفتت\n"
     ]
    }
   ],
   "source": [
    "get_nn_oov(\"ابرا كادبرا\", translated_vector.data, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy Of The Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.5828 - الميداني\n",
      "0.5297 - يثني\n",
      "0.5139 - تأليف\n",
      "0.5138 - وحمل\n",
      "0.5138 - أفتكر\n"
     ]
    }
   ],
   "source": [
    "get_nn(\"باشا\", src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"باشا\":\n",
      "0.5826 - الميداني\n",
      "0.5294 - يثني\n",
      "0.5141 - تأليف\n",
      "0.5136 - أفتكر\n",
      "0.5128 - ㅤ\n"
     ]
    }
   ],
   "source": [
    "vector = torch.from_numpy(orignal_egy[\"باشا\"])\n",
    "translated_vector = mapping_egy(vector)\n",
    "get_nn_oov(\"باشا\", translated_vector.data, tgt_embeddings, tgt_id2word, K=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afc42d48389851c183feefb692ff96d7f563ac08c561c2826665d9c8da615340"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('SHIT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
